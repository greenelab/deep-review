@article{g2vvbB91,
 abstract = {After a more than decade-long period of relatively little research activity
in the area of recurrent neural networks, several new developments will be
reviewed here that have allowed substantial progress both in understanding and
in technical solutions towards more efficient training of recurrent networks.
These advances have been motivated by and related to the optimization issues
surrounding deep learning. Although recurrent networks are extremely powerful
in what they can in principle represent in terms of modelling sequences,their
training is plagued by two aspects of the same issue regarding the learning of
long-term dependencies. Experiments reported here evaluate the use of clipping
gradients, spanning longer time ranges with leaky integration, advanced
momentum techniques, using more powerful output probability models, and
encouraging sparser gradients to help symmetry breaking and credit assignment.
The experiments are performed on text and music data and show off the combined
effects of these techniques in generally improving both training and test
error.},
 archiveprefix = {arXiv},
 author = {Yoshua Bengio and Nicolas Boulanger-Lewandowski and Razvan Pascanu},
 eprint = {1212.0901v2},
 file = {1212.0901v2.pdf},
 link = {http://arxiv.org/abs/1212.0901v2},
 month = {12},
 primaryclass = {cs.LG},
 title = {Advances in Optimizing Recurrent Networks},
 year = {2012}
}


@article{8t43CQ9m,
 abstract = {Predicting protein secondary structure is a fundamental problem in protein
structure prediction. Here we present a new supervised generative stochastic
network (GSN) based method to predict local secondary structure with deep
hierarchical representations. GSN is a recently proposed deep learning
technique (Bengio & Thibodeau-Laufer, 2013) to globally train deep generative
model. We present the supervised extension of GSN, which learns a Markov chain
to sample from a conditional distribution, and applied it to protein structure
prediction. To scale the model to full-sized, high-dimensional data, like
protein sequences with hundreds of amino acids, we introduce a convolutional
architecture, which allows efficient learning across multiple layers of
hierarchical representations. Our architecture uniquely focuses on predicting
structured low-level labels informed with both low and high-level
representations learned by the model. In our application this corresponds to
labeling the secondary structure state of each amino-acid residue. We trained
and tested the model on separate sets of non-homologous proteins sharing less
than 30% sequence identity. Our model achieves 66.4% Q8 accuracy on the CB513
dataset, better than the previously reported best performance 64.9% (Wang et
al., 2011) for this challenging secondary structure prediction problem.},
 archiveprefix = {arXiv},
 author = {Jian Zhou and Olga G. Troyanskaya},
 eprint = {1403.1347v1},
 file = {1403.1347v1.pdf},
 link = {http://arxiv.org/abs/1403.1347v1},
 month = {Mar},
 primaryclass = {q-bio.QM},
 title = {Deep Supervised and Convolutional Generative Stochastic Network for
Protein Secondary Structure Prediction},
 year = {2014}
}


@article{HKA6hi9E,
 abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in
object category classification and detection on hundreds of object categories
and millions of images. The challenge has been run annually from 2010 to
present, attracting participation from more than fifty institutions.
This paper describes the creation of this benchmark dataset and the advances
in object recognition that have been possible as a result. We discuss the
challenges of collecting large-scale ground truth annotation, highlight key
breakthroughs in categorical object recognition, provide a detailed analysis of
the current state of the field of large-scale image classification and object
detection, and compare the state-of-the-art computer vision accuracy with human
accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
 archiveprefix = {arXiv},
 author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
 eprint = {1409.0575v3},
 file = {1409.0575v3.pdf},
 link = {http://arxiv.org/abs/1409.0575v3},
 month = {Sep},
 primaryclass = {cs.CV},
 title = {ImageNet Large Scale Visual Recognition Challenge},
 year = {2014}
}


@article{pxdeuhMS,
 abstract = {We describe \textit{deep exponential families} (DEFs), a class of latent
variable models that are inspired by the hidden structures used in deep neural
networks. DEFs capture a hierarchy of dependencies between latent variables, and are easily generalized to many settings through exponential families. We
perform inference using recent "black box" variational inference techniques. We
then evaluate various DEFs on text and combine multiple DEFs into a model for
pairwise recommendation data. In an extensive study, we show that going beyond
one layer improves predictions for DEFs. We demonstrate that DEFs find
interesting exploratory structure in large data sets, and give better
predictive performance than state-of-the-art models.},
 archiveprefix = {arXiv},
 author = {Rajesh Ranganath and Linpeng Tang and Laurent Charlin and David M. Blei},
 eprint = {1411.2581v1},
 file = {1411.2581v1.pdf},
 link = {http://arxiv.org/abs/1411.2581v1},
 month = {Dec},
 primaryclass = {stat.ML},
 title = {Deep Exponential Families},
 year = {2014}
}


@article{Z7fd0BYf,
 abstract = {Deep convolutional neural networks comprise a subclass of deep neural
networks (DNN) with a constrained architecture that leverages the spatial and
temporal structure of the domain they model. Convolutional networks achieve the
best predictive performance in areas such as speech and image recognition by
hierarchically composing simple local features into complex models. Although
DNNs have been used in drug discovery for QSAR and ligand-based bioactivity
predictions, none of these models have benefited from this powerful
convolutional architecture. This paper introduces AtomNet, the first
structure-based, deep convolutional neural network designed to predict the
bioactivity of small molecules for drug discovery applications. We demonstrate
how to apply the convolutional concepts of feature locality and hierarchical
composition to the modeling of bioactivity and chemical interactions. In
further contrast to existing DNN techniques, we show that AtomNet's application
of local convolutional filters to structural target information successfully
predicts new active molecules for targets with no previously known modulators.
Finally, we show that AtomNet outperforms previous docking approaches on a
diverse set of benchmarks by a large margin, achieving an AUC greater than 0.9
on 57.8% of the targets in the DUDE benchmark.},
 archiveprefix = {arXiv},
 author = {Izhar Wallach and Michael Dzamba and Abraham Heifets},
 eprint = {1510.02855v1},
 file = {1510.02855v1.pdf},
 link = {http://arxiv.org/abs/1510.02855v1},
 month = {Nov},
 primaryclass = {cs.LG},
 title = {AtomNet: A Deep Convolutional Neural Network for Bioactivity Prediction
in Structure-based Drug Discovery},
 year = {2015}
}


@article{HRXii6Ni,
 abstract = {Personalized predictive medicine necessitates the modeling of patient illness
and care processes, which inherently have long-term temporal dependencies.
Healthcare observations, recorded in electronic medical records, are episodic
and irregular in time. We introduce DeepCare, an end-to-end deep dynamic neural
network that reads medical records, stores previous illness history, infers
current illness states and predicts future medical outcomes. At the data level, DeepCare represents care episodes as vectors in space, models patient health
state trajectories through explicit memory of historical records. Built on Long
Short-Term Memory (LSTM), DeepCare introduces time parameterizations to handle
irregular timed events by moderating the forgetting and consolidation of memory
cells. DeepCare also incorporates medical interventions that change the course
of illness and shape future medical risk. Moving up to the health state level, historical and present health states are then aggregated through multiscale
temporal pooling, before passing through a neural network that estimates future
outcomes. We demonstrate the efficacy of DeepCare for disease progression
modeling, intervention recommendation, and future risk prediction. On two
important cohorts with heavy social and economic burden -- diabetes and mental
health -- the results show improved modeling and risk prediction accuracy.},
 archiveprefix = {arXiv},
 author = {Trang Pham and Truyen Tran and Dinh Phung and Svetha Venkatesh},
 eprint = {1602.00357v2},
 file = {1602.00357v2.pdf},
 link = {http://arxiv.org/abs/1602.00357v2},
 month = {Feb},
 primaryclass = {stat.ML},
 title = {DeepCare: A Deep Dynamic Memory Model for Predictive Medicine},
 year = {2016}
}


@article{5Il3kN32,
 abstract = {Large labeled training sets are the critical building blocks of supervised
learning methods and are key enablers of deep learning techniques. For some
applications, creating labeled training sets is the most time-consuming and
expensive part of applying machine learning. We therefore propose a paradigm
for the programmatic creation of training sets called data programming in which
users express weak supervision strategies or domain heuristics as labeling
functions, which are programs that label subsets of the data, but that are
noisy and may conflict. We show that by explicitly representing this training
set labeling process as a generative model, we can "denoise" the generated
training set, and establish theoretically that we can recover the parameters of
these generative models in a handful of settings. We then show how to modify a
discriminative loss function to make it noise-aware, and demonstrate our method
over a range of discriminative models including logistic regression and LSTMs.
Experimentally, on the 2014 TAC-KBP Slot Filling challenge, we show that data
programming would have led to a new winning score, and also show that applying
data programming to an LSTM model leads to a TAC-KBP score almost 6 F1 points
over a state-of-the-art LSTM baseline (and into second place in the
competition). Additionally, in initial user studies we observed that data
programming may be an easier way for non-experts to create machine learning
models when training data is limited or unavailable.},
 archiveprefix = {arXiv},
 author = {Alexander Ratner and Christopher De Sa and Sen Wu and Daniel Selsam and Christopher Ré},
 eprint = {1605.07723v3},
 file = {1605.07723v3.pdf},
 link = {http://arxiv.org/abs/1605.07723v3},
 month = {May},
 primaryclass = {stat.ML},
 title = {Data Programming: Creating Large Training Sets, Quickly},
 year = {2016}
}


@article{1FE0F2pQ,
 abstract = {Previous research has shown that neural networks can model survival data in
situations in which some patients' death times are unknown, e.g.
right-censored. However, neural networks have rarely been shown to outperform
their linear counterparts such as the Cox proportional hazards model. In this
paper, we run simulated experiments and use real survival data to build upon
the risk-regression architecture proposed by Faraggi and Simon. We demonstrate
that our model, DeepSurv, not only works as well as other survival models but
actually outperforms in predictive ability on survival data with linear and
nonlinear risk functions. We then show that the neural network can also serve
as a recommender system by including a categorical variable representing a
treatment group. This can be used to provide personalized treatment
recommendations based on an individual's calculated risk. We provide an open
source Python module that implements these methods in order to advance research
on deep learning and survival analysis.},
 archiveprefix = {arXiv},
 author = {Jared Katzman and Uri Shaham and Jonathan Bates and Alexander Cloninger and Tingting Jiang and Yuval Kluger},
 eprint = {1606.00931v2},
 file = {1606.00931v2.pdf},
 link = {http://arxiv.org/abs/1606.00931v2},
 month = {Jun},
 primaryclass = {stat.ML},
 title = {Deep Survival: A Deep Cox Proportional Hazards Network},
 year = {2016}
}


@article{mbEp6jNr,
 abstract = {The International Symposium on Biomedical Imaging (ISBI) held a grand
challenge to evaluate computational systems for the automated detection of
metastatic breast cancer in whole slide images of sentinel lymph node biopsies.
Our team won both competitions in the grand challenge, obtaining an area under
the receiver operating curve (AUC) of 0.925 for the task of whole slide image
classification and a score of 0.7051 for the tumor localization task. A
pathologist independently reviewed the same images, obtaining a whole slide
image classification AUC of 0.966 and a tumor localization score of 0.733.
Combining our deep learning system's predictions with the human pathologist's
diagnoses increased the pathologist's AUC to 0.995, representing an
approximately 85 percent reduction in human error rate. These results
demonstrate the power of using deep learning to produce significant
improvements in the accuracy of pathological diagnoses.},
 archiveprefix = {arXiv},
 author = {Dayong Wang and Aditya Khosla and Rishab Gargeya and Humayun Irshad and Andrew H. Beck},
 eprint = {1606.05718v1},
 file = {1606.05718v1.pdf},
 link = {http://arxiv.org/abs/1606.05718v1},
 month = {Jun},
 primaryclass = {q-bio.QM},
 title = {Deep Learning for Identifying Metastatic Breast Cancer},
 year = {2016}
}


@article{7yE9K08a,
 abstract = {We summarize the potential impact that the European Union's new General Data
Protection Regulation will have on the routine use of machine learning
algorithms. Slated to take effect as law across the EU in 2018, it will
restrict automated individual decision-making (that is, algorithms that make
decisions based on user-level predictors) which "significantly affect" users.
The law will also effectively create a "right to explanation," whereby a user
can ask for an explanation of an algorithmic decision that was made about them.
We argue that while this law will pose large challenges for industry, it
highlights opportunities for computer scientists to take the lead in designing
algorithms and evaluation frameworks which avoid discrimination and enable
explanation.},
 archiveprefix = {arXiv},
 author = {Bryce Goodman and Seth Flaxman},
 eprint = {1606.08813v3},
 file = {1606.08813v3.pdf},
 link = {http://arxiv.org/abs/1606.08813v3},
 month = {Jun},
 primaryclass = {stat.ML},
 title = {European Union regulations on algorithmic decision-making and a "right
to explanation"},
 year = {2016}
}


@article{ucHUOABT,
 abstract = {Machine learning techniques based on neural networks are achieving remarkable
results in a wide variety of domains. Often, the training of models requires
large, representative datasets, which may be crowdsourced and contain sensitive
information. The models should not expose private information in these
datasets. Addressing this goal, we develop new algorithmic techniques for
learning and a refined analysis of privacy costs within the framework of
differential privacy. Our implementation and experiments demonstrate that we
can train deep neural networks with non-convex objectives, under a modest
privacy budget, and at a manageable cost in software complexity, training
efficiency, and model quality.},
 archiveprefix = {arXiv},
 author = {Martín Abadi and Andy Chu and Ian Goodfellow and H. Brendan McMahan and Ilya Mironov and Kunal Talwar and Li Zhang},
 doi = {10.1145/2976749.2978318},
 eprint = {1607.00133v2},
 file = {1607.00133v2.pdf},
 link = {http://arxiv.org/abs/1607.00133v2},
 month = {Jul},
 primaryclass = {stat.ML},
 title = {Deep Learning with Differential Privacy},
 year = {2016}
}


@article{Ohd1Q9Xw,
 abstract = {Feature engineering remains a major bottleneck when creating predictive
systems from electronic medical records. At present, an important missing
element is detecting predictive regular clinical motifs from irregular episodic
records. We present Deepr (short for Deep record), a new end-to-end deep
learning system that learns to extract features from medical records and
predicts future risk automatically. Deepr transforms a record into a sequence
of discrete elements separated by coded time gaps and hospital transfers. On
top of the sequence is a convolutional neural net that detects and combines
predictive local clinical motifs to stratify the risk. Deepr permits
transparent inspection and visualization of its inner working. We validate
Deepr on hospital data to predict unplanned readmission after discharge. Deepr
achieves superior accuracy compared to traditional techniques, detects
meaningful clinical motifs, and uncovers the underlying structure of the
disease and intervention space.},
 archiveprefix = {arXiv},
 author = {Phuoc Nguyen and Truyen Tran and Nilmini Wickramasinghe and Svetha Venkatesh},
 eprint = {1607.07519v1},
 file = {1607.07519v1.pdf},
 link = {http://arxiv.org/abs/1607.07519v1},
 month = {Jul},
 primaryclass = {stat.ML},
 title = {Deepr: A Convolutional Net for Medical Records},
 year = {2016}
}


@article{qXdO2aMm,
 abstract = {The electronic health record (EHR) provides an unprecedented opportunity to
build actionable tools to support physicians at the point of care. In this
paper, we investigate survival analysis in the context of EHR data. We
introduce deep survival analysis, a hierarchical generative approach to
survival analysis. It departs from previous approaches in two primary ways: (1)
all observations, including covariates, are modeled jointly conditioned on a
rich latent structure; and (2) the observations are aligned by their failure
time, rather than by an arbitrary time zero as in traditional survival
analysis. Further, it (3) scalably handles heterogeneous (continuous and
discrete) data types that occur in the EHR. We validate deep survival analysis
model by stratifying patients according to risk of developing coronary heart
disease (CHD). Specifically, we study a dataset of 313,000 patients
corresponding to 5.5 million months of observations. When compared to the
clinically validated Framingham CHD risk score, deep survival analysis is
significantly superior in stratifying patients according to their risk.},
 archiveprefix = {arXiv},
 author = {Rajesh Ranganath and Adler Perotte and Noémie Elhadad and David Blei},
 eprint = {1608.02158v2},
 file = {1608.02158v2.pdf},
 link = {http://arxiv.org/abs/1608.02158v2},
 month = {Aug},
 primaryclass = {stat.ML},
 title = {Deep Survival Analysis},
 year = {2016}
}


@article{ULSPV0rh,
 abstract = {Machine learning (ML) models may be deemed confidential due to their
sensitive training data, commercial value, or use in security applications.
Increasingly often, confidential ML models are being deployed with publicly
accessible query interfaces. ML-as-a-service ("predictive analytics") systems
are an example: Some allow users to train models on potentially sensitive data
and charge others for access on a pay-per-query basis.
The tension between model confidentiality and public access motivates our
investigation of model extraction attacks. In such attacks, an adversary with
black-box access, but no prior knowledge of an ML model's parameters or
training data, aims to duplicate the functionality of (i.e., "steal") the
model. Unlike in classical learning theory settings, ML-as-a-service offerings
may accept partial feature vectors as inputs and include confidence values with
predictions. Given these practices, we show simple, efficient attacks that
extract target ML models with near-perfect fidelity for popular model classes
including logistic regression, neural networks, and decision trees. We
demonstrate these attacks against the online services of BigML and Amazon
Machine Learning. We further show that the natural countermeasure of omitting
confidence values from model outputs still admits potentially harmful model
extraction attacks. Our results highlight the need for careful ML model
deployment and new model extraction countermeasures.},
 archiveprefix = {arXiv},
 author = {Florian Tramèr and Fan Zhang and Ari Juels and Michael K. Reiter and Thomas Ristenpart},
 eprint = {1609.02943v2},
 file = {1609.02943v2.pdf},
 link = {http://arxiv.org/abs/1609.02943v2},
 month = {Sep},
 primaryclass = {cs.CR},
 title = {Stealing Machine Learning Models via Prediction APIs},
 year = {2016}
}


@article{dO844vZn,
 abstract = {Automated extraction of concepts from patient clinical records is an
essential facilitator of clinical research. For this reason, the 2010 i2b2/VA
Natural Language Processing Challenges for Clinical Records introduced a
concept extraction task aimed at identifying and classifying concepts into
predefined categories (i.e., treatments, tests and problems). State-of-the-art
concept extraction approaches heavily rely on handcrafted features and
domain-specific resources which are hard to collect and define. For this
reason, this paper proposes an alternative, streamlined approach: a recurrent
neural network (the bidirectional LSTM with CRF decoding) initialized with
general-purpose, off-the-shelf word embeddings. The experimental results
achieved on the 2010 i2b2/VA reference corpora using the proposed framework
outperform all recent methods and ranks closely to the best submission from the
original 2010 i2b2/VA challenge.},
 archiveprefix = {arXiv},
 author = {Raghavendra Chalapathy and Ehsan Zare Borzeshi and Massimo Piccardi},
 eprint = {1611.08373v1},
 file = {1611.08373v1.pdf},
 link = {http://arxiv.org/abs/1611.08373v1},
 month = {Dec},
 primaryclass = {stat.ML},
 title = {Bidirectional LSTM-CRF for Clinical Concept Extraction},
 year = {2016}
}


@article{bNBiIiTt,
 abstract = {Computational approaches to drug discovery can reduce the time and cost
associated with experimental assays and enable the screening of novel
chemotypes. Structure-based drug design methods rely on scoring functions to
rank and predict binding affinities and poses. The ever-expanding amount of
protein-ligand binding and structural data enables the use of deep machine
learning techniques for protein-ligand scoring.
We describe convolutional neural network (CNN) scoring functions that take as
input a comprehensive 3D representation of a protein-ligand interaction. A CNN
scoring function automatically learns the key features of protein-ligand
interactions that correlate with binding. We train and optimize our CNN scoring
functions to discriminate between correct and incorrect binding poses and known
binders and non-binders. We find that our CNN scoring function outperforms the
AutoDock Vina scoring function when ranking poses both for pose prediction and
virtual screening.},
 archiveprefix = {arXiv},
 author = {Matthew Ragoza and Joshua Hochuli and Elisa Idrobo and Jocelyn Sunseri and David Ryan Koes},
 eprint = {1612.02751v1},
 file = {1612.02751v1.pdf},
 link = {http://arxiv.org/abs/1612.02751v1},
 month = {12},
 primaryclass = {stat.ML},
 title = {Protein-Ligand Scoring with Convolutional Neural Networks},
 year = {2016}
}


@article{HRoooKGh,
 abstract = {Recent advances in machine learning have made significant contributions to
drug discovery. Deep neural networks in particular have been demonstrated to
provide significant boosts in predictive power when inferring the properties
and activities of small-molecule compounds. However, the applicability of these
techniques has been limited by the requirement for large amounts of training
data. In this work, we demonstrate how one-shot learning can be used to
significantly lower the amounts of data required to make meaningful predictions
in drug discovery applications. We introduce a new architecture, the residual
LSTM embedding, that, when combined with graph convolutional neural networks, significantly improves the ability to learn meaningful distance metrics over
small-molecules. We open source all models introduced in this work as part of
DeepChem, an open-source framework for deep-learning in drug discovery.},
 archiveprefix = {arXiv},
 author = {Han Altae-Tran and Bharath Ramsundar and Aneesh S. Pappu and Vijay Pande},
 eprint = {1611.03199v1},
 file = {1611.03199v1.pdf},
 link = {http://arxiv.org/abs/1611.03199v1},
 month = {Dec},
 primaryclass = {cs.LG},
 title = {Low Data Drug Discovery with One-shot Learning},
 year = {2016}
}


@article{1G3owNNps,
 abstract = {Multipliers are the most space and power-hungry arithmetic operators of the
digital implementation of deep neural networks. We train a set of
state-of-the-art neural networks (Maxout networks) on three benchmark datasets:
MNIST, CIFAR-10 and SVHN. They are trained with three distinct formats:
floating point, fixed point and dynamic fixed point. For each of those datasets
and for each of those formats, we assess the impact of the precision of the
multiplications on the final error after training. We find that very low
precision is sufficient not just for running trained networks but also for
training them. For example, it is possible to train Maxout networks with 10
bits multiplications.},
 archiveprefix = {arXiv},
 author = {Matthieu Courbariaux and Yoshua Bengio and Jean-Pierre David},
 eprint = {1412.7024v5},
 file = {1412.7024v5.pdf},
 link = {http://arxiv.org/abs/1412.7024v5},
 month = {12},
 primaryclass = {cs.LG},
 title = {Training deep neural networks with low precision multiplications},
 year = {2014}
}


@article{1BTJ1KqRa,
 abstract = {Motivation: The MinION device by Oxford Nanopore is the first portable
sequencing device. MinION is able to produce very long reads (reads over
100~kBp were reported), however it suffers from high sequencing error rate. In
this paper, we show that the error rate can be reduced by improving the base
calling process.
Results: We present the first open-source DNA base caller for the MinION
sequencing platform by Oxford Nanopore. By employing carefully crafted
recurrent neural networks, our tool improves the base calling accuracy compared
to the default base caller supplied by the manufacturer. This advance may
further enhance applicability of MinION for genome sequencing and various
clinical applications.
Availability: DeepNano can be downloaded at
http://compbio.fmph.uniba.sk/deepnano/.
Contact: boza@fmph.uniba.sk},
 archiveprefix = {arXiv},
 author = {Vladimír Boža and Broňa Brejová and Tomáš Vinař},
 eprint = {1603.09195v1},
 file = {1603.09195v1.pdf},
 link = {http://arxiv.org/abs/1603.09195v1},
 month = {Mar},
 primaryclass = {q-bio.GN},
 title = {DeepNano: Deep Recurrent Neural Networks for Base Calling in MinION
Nanopore Reads},
 year = {2016}
}


@article{1AhGoHZP9,
 abstract = {Currently, deep neural networks are the state of the art on problems such as
speech recognition and computer vision. In this extended abstract, we show that
shallow feed-forward networks can learn the complex functions previously
learned by deep nets and achieve accuracies previously only achievable with
deep models. Moreover, in some cases the shallow neural nets can learn these
deep functions using a total number of parameters similar to the original deep
model. We evaluate our method on the TIMIT phoneme recognition task and are
able to train shallow fully-connected nets that perform similarly to complex, well-engineered, deep convolutional architectures. Our success in training
shallow neural nets to mimic deeper models suggests that there probably exist
better algorithms for training shallow feed-forward nets than those currently
available.},
 archiveprefix = {arXiv},
 author = {Lei Jimmy Ba and Rich Caruana},
 eprint = {1312.6184v7},
 file = {1312.6184v7.pdf},
 link = {http://arxiv.org/abs/1312.6184v7},
 month = {12},
 primaryclass = {cs.LG},
 title = {Do Deep Nets Really Need to be Deep?},
 year = {2013}
}


@article{15lYGmZpY,
 abstract = {As deep nets are increasingly used in applications suited for mobile devices, a fundamental dilemma becomes apparent: the trend in deep learning is to grow
models to absorb ever-increasing data set sizes; however mobile devices are
designed with very little memory and cannot store such large models. We present
a novel network architecture, HashedNets, that exploits inherent redundancy in
neural networks to achieve drastic reductions in model sizes. HashedNets uses a
low-cost hash function to randomly group connection weights into hash buckets, and all connections within the same hash bucket share a single parameter value.
These parameters are tuned to adjust to the HashedNets weight sharing
architecture with standard backprop during training. Our hashing procedure
introduces no additional memory overhead, and we demonstrate on several
benchmark data sets that HashedNets shrink the storage requirements of neural
networks substantially while mostly preserving generalization performance.},
 archiveprefix = {arXiv},
 author = {Wenlin Chen and James T. Wilson and Stephen Tyree and Kilian Q. Weinberger and Yixin Chen},
 eprint = {1504.04788v1},
 file = {1504.04788v1.pdf},
 link = {http://arxiv.org/abs/1504.04788v1},
 month = {Apr},
 primaryclass = {cs.LG},
 title = {Compressing Neural Networks with the Hashing Trick},
 year = {2015}
}


@article{YwdqeYZi,
 abstract = {We present a library of efficient implementations of deep learning
primitives. Deep learning workloads are computationally intensive, and
optimizing their kernels is difficult and time-consuming. As parallel
architectures evolve, kernels must be reoptimized, which makes maintaining
codebases difficult over time. Similar issues have long been addressed in the
HPC community by libraries such as the Basic Linear Algebra Subroutines (BLAS).
However, there is no analogous library for deep learning. Without such a
library, researchers implementing deep learning workloads on parallel
processors must create and optimize their own implementations of the main
computational kernels, and this work must be repeated as new parallel
processors emerge. To address this problem, we have created a library similar
in intent to BLAS, with optimized routines for deep learning workloads. Our
implementation contains routines for GPUs, although similarly to the BLAS
library, these routines could be implemented for other platforms. The library
is easy to integrate into existing frameworks, and provides optimized
performance and memory usage. For example, integrating cuDNN into Caffe, a
popular framework for convolutional networks, improves performance by 36% on a
standard model while also reducing memory consumption.},
 archiveprefix = {arXiv},
 author = {Sharan Chetlur and Cliff Woolley and Philippe Vandermersch and Jonathan Cohen and John Tran and Bryan Catanzaro and Evan Shelhamer},
 eprint = {1410.0759v3},
 file = {1410.0759v3.pdf},
 link = {http://arxiv.org/abs/1410.0759v3},
 month = {Nov},
 primaryclass = {cs.NE},
 title = {cuDNN: Efficient Primitives for Deep Learning},
 year = {2014}
}


@article{2dU8f4XJ,
 abstract = {We report a method to convert discrete representations of molecules to and
from a multidimensional continuous representation. This generative model allows
efficient search and optimization through open-ended spaces of chemical
compounds. We train deep neural networks on hundreds of thousands of existing
chemical structures to construct two coupled functions: an encoder and a
decoder. The encoder converts the discrete representation of a molecule into a
real-valued continuous vector, and the decoder converts these continuous
vectors back to the discrete representation from this latent space. Continuous
representations allow us to automatically generate novel chemical structures by
performing simple operations in the latent space, such as decoding random
vectors, perturbing known chemical structures, or interpolating between
molecules. Continuous representations also allow the use of powerful
gradient-based optimization to efficiently guide the search for optimized
functional compounds. We demonstrate our method in the design of drug-like
molecules as well as organic light-emitting diodes.},
 archiveprefix = {arXiv},
 author = {Rafael Gómez-Bombarelli and David Duvenaud and José Miguel Hernández-Lobato and Jorge Aguilera-Iparraguirre and Timothy D. Hirzel and Ryan P. Adams and Alán Aspuru-Guzik},
 eprint = {1610.02415v2},
 file = {1610.02415v2.pdf},
 link = {http://arxiv.org/abs/1610.02415v2},
 month = {Nov},
 primaryclass = {cs.LG},
 title = {Automatic chemical design using a data-driven continuous representation
of molecules},
 year = {2016}
}


@article{CKcJuj03,
 abstract = {Training of large-scale deep neural networks is often constrained by the
available computational resources. We study the effect of limited precision
data representation and computation on neural network training. Within the
context of low-precision fixed-point computations, we observe the rounding
scheme to play a crucial role in determining the network's behavior during
training. Our results show that deep networks can be trained using only 16-bit
wide fixed-point number representation when using stochastic rounding, and
incur little to no degradation in the classification accuracy. We also
demonstrate an energy-efficient hardware accelerator that implements
low-precision fixed-point arithmetic with stochastic rounding.},
 archiveprefix = {arXiv},
 author = {Suyog Gupta and Ankur Agrawal and Kailash Gopalakrishnan and Pritish Narayanan},
 eprint = {1502.02551v1},
 file = {1502.02551v1.pdf},
 link = {http://arxiv.org/abs/1502.02551v1},
 month = {Feb},
 primaryclass = {cs.LG},
 title = {Deep Learning with Limited Numerical Precision},
 year = {2015}
}


@article{13KjSCKB2,
 abstract = {We present Caffe con Troll (CcT), a fully compatible end-to-end version of
the popular framework Caffe with rebuilt internals. We built CcT to examine the
performance characteristics of training and deploying general-purpose
convolutional neural networks across different hardware architectures. We find
that, by employing standard batching optimizations for CPU training, we achieve
a 4.5x throughput improvement over Caffe on popular networks like CaffeNet.
Moreover, with these improvements, the end-to-end training time for CNNs is
directly proportional to the FLOPS delivered by the CPU, which enables us to
efficiently train hybrid CPU-GPU systems for CNNs.},
 archiveprefix = {arXiv},
 author = {Stefan Hadjis and Firas Abuzaid and Ce Zhang and Christopher Ré},
 eprint = {1504.04343v2},
 file = {1504.04343v2.pdf},
 link = {http://arxiv.org/abs/1504.04343v2},
 month = {Apr},
 primaryclass = {cs.LG},
 title = {Caffe con Troll: Shallow Ideas to Speed Up Deep Learning},
 year = {2015}
}


@article{j7KrVyi8,
 abstract = {Deeper neural networks are more difficult to train. We present a residual
learning framework to ease the training of networks that are substantially
deeper than those used previously. We explicitly reformulate the layers as
learning residual functions with reference to the layer inputs, instead of
learning unreferenced functions. We provide comprehensive empirical evidence
showing that these residual networks are easier to optimize, and can gain
accuracy from considerably increased depth. On the ImageNet dataset we evaluate
residual nets with a depth of up to 152 layers---8x deeper than VGG nets but
still having lower complexity. An ensemble of these residual nets achieves
3.57% error on the ImageNet test set. This result won the 1st place on the
ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100
and 1000 layers.
The depth of representations is of central importance for many visual
recognition tasks. Solely due to our extremely deep representations, we obtain
a 28% relative improvement on the COCO object detection dataset. Deep residual
nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet
localization, COCO detection, and COCO segmentation.},
 archiveprefix = {arXiv},
 author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
 eprint = {1512.03385v1},
 file = {1512.03385v1.pdf},
 link = {http://arxiv.org/abs/1512.03385v1},
 month = {12},
 primaryclass = {cs.CV},
 title = {Deep Residual Learning for Image Recognition},
 year = {2015}
}


@article{1CRF3gAV,
 abstract = {A very simple way to improve the performance of almost any machine learning
algorithm is to train many different models on the same data and then to
average their predictions. Unfortunately, making predictions using a whole
ensemble of models is cumbersome and may be too computationally expensive to
allow deployment to a large number of users, especially if the individual
models are large neural nets. Caruana and his collaborators have shown that it
is possible to compress the knowledge in an ensemble into a single model which
is much easier to deploy and we develop this approach further using a different
compression technique. We achieve some surprising results on MNIST and we show
that we can significantly improve the acoustic model of a heavily used
commercial system by distilling the knowledge in an ensemble of models into a
single model. We also introduce a new type of ensemble composed of one or more
full models and many specialist models which learn to distinguish fine-grained
classes that the full models confuse. Unlike a mixture of experts, these
specialist models can be trained rapidly and in parallel.},
 archiveprefix = {arXiv},
 author = {Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
 eprint = {1503.02531v1},
 file = {1503.02531v1.pdf},
 link = {http://arxiv.org/abs/1503.02531v1},
 month = {Mar},
 primaryclass = {stat.ML},
 title = {Distilling the Knowledge in a Neural Network},
 year = {2015}
}


@article{1GUizyE8e,
 abstract = {We introduce a method to train Quantized Neural Networks (QNNs) --- neural
networks with extremely low precision (e.g., 1-bit) weights and activations, at
run-time. At train-time the quantized weights and activations are used for
computing the parameter gradients. During the forward pass, QNNs drastically
reduce memory size and accesses, and replace most arithmetic operations with
bit-wise operations. As a result, power consumption is expected to be
drastically reduced. We trained QNNs over the MNIST, CIFAR-10, SVHN and
ImageNet datasets. The resulting QNNs achieve prediction accuracy comparable to
their 32-bit counterparts. For example, our quantized version of AlexNet with
1-bit weights and 2-bit activations achieves $51\%$ top-1 accuracy. Moreover, we quantize the parameter gradients to 6-bits as well which enables gradients
computation using only bit-wise operation. Quantized recurrent neural networks
were tested over the Penn Treebank dataset, and achieved comparable accuracy as
their 32-bit counterparts using only 4-bits. Last but not least, we programmed
a binary matrix multiplication GPU kernel with which it is possible to run our
MNIST QNN 7 times faster than with an unoptimized GPU kernel, without suffering
any loss in classification accuracy. The QNN code is available online.},
 archiveprefix = {arXiv},
 author = {Itay Hubara and Matthieu Courbariaux and Daniel Soudry and Ran El-Yaniv and Yoshua Bengio},
 eprint = {1609.07061v1},
 file = {1609.07061v1.pdf},
 link = {http://arxiv.org/abs/1609.07061v1},
 month = {Sep},
 primaryclass = {cs.NE},
 title = {Quantized Neural Networks: Training Neural Networks with Low Precision
Weights and Activations},
 year = {2016}
}


@article{uP7SgBVd,
 abstract = {Deep learning methods such as multitask neural networks have recently been
applied to ligand-based virtual screening and other drug discovery
applications. Using a set of industrial ADMET datasets, we compare neural
networks to standard baseline models and analyze multitask learning effects
with both random cross-validation and a more relevant temporal validation
scheme. We confirm that multitask learning can provide modest benefits over
single-task models and show that smaller datasets tend to benefit more than
larger datasets from multitask learning. Additionally, we find that adding
massive amounts of side information is not guaranteed to improve performance
relative to simpler multitask learning. Our results emphasize that multitask
effects are highly dataset-dependent, suggesting the use of dataset-specific
models to maximize overall performance.},
 archiveprefix = {arXiv},
 author = {Steven Kearnes and Brian Goldman and Vijay Pande},
 eprint = {1606.08793v3},
 file = {1606.08793v3.pdf},
 link = {http://arxiv.org/abs/1606.08793v3},
 month = {Jun},
 primaryclass = {stat.ML},
 title = {Modeling Industrial ADMET Data with Multitask Networks},
 year = {2016}
}


@article{ZSVsnPVO,
 abstract = {I present a new way to parallelize the training of convolutional neural
networks across multiple GPUs. The method scales significantly better than all
alternatives when applied to modern convolutional neural networks.},
 archiveprefix = {arXiv},
 author = {Alex Krizhevsky},
 eprint = {1404.5997v2},
 file = {1404.5997v2.pdf},
 link = {http://arxiv.org/abs/1404.5997v2},
 month = {Apr},
 primaryclass = {cs.NE},
 title = {One weird trick for parallelizing convolutional neural networks},
 year = {2014}
}


@article{9NKsJjSw,
 abstract = {The rapid growth of data size and accessibility in recent years has
instigated a shift of philosophy in algorithm design for artificial
intelligence. Instead of engineering algorithms by hand, the ability to learn
composable systems automatically from massive amounts of data has led to
ground-breaking performance in important domains such as computer vision, speech recognition, and natural language processing. The most popular class of
techniques used in these domains is called deep learning, and is seeing
significant attention from industry. However, these models require incredible
amounts of data and compute power to train, and are limited by the need for
better hardware acceleration to accommodate scaling beyond current data and
model sizes. While the current solution has been to use clusters of graphics
processing units (GPU) as general purpose processors (GPGPU), the use of field
programmable gate arrays (FPGA) provide an interesting alternative. Current
trends in design tools for FPGAs have made them more compatible with the
high-level software practices typically practiced in the deep learning
community, making FPGAs more accessible to those who build and deploy models.
Since FPGA architectures are flexible, this could also allow researchers the
ability to explore model-level optimizations beyond what is possible on fixed
architectures such as GPUs. As well, FPGAs tend to provide high performance per
watt of power consumption, which is of particular importance for application
scientists interested in large scale server-based deployment or
resource-limited embedded applications. This review takes a look at deep
learning and FPGAs from a hardware acceleration perspective, identifying trends
and innovations that make these technologies a natural fit, and motivates a
discussion on how FPGAs may best serve the needs of the deep learning community
moving forward.},
 archiveprefix = {arXiv},
 author = {Griffin Lacey and Graham W. Taylor and Shawki Areibi},
 eprint = {1602.04283v1},
 file = {1602.04283v1.pdf},
 link = {http://arxiv.org/abs/1602.04283v1},
 month = {Feb},
 primaryclass = {cs.DC},
 title = {Deep Learning on FPGAs: Past, Present, and Future},
 year = {2016}
}


@article{Dwi2eAvT,
 abstract = {Deep neural network (DNN) models have recently obtained state-of-the-art
prediction accuracy for the transcription factor binding (TFBS) site
classification task. However, it remains unclear how these approaches identify
meaningful DNA sequence signals and give insights as to why TFs bind to certain
locations. In this paper, we propose a toolkit called the Deep Motif Dashboard
(DeMo Dashboard) which provides a suite of visualization strategies to extract
motifs, or sequence patterns from deep neural network models for TFBS
classification. We demonstrate how to visualize and understand three important
DNN models: convolutional, recurrent, and convolutional-recurrent networks. Our
first visualization method is finding a test sequence's saliency map which uses
first-order derivatives to describe the importance of each nucleotide in making
the final prediction. Second, considering recurrent models make predictions in
a temporal manner (from one end of a TFBS sequence to the other), we introduce
temporal output scores, indicating the prediction score of a model over time
for a sequential input. Lastly, a class-specific visualization strategy finds
the optimal input sequence for a given TFBS positive class via stochastic
gradient optimization. Our experimental results indicate that a
convolutional-recurrent architecture performs the best among the three
architectures. The visualization techniques indicate that CNN-RNN makes
predictions by modeling both motifs as well as dependencies among them.},
 archiveprefix = {arXiv},
 author = {Jack Lanchantin and Ritambhara Singh and Beilun Wang and Yanjun Qi},
 eprint = {1608.03644v4},
 file = {1608.03644v4.pdf},
 link = {http://arxiv.org/abs/1608.03644v4},
 month = {Aug},
 primaryclass = {cs.LG},
 title = {Deep Motif Dashboard: Visualizing and Understanding Genomic Sequences
Using Deep Neural Networks},
 year = {2016}
}


@article{rZnxDitd,
 abstract = {Apache Spark is a popular open-source platform for large-scale data
processing that is well-suited for iterative machine learning tasks. In this
paper we present MLlib, Spark's open-source distributed machine learning
library. MLlib provides efficient functionality for a wide range of learning
settings and includes several underlying statistical, optimization, and linear
algebra primitives. Shipped with Spark, MLlib supports several languages and
provides a high-level API that leverages Spark's rich ecosystem to simplify the
development of end-to-end machine learning pipelines. MLlib has experienced a
rapid growth due to its vibrant open-source community of over 140 contributors, and includes extensive documentation to support further growth and to let users
quickly get up to speed.},
 archiveprefix = {arXiv},
 author = {Xiangrui Meng and Joseph Bradley and Burak Yavuz and Evan Sparks and Shivaram Venkataraman and Davies Liu and Jeremy Freeman and DB Tsai and Manish Amde and Sean Owen and Doris Xin and Reynold Xin and Michael J. Franklin and Reza Zadeh and Matei Zaharia and Ameet Talwalkar},
 eprint = {1505.06807v1},
 file = {1505.06807v1.pdf},
 link = {http://arxiv.org/abs/1505.06807v1},
 month = {May},
 primaryclass = {cs.LG},
 title = {MLlib: Machine Learning in Apache Spark},
 year = {2015}
}


@article{rmJZ2Aui,
 abstract = {Training deep networks is a time-consuming process, with networks for object
recognition often requiring multiple days to train. For this reason, leveraging
the resources of a cluster to speed up training is an important area of work.
However, widely-popular batch-processing computational frameworks like
MapReduce and Spark were not designed to support the asynchronous and
communication-intensive workloads of existing distributed deep learning
systems. We introduce SparkNet, a framework for training deep networks in
Spark. Our implementation includes a convenient interface for reading data from
Spark RDDs, a Scala interface to the Caffe deep learning framework, and a
lightweight multi-dimensional tensor library. Using a simple parallelization
scheme for stochastic gradient descent, SparkNet scales well with the cluster
size and tolerates very high-latency communication. Furthermore, it is easy to
deploy and use with no parameter tuning, and it is compatible with existing
Caffe models. We quantify the dependence of the speedup obtained by SparkNet on
the number of machines, the communication frequency, and the cluster's
communication overhead, and we benchmark our system's performance on the
ImageNet dataset.},
 archiveprefix = {arXiv},
 author = {Philipp Moritz and Robert Nishihara and Ion Stoica and Michael I. Jordan},
 eprint = {1511.06051v4},
 file = {1511.06051v4.pdf},
 link = {http://arxiv.org/abs/1511.06051v4},
 month = {Dec},
 primaryclass = {stat.ML},
 title = {SparkNet: Training Deep Networks in Spark},
 year = {2015}
}


@article{w6CoVmFK,
 abstract = {Stochastic gradient descent (SGD) is a ubiquitous algorithm for a variety of
machine learning problems. Researchers and industry have developed several
techniques to optimize SGD's runtime performance, including asynchronous
execution and reduced precision. Our main result is a martingale-based analysis
that enables us to capture the rich noise models that may arise from such
techniques. Specifically, we use our new analysis in three ways: (1) we derive
convergence rates for the convex case (Hogwild!) with relaxed assumptions on
the sparsity of the problem; (2) we analyze asynchronous SGD algorithms for
non-convex matrix problems including matrix completion; and (3) we design and
analyze an asynchronous SGD algorithm, called Buckwild!, that uses
lower-precision arithmetic. We show experimentally that our algorithms run
efficiently for a variety of problems on modern hardware.},
 archiveprefix = {arXiv},
 author = {Christopher De Sa and Ce Zhang and Kunle Olukotun and Christopher Ré},
 eprint = {1506.06438v2},
 file = {1506.06438v2.pdf},
 link = {http://arxiv.org/abs/1506.06438v2},
 month = {Jun},
 primaryclass = {cs.LG},
 title = {Taming the Wild: A Unified Analysis of Hogwild!-Style Algorithms},
 year = {2015}
}


@article{T2Md9xLY,
 abstract = {Sources of variability in experimentally derived data include measurement
error in addition to the physical phenomena of interest. This measurement error
is a combination of systematic components, originating from the measuring
instrument, and random measurement errors. Several novel biological
technologies, such as mass cytometry and single-cell RNA-seq, are plagued with
systematic errors that may severely affect statistical analysis if the data is
not properly calibrated. We propose a novel deep learning approach for removing
systematic batch effects. Our method is based on a residual network, trained to
minimize the Maximum Mean Discrepancy (MMD) between the multivariate
distributions of two replicates, measured in different batches. We apply our
method to mass cytometry and single-cell RNA-seq datasets, and demonstrate that
it effectively attenuates batch effects.},
 archiveprefix = {arXiv},
 author = {Uri Shaham and Kelly P. Stanton and Jun Zhao and Huamin Li and Khadir Raddassi and Ruth Montgomery and Yuval Kluger},
 eprint = {1610.04181v5},
 file = {1610.04181v5.pdf},
 link = {http://arxiv.org/abs/1610.04181v5},
 month = {Nov},
 primaryclass = {stat.ML},
 title = {Removal of Batch Effects using Distribution-Matching Residual Networks},
 year = {2016}
}


@article{xAbGxia4,
 abstract = {Note: This paper describes an older version of DeepLIFT. See
https://arxiv.org/abs/1704.02685 for the newer version. Original abstract
follows: The purported "black box" nature of neural networks is a barrier to
adoption in applications where interpretability is essential. Here we present
DeepLIFT (Learning Important FeaTures), an efficient and effective method for
computing importance scores in a neural network. DeepLIFT compares the
activation of each neuron to its 'reference activation' and assigns
contribution scores according to the difference. We apply DeepLIFT to models
trained on natural images and genomic data, and show significant advantages
over gradient-based methods.},
 archiveprefix = {arXiv},
 author = {Avanti Shrikumar and Peyton Greenside and Anna Shcherbina and Anshul Kundaje},
 eprint = {1605.01713v3},
 file = {1605.01713v3.pdf},
 link = {http://arxiv.org/abs/1605.01713v3},
 month = {May},
 primaryclass = {cs.LG},
 title = {Not Just a Black Box: Learning Important Features Through Propagating
Activation Differences},
 year = {2016}
}


@article{G10wkFHt,
 abstract = {Motivation: Histone modifications are among the most important factors that
control gene regulation. Computational methods that predict gene expression
from histone modification signals are highly desirable for understanding their
combinatorial effects in gene regulation. This knowledge can help in developing
'epigenetic drugs' for diseases like cancer. Previous studies for quantifying
the relationship between histone modifications and gene expression levels
either failed to capture combinatorial effects or relied on multiple methods
that separate predictions and combinatorial analysis. This paper develops a
unified discriminative framework using a deep convolutional neural network to
classify gene expression using histone modification data as input. Our system, called DeepChrome, allows automatic extraction of complex interactions among
important features. To simultaneously visualize the combinatorial interactions
among histone modifications, we propose a novel optimization-based technique
that generates feature pattern maps from the learnt deep model. This provides
an intuitive description of underlying epigenetic mechanisms that regulate
genes. Results: We show that DeepChrome outperforms state-of-the-art models
like Support Vector Machines and Random Forests for gene expression
classification task on 56 different cell-types from REMC database. The output
of our visualization technique not only validates the previous observations but
also allows novel insights about combinatorial interactions among histone
modification marks, some of which have recently been observed by experimental
studies.},
 archiveprefix = {arXiv},
 author = {Ritambhara Singh and Jack Lanchantin and Gabriel Robins and Yanjun Qi},
 eprint = {1607.02078v1},
 file = {1607.02078v1.pdf},
 link = {http://arxiv.org/abs/1607.02078v1},
 month = {Jul},
 primaryclass = {cs.LG},
 title = {DeepChrome: Deep-learning for predicting gene expression from histone
modifications},
 year = {2016}
}


@article{81Cl5QSM,
 abstract = {Machine learning is widely used to analyze biological sequence data.
Non-sequential models such as SVMs or feed-forward neural networks are often
used although they have no natural way of handling sequences of varying length.
Recurrent neural networks such as the long short term memory (LSTM) model on
the other hand are designed to handle sequences. In this study we demonstrate
that LSTM networks predict the subcellular location of proteins given only the
protein sequence with high accuracy (0.902) outperforming current state of the
art algorithms. We further improve the performance by introducing convolutional
filters and experiment with an attention mechanism which lets the LSTM focus on
specific parts of the protein. Lastly we introduce new visualizations of both
the convolutional filters and the attention mechanisms and show how they can be
used to extract biological relevant knowledge from the LSTM networks.},
 archiveprefix = {arXiv},
 author = {Søren Kaae Sønderby and Casper Kaae Sønderby and Henrik Nielsen and Ole Winther},
 doi = {10.1007/978-3-319-21233-3_6},
 eprint = {1503.01919v1},
 file = {1503.01919v1.pdf},
 link = {http://arxiv.org/abs/1503.01919v1},
 month = {Mar},
 note = {Algorithms for Computational Biology 9199 (2015) 68},
 primaryclass = {q-bio.QM},
 title = {Convolutional LSTM Networks for Subcellular Localization of Proteins},
 year = {2015}
}


@article{aClNvbyM,
 abstract = {In this work we apply model averaging to parallel training of deep neural
network (DNN). Parallelization is done in a model averaging manner. Data is
partitioned and distributed to different nodes for local model updates, and
model averaging across nodes is done every few minibatches. We use multiple
GPUs for data parallelization, and Message Passing Interface (MPI) for
communication between nodes, which allows us to perform model averaging
frequently without losing much time on communication. We investigate the
effectiveness of Natural Gradient Stochastic Gradient Descent (NG-SGD) and
Restricted Boltzmann Machine (RBM) pretraining for parallel training in
model-averaging framework, and explore the best setups in term of different
learning rate schedules, averaging frequencies and minibatch sizes. It is shown
that NG-SGD and RBM pretraining benefits parameter-averaging based model
training. On the 300h Switchboard dataset, a 9.3 times speedup is achieved
using 16 GPUs and 17 times speedup using 32 GPUs with limited decoding accuracy
loss.},
 archiveprefix = {arXiv},
 author = {Hang Su and Haoyu Chen},
 eprint = {1507.01239v2},
 file = {1507.01239v2.pdf},
 link = {http://arxiv.org/abs/1507.01239v2},
 month = {Jul},
 primaryclass = {cs.LG},
 title = {Experiments on Parallel Training of Deep Neural Network using Model
Averaging},
 year = {2015}
}


@article{JUF9VoRD,
 abstract = {In recent year, parallel implementations have been used to speed up the
training of deep neural networks (DNN). Typically, the parameters of the local
models are periodically communicated and averaged to get a global model until
the training curve converges (denoted as MA-DNN). However, since DNN is a
highly non-convex model, the global model obtained by averaging parameters does
not have guarantee on its performance improvement over the local models and
might even be worse than the average performance of the local models, which
leads to the slow-down of convergence and the decrease of the final
performance. To tackle this problem, we propose a new parallel training method
called \emph{Ensemble-Compression} (denoted as EC-DNN). Specifically, we
propose to aggregate the local models by ensemble, i.e., the outputs of the
local models are averaged instead of the parameters. Considering that the
widely used loss functions are convex to the output of the model, the
performance of the global model obtained in this way is guaranteed to be at
least as good as the average performance of local models. However, the size of
the global model will increase after each ensemble and may explode after
multiple rounds of ensembles. Thus, we conduct model compression after each
ensemble, to ensure the size of the global model to be the same as the local
models. We conducted experiments on a benchmark dataset. The experimental
results demonstrate that our proposed EC-DNN can stably achieve better
performance than MA-DNN.},
 archiveprefix = {arXiv},
 author = {Shizhao Sun and Wei Chen and Tie-Yan Liu},
 eprint = {1606.00575v1},
 file = {1606.00575v1.pdf},
 link = {http://arxiv.org/abs/1606.00575v1},
 month = {Jun},
 primaryclass = {cs.DC},
 title = {Ensemble-Compression: A New Method for Parallel Training of Deep Neural
Networks},
 year = {2016}
}


@article{2cMhMv5A,
 abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent
performance on difficult learning tasks. Although DNNs work well whenever large
labeled training sets are available, they cannot be used to map sequences to
sequences. In this paper, we present a general end-to-end approach to sequence
learning that makes minimal assumptions on the sequence structure. Our method
uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to
a vector of a fixed dimensionality, and then another deep LSTM to decode the
target sequence from the vector. Our main result is that on an English to
French translation task from the WMT'14 dataset, the translations produced by
the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's
BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did
not have difficulty on long sentences. For comparison, a phrase-based SMT
system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM
to rerank the 1000 hypotheses produced by the aforementioned SMT system, its
BLEU score increases to 36.5, which is close to the previous best result on
this task. The LSTM also learned sensible phrase and sentence representations
that are sensitive to word order and are relatively invariant to the active and
the passive voice. Finally, we found that reversing the order of the words in
all source sentences (but not target sentences) improved the LSTM's performance
markedly, because doing so introduced many short term dependencies between the
source and the target sentence which made the optimization problem easier.},
 archiveprefix = {arXiv},
 author = {Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
 eprint = {1409.3215v3},
 file = {1409.3215v3.pdf},
 link = {http://arxiv.org/abs/1409.3215v3},
 month = {Sep},
 primaryclass = {cs.CL},
 title = {Sequence to Sequence Learning with Neural Networks},
 year = {2014}
}


@article{1GhHIDxuW,
 abstract = {We propose two novel model architectures for computing continuous vector
representations of words from very large data sets. The quality of these
representations is measured in a word similarity task, and the results are
compared to the previously best performing techniques based on different types
of neural networks. We observe large improvements in accuracy at much lower
computational cost, i.e. it takes less than a day to learn high quality word
vectors from a 1.6 billion words data set. Furthermore, we show that these
vectors provide state-of-the-art performance on our test set for measuring
syntactic and semantic word similarities.},
 archiveprefix = {arXiv},
 author = {Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
 eprint = {1301.3781v3},
 file = {1301.3781v3.pdf},
 link = {http://arxiv.org/abs/1301.3781v3},
 month = {Jan},
 primaryclass = {cs.CL},
 title = {Efficient Estimation of Word Representations in Vector Space},
 year = {2013}
}


@article{1AkF8Wsv7,
 abstract = {Deep neural networks (DNNs) have recently been achieving state-of-the-art
performance on a variety of pattern-recognition tasks, most notably visual
classification problems. Given that DNNs are now able to classify objects in
images with near-human-level performance, questions naturally arise as to what
differences remain between computer and human vision. A recent study revealed
that changing an image (e.g. of a lion) in a way imperceptible to humans can
cause a DNN to label the image as something else entirely (e.g. mislabeling a
lion a library). Here we show a related result: it is easy to produce images
that are completely unrecognizable to humans, but that state-of-the-art DNNs
believe to be recognizable objects with 99.99% confidence (e.g. labeling with
certainty that white noise static is a lion). Specifically, we take
convolutional neural networks trained to perform well on either the ImageNet or
MNIST datasets and then find images with evolutionary algorithms or gradient
ascent that DNNs label with high confidence as belonging to each dataset class.
It is possible to produce images totally unrecognizable to human eyes that DNNs
believe with near certainty are familiar objects, which we call "fooling
images" (more generally, fooling examples). Our results shed light on
interesting differences between human vision and current DNNs, and raise
questions about the generality of DNN computer vision.},
 archiveprefix = {arXiv},
 author = {Anh Nguyen and Jason Yosinski and Jeff Clune},
 eprint = {1412.1897v4},
 file = {1412.1897v4.pdf},
 link = {http://arxiv.org/abs/1412.1897v4},
 month = {12},
 primaryclass = {cs.CV},
 title = {Deep Neural Networks are Easily Fooled: High Confidence Predictions for
Unrecognizable Images},
 year = {2014}
}


@article{glyI7H6F,
 abstract = {We present a novel application of LSTM recurrent neural networks to
multilabel classification of diagnoses given variable-length time series of
clinical measurements. Our method outperforms a strong baseline on a variety of
metrics.},
 archiveprefix = {arXiv},
 author = {Zachary C. Lipton and David C. Kale and Randall C. Wetzel},
 eprint = {1510.07641v2},
 file = {1510.07641v2.pdf},
 link = {http://arxiv.org/abs/1510.07641v2},
 month = {Nov},
 primaryclass = {cs.LG},
 title = {Phenotyping of Clinical Time Series with LSTM Recurrent Neural Networks},
 year = {2015}
}


@article{14DAmZTDg,
 abstract = {Exponential growth in Electronic Healthcare Records (EHR) has resulted in new
opportunities and urgent needs for discovery of meaningful data-driven
representations and patterns of diseases in Computational Phenotyping research.
Deep Learning models have shown superior performance for robust prediction in
computational phenotyping tasks, but suffer from the issue of model
interpretability which is crucial for clinicians involved in decision-making.
In this paper, we introduce a novel knowledge-distillation approach called
Interpretable Mimic Learning, to learn interpretable phenotype features for
making robust prediction while mimicking the performance of deep learning
models. Our framework uses Gradient Boosting Trees to learn interpretable
features from deep learning models such as Stacked Denoising Autoencoder and
Long Short-Term Memory. Exhaustive experiments on a real-world clinical
time-series dataset show that our method obtains similar or better performance
than the deep learning models, and it provides interpretable phenotypes for
clinical decision making.},
 archiveprefix = {arXiv},
 author = {Zhengping Che and Sanjay Purushotham and Robinder Khemani and Yan Liu},
 eprint = {1512.03542v1},
 file = {1512.03542v1.pdf},
 link = {http://arxiv.org/abs/1512.03542v1},
 month = {12},
 primaryclass = {stat.ML},
 title = {Distilling Knowledge from Deep Networks with Applications to Healthcare
Domain},
 year = {2015}
}


@article{QwXSJhr0,
 abstract = {Despite widespread adoption, machine learning models remain mostly black
boxes. Understanding the reasons behind predictions is, however, quite
important in assessing trust, which is fundamental if one plans to take action
based on a prediction, or when choosing whether to deploy a new model. Such
understanding also provides insights into the model, which can be used to
transform an untrustworthy model or prediction into a trustworthy one. In this
work, we propose LIME, a novel explanation technique that explains the
predictions of any classifier in an interpretable and faithful manner, by
learning an interpretable model locally around the prediction. We also propose
a method to explain models by presenting representative individual predictions
and their explanations in a non-redundant way, framing the task as a submodular
optimization problem. We demonstrate the flexibility of these methods by
explaining different models for text (e.g. random forests) and image
classification (e.g. neural networks). We show the utility of explanations via
novel experiments, both simulated and with human subjects, on various scenarios
that require trust: deciding if one should trust a prediction, choosing between
models, improving an untrustworthy classifier, and identifying why a classifier
should not be trusted.},
 archiveprefix = {arXiv},
 author = {Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin},
 eprint = {1602.04938v3},
 file = {1602.04938v3.pdf},
 link = {http://arxiv.org/abs/1602.04938v3},
 month = {Feb},
 primaryclass = {cs.LG},
 title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
 year = {2016}
}


@article{O7Vbecm2,
 abstract = {Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In
time series prediction and other related tasks, it has been noted that missing
values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the
missing patterns for effective imputation and improving prediction performance.
In this paper, we develop novel deep learning models, namely GRU-D, as one of
the early attempts. GRU-D is based on Gated Recurrent Unit (GRU), a
state-of-the-art recurrent neural network. It takes two representations of
missing patterns, i.e., masking and time interval, and effectively incorporates
them into a deep model architecture so that it not only captures the long-term
temporal dependencies in time series, but also utilizes the missing patterns to
achieve better prediction results. Experiments of time series classification
tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic
datasets demonstrate that our models achieve state-of-the-art performance and
provides useful insights for better understanding and utilization of missing
values in time series analysis.},
 archiveprefix = {arXiv},
 author = {Zhengping Che and Sanjay Purushotham and Kyunghyun Cho and David Sontag and Yan Liu},
 eprint = {1606.01865v2},
 file = {1606.01865v2.pdf},
 link = {http://arxiv.org/abs/1606.01865v2},
 month = {Jun},
 primaryclass = {cs.LG},
 title = {Recurrent Neural Networks for Multivariate Time Series with Missing
Values},
 year = {2016}
}


@article{4zpZxjHR,
 abstract = {We demonstrate a simple strategy to cope with missing data in sequential
inputs, addressing the task of multilabel classification of diagnoses given
clinical time series. Collected from the pediatric intensive care unit (PICU)
at Children's Hospital Los Angeles, our data consists of multivariate time
series of observations. The measurements are irregularly spaced, leading to
missingness patterns in temporally discretized sequences. While these artifacts
are typically handled by imputation, we achieve superior predictive performance
by treating the artifacts as features. Unlike linear models, recurrent neural
networks can realize this improvement using only simple binary indicators of
missingness. For linear models, we show an alternative strategy to capture this
signal. Training models on missingness patterns only, we show that for some
diseases, what tests are run can be as predictive as the results themselves.},
 archiveprefix = {arXiv},
 author = {Zachary C. Lipton and David C. Kale and Randall Wetzel},
 eprint = {1606.04130v5},
 file = {1606.04130v5.pdf},
 link = {http://arxiv.org/abs/1606.04130v5},
 month = {Jun},
 primaryclass = {cs.LG},
 title = {Modeling Missing Data in Clinical Time Series with RNNs},
 year = {2016}
}


@article{UcRbawKo,
 abstract = {Accuracy and interpretability are two dominant features of successful
predictive models. Typically, a choice must be made in favor of complex black
box models such as recurrent neural networks (RNN) for accuracy versus less
accurate but more interpretable traditional models such as logistic regression.
This tradeoff poses challenges in medicine where both accuracy and
interpretability are important. We addressed this challenge by developing the
REverse Time AttentIoN model (RETAIN) for application to Electronic Health
Records (EHR) data. RETAIN achieves high accuracy while remaining clinically
interpretable and is based on a two-level neural attention model that detects
influential past visits and significant clinical variables within those visits
(e.g. key diagnoses). RETAIN mimics physician practice by attending the EHR
data in a reverse time order so that recent clinical visits are likely to
receive higher attention. RETAIN was tested on a large health system EHR
dataset with 14 million visits completed by 263K patients over an 8 year period
and demonstrated predictive accuracy and computational scalability comparable
to state-of-the-art methods such as RNN, and ease of interpretability
comparable to traditional models.},
 archiveprefix = {arXiv},
 author = {Edward Choi and Mohammad Taha Bahadori and Joshua A. Kulas and Andy Schuetz and Walter F. Stewart and Jimeng Sun},
 eprint = {1608.05745v4},
 file = {1608.05745v4.pdf},
 link = {http://arxiv.org/abs/1608.05745v4},
 month = {Aug},
 primaryclass = {cs.LG},
 title = {RETAIN: An Interpretable Predictive Model for Healthcare using Reverse
Time Attention Mechanism},
 year = {2016}
}


@article{10nDTiETi,
 abstract = {Deep learning methods exhibit promising performance for predictive modeling
in healthcare, but two important challenges remain: -Data insufficiency:Often
in healthcare predictive modeling, the sample size is insufficient for deep
learning methods to achieve satisfactory results. -Interpretation:The
representations learned by deep learning methods should align with medical
knowledge. To address these challenges, we propose a GRaph-based Attention
Model, GRAM that supplements electronic health records (EHR) with hierarchical
information inherent to medical ontologies. Based on the data volume and the
ontology structure, GRAM represents a medical concept as a combination of its
ancestors in the ontology via an attention mechanism. We compared predictive
performance (i.e. accuracy, data needs, interpretability) of GRAM to various
methods including the recurrent neural network (RNN) in two sequential
diagnoses prediction tasks and one heart failure prediction task. Compared to
the basic RNN, GRAM achieved 10% higher accuracy for predicting diseases rarely
observed in the training data and 3% improved area under the ROC curve for
predicting heart failure using an order of magnitude less training data.
Additionally, unlike other methods, the medical concept representations learned
by GRAM are well aligned with the medical ontology. Finally, GRAM exhibits
intuitive attention behaviors by adaptively generalizing to higher level
concepts when facing data insufficiency at the lower level concepts.},
 archiveprefix = {arXiv},
 author = {Edward Choi and Mohammad Taha Bahadori and Le Song and Walter F. Stewart and Jimeng Sun},
 eprint = {1611.07012v3},
 file = {1611.07012v3.pdf},
 link = {http://arxiv.org/abs/1611.07012v3},
 month = {Dec},
 primaryclass = {cs.LG},
 title = {GRAM: Graph-based Attention Model for Healthcare Representation Learning},
 year = {2016}
}


@article{y4t9EzPn,
 abstract = {As machine learning algorithms are increasingly applied to high impact yet
high risk tasks, e.g. problems in health, it is critical that researchers can
explain how such algorithms arrived at their predictions. In recent years, a
number of image saliency methods have been developed to summarize where highly
complex neural networks "look" in an image for evidence for their predictions.
However, these techniques are limited by their heuristic nature and
architectural constraints.
In this paper, we make two main contributions: First, we propose a general
framework for learning different kinds of explanations for any black box
algorithm. Second, we introduce a paradigm that learns the minimally salient
part of an image by directly editing it and learning from the corresponding
changes to its output. Unlike previous works, our method is model-agnostic and
testable because it is grounded in replicable image perturbations.},
 archiveprefix = {arXiv},
 author = {Ruth Fong and Andrea Vedaldi},
 eprint = {1704.03296v1},
 file = {1704.03296v1.pdf},
 link = {http://arxiv.org/abs/1704.03296v1},
 month = {Apr},
 primaryclass = {cs.CV},
 title = {Interpretable Explanations of Black Boxes by Meaningful Perturbation},
 year = {2017}
}


@article{8RAYEOPl,
 abstract = {We develop stochastic variational inference, a scalable algorithm for
approximating posterior distributions. We develop this technique for a large
class of probabilistic models and we demonstrate it with two probabilistic
topic models, latent Dirichlet allocation and the hierarchical Dirichlet
process topic model. Using stochastic variational inference, we analyze several
large collections of documents: 300K articles from Nature, 1.8M articles from
The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can
easily handle data sets of this size and outperforms traditional variational
inference, which can only handle a smaller subset. (We also show that the
Bayesian nonparametric topic model outperforms its parametric counterpart.)
Stochastic variational inference lets us apply complex Bayesian models to
massive data sets.},
 archiveprefix = {arXiv},
 author = {Matt Hoffman and David M. Blei and Chong Wang and John Paisley},
 eprint = {1206.7051v3},
 file = {1206.7051v3.pdf},
 link = {http://arxiv.org/abs/1206.7051v3},
 month = {Jun},
 primaryclass = {stat.ML},
 title = {Stochastic Variational Inference},
 year = {2012}
}


@article{15lbUf0as,
 abstract = {Black box variational inference allows researchers to easily prototype and
evaluate an array of models. Recent advances allow such algorithms to scale to
high dimensions. However, a central question remains: How to specify an
expressive variational distribution that maintains efficient computation? To
address this, we develop hierarchical variational models (HVMs). HVMs augment a
variational approximation with a prior on its parameters, which allows it to
capture complex structure for both discrete and continuous latent variables.
The algorithm we develop is black box, can be used for any HVM, and has the
same computational efficiency as the original approximation. We study HVMs on a
variety of deep discrete latent variable models. HVMs generalize other
expressive variational distributions and maintains higher fidelity to the
posterior.},
 archiveprefix = {arXiv},
 author = {Rajesh Ranganath and Dustin Tran and David M. Blei},
 eprint = {1511.02386v2},
 file = {1511.02386v2.pdf},
 link = {http://arxiv.org/abs/1511.02386v2},
 month = {Dec},
 primaryclass = {stat.ML},
 title = {Hierarchical Variational Models},
 year = {2015}
}


@article{TaPZBxYS,
 abstract = {Modern mobile devices have access to a wealth of data suitable for learning
models, which in turn can greatly improve the user experience on the device.
For example, language models can improve speech recognition and text entry, and
image models can automatically select good photos. However, this rich data is
often privacy sensitive, large in quantity, or both, which may preclude logging
to the data center and training there using conventional approaches. We
advocate an alternative that leaves the training data distributed on the mobile
devices, and learns a shared model by aggregating locally-computed updates. We
term this decentralized approach Federated Learning.
We present a practical method for the federated learning of deep networks
based on iterative model averaging, and conduct an extensive empirical
evaluation, considering five different model architectures and four datasets.
These experiments demonstrate the approach is robust to the unbalanced and
non-IID data distributions that are a defining characteristic of this setting.
Communication costs are the principal constraint, and we show a reduction in
required communication rounds by 10-100x as compared to synchronized stochastic
gradient descent.},
 archiveprefix = {arXiv},
 author = {H. Brendan McMahan and Eider Moore and Daniel Ramage and Seth Hampson and Blaise Agüera y Arcas},
 eprint = {1602.05629v3},
 file = {1602.05629v3.pdf},
 link = {http://arxiv.org/abs/1602.05629v3},
 month = {Feb},
 note = {Proceedings of the 20 th International Conference on Artificial
Intelligence and Statistics (AISTATS) 2017. JMLR: W&CP volume 54},
 primaryclass = {cs.LG},
 title = {Communication-Efficient Learning of Deep Networks from Decentralized
Data},
 year = {2016}
}


@article{1HbRTExaU,
 abstract = {We quantitatively investigate how machine learning models leak information
about the individual data records on which they were trained. We focus on the
basic membership inference attack: given a data record and black-box access to
a model, determine if the record was in the model's training dataset. To
perform membership inference against a target model, we make adversarial use of
machine learning and train our own inference model to recognize differences in
the target model's predictions on the inputs that it trained on versus the
inputs that it did not train on.
We empirically evaluate our inference techniques on classification models
trained by commercial "machine learning as a service" providers such as Google
and Amazon. Using realistic datasets and classification tasks, including a
hospital discharge dataset whose membership is sensitive from the privacy
perspective, we show that these models can be vulnerable to membership
inference attacks. We then investigate the factors that influence this leakage
and evaluate mitigation strategies.},
 archiveprefix = {arXiv},
 author = {Reza Shokri and Marco Stronati and Congzheng Song and Vitaly Shmatikov},
 eprint = {1610.05820v2},
 file = {1610.05820v2.pdf},
 link = {http://arxiv.org/abs/1610.05820v2},
 month = {Nov},
 primaryclass = {cs.CR},
 title = {Membership Inference Attacks against Machine Learning Models},
 year = {2016}
}


@article{17YaKNLKk,
 abstract = {Empirical scoring functions based on either molecular force fields or
cheminformatics descriptors are widely used, in conjunction with molecular
docking, during the early stages of drug discovery to predict potency and
binding affinity of a drug-like molecule to a given target. These models
require expert-level knowledge of physical chemistry and biology to be encoded
as hand-tuned parameters or features rather than allowing the underlying model
to select features in a data-driven procedure. Here, we develop a general
3-dimensional spatial convolution operation for learning atomic-level chemical
interactions directly from atomic coordinates and demonstrate its application
to structure-based bioactivity prediction. The atomic convolutional neural
network is trained to predict the experimentally determined binding affinity of
a protein-ligand complex by direct calculation of the energy associated with
the complex, protein, and ligand given the crystal structure of the binding
pose. Non-covalent interactions present in the complex that are absent in the
protein-ligand sub-structures are identified and the model learns the
interaction strength associated with these features. We test our model by
predicting the binding free energy of a subset of protein-ligand complexes
found in the PDBBind dataset and compare with state-of-the-art cheminformatics
and machine learning-based approaches. We find that all methods achieve
experimental accuracy and that atomic convolutional networks either outperform
or perform competitively with the cheminformatics based methods. Unlike all
previous protein-ligand prediction systems, atomic convolutional networks are
end-to-end and fully-differentiable. They represent a new data-driven, physics-based deep learning model paradigm that offers a strong foundation for
future improvements in structure-based bioactivity prediction.},
 archiveprefix = {arXiv},
 author = {Joseph Gomes and Bharath Ramsundar and Evan N. Feinberg and Vijay S. Pande},
 eprint = {1703.10603v1},
 file = {1703.10603v1.pdf},
 link = {http://arxiv.org/abs/1703.10603v1},
 month = {Mar},
 primaryclass = {cs.LG},
 title = {Atomic Convolutional Networks for Predicting Protein-Ligand Binding
Affinity},
 year = {2017}
}


@article{1Dzz0P0qr,
 abstract = {Although artificial neural networks have occasionally been used for
Quantitative Structure-Activity/Property Relationship (QSAR/QSPR) studies in
the past, the literature has of late been dominated by other machine learning
techniques such as random forests. However, a variety of new neural net
techniques along with successful applications in other domains have renewed
interest in network approaches. In this work, inspired by the winning team's
use of neural networks in a recent QSAR competition, we used an artificial
neural network to learn a function that predicts activities of compounds for
multiple assays at the same time. We conducted experiments leveraging recent
methods for dealing with overfitting in neural networks as well as other tricks
from the neural networks literature. We compared our methods to alternative
methods reported to perform well on these tasks and found that our neural net
methods provided superior performance.},
 archiveprefix = {arXiv},
 author = {George E. Dahl and Navdeep Jaitly and Ruslan Salakhutdinov},
 eprint = {1406.1231v1},
 file = {1406.1231v1.pdf},
 link = {http://arxiv.org/abs/1406.1231v1},
 month = {Jun},
 primaryclass = {stat.ML},
 title = {Multi-task Neural Networks for QSAR Predictions},
 year = {2014}
}


@article{yAoN5gTU,
 abstract = {Massively multitask neural architectures provide a learning framework for
drug discovery that synthesizes information from many distinct biological
sources. To train these architectures at scale, we gather large amounts of data
from public sources to create a dataset of nearly 40 million measurements
across more than 200 biological targets. We investigate several aspects of the
multitask framework by performing a series of empirical studies and obtain some
interesting results: (1) massively multitask networks obtain predictive
accuracies significantly better than single-task methods, (2) the predictive
power of multitask networks improves as additional tasks and data are added, (3) the total amount of data and the total number of tasks both contribute
significantly to multitask improvement, and (4) multitask networks afford
limited transferability to tasks not in the training set. Our results
underscore the need for greater data sharing and further algorithmic innovation
to accelerate the drug discovery process.},
 archiveprefix = {arXiv},
 author = {Bharath Ramsundar and Steven Kearnes and Patrick Riley and Dale Webster and David Konerding and Vijay Pande},
 eprint = {1502.02072v1},
 file = {1502.02072v1.pdf},
 link = {http://arxiv.org/abs/1502.02072v1},
 month = {Feb},
 primaryclass = {stat.ML},
 title = {Massively Multitask Networks for Drug Discovery},
 year = {2015}
}


@article{16OPHvAij,
 abstract = {Molecular machine learning has been maturing rapidly over the last few years.
Improved methods and the presence of larger datasets have enabled machine
learning algorithms to make increasingly accurate predictions about molecular
properties. However, algorithmic progress has been limited due to the lack of a
standard benchmark to compare the efficacy of proposed methods; most new
algorithms are benchmarked on different datasets making it challenging to gauge
the quality of proposed methods. This work introduces MoleculeNet, a large
scale benchmark for molecular machine learning. MoleculeNet curates multiple
public datasets, establishes metrics for evaluation, and offers high quality
open-source implementations of multiple previously proposed molecular
featurization and learning algorithms (released as part of the DeepChem open
source library). MoleculeNet benchmarks demonstrate that learnable
representations, and in particular graph convolutional networks, are powerful
tools for molecular machine learning and broadly offer the best performance.
However, for quantum mechanical and biophysical datasets, the use of
physics-aware featurizations can be significantly more important than choice of
particular learning algorithm.},
 archiveprefix = {arXiv},
 author = {Zhenqin Wu and Bharath Ramsundar and Evan N. Feinberg and Joseph Gomes and Caleb Geniesse and Aneesh S. Pappu and Karl Leswing and Vijay Pande},
 eprint = {1703.00564v1},
 file = {1703.00564v1.pdf},
 link = {http://arxiv.org/abs/1703.00564v1},
 month = {Mar},
 primaryclass = {cs.LG},
 title = {MoleculeNet: A Benchmark for Molecular Machine Learning},
 year = {2017}
}


@article{xl1ijigK,
 abstract = {Access to electronic health records (EHR) data has motivated computational
advances in medical research. However, various concerns, particularly over
privacy, can limit access to and collaborative use of EHR data. Sharing
synthetic EHR data could mitigate risk. In this paper, we propose a new
approach, medical Generative Adversarial Network (medGAN), to generate
realistic synthetic EHRs. Based on an input EHR dataset, medGAN can generate
high-dimensional discrete variables (e.g., binary and count features) via a
combination of an autoencoder and generative adversarial networks. We also
propose minibatch averaging to efficiently avoid mode collapse, and increase
the learning efficiency with batch normalization and shortcut connections. To
demonstrate feasibility, we showed that medGAN generates synthetic EHR datasets
that achieve comparable performance to real data on many experiments including
distribution statistics, predictive modeling tasks and medical expert review.},
 archiveprefix = {arXiv},
 author = {Edward Choi and Siddharth Biswal and Bradley Malin and Jon Duke and Walter F. Stewart and Jimeng Sun},
 eprint = {1703.06490v1},
 file = {1703.06490v1.pdf},
 link = {http://arxiv.org/abs/1703.06490v1},
 month = {Mar},
 primaryclass = {cs.LG},
 title = {Generating Multi-label Discrete Electronic Health Records using
Generative Adversarial Networks},
 year = {2017}
}


@article{173ftiSzF,
 abstract = {Observational studies are rising in importance due to the widespread
accumulation of data in fields such as healthcare, education, employment and
ecology. We consider the task of answering counterfactual questions such as, "Would this patient have lower blood sugar had she received a different
medication?". We propose a new algorithmic framework for counterfactual
inference which brings together ideas from domain adaptation and representation
learning. In addition to a theoretical justification, we perform an empirical
comparison with previous approaches to causal inference from observational
data. Our deep learning algorithm significantly outperforms the previous
state-of-the-art.},
 archiveprefix = {arXiv},
 author = {Fredrik D. Johansson and Uri Shalit and David Sontag},
 eprint = {1605.03661v2},
 file = {1605.03661v2.pdf},
 link = {http://arxiv.org/abs/1605.03661v2},
 month = {May},
 primaryclass = {stat.ML},
 title = {Learning Representations for Counterfactual Inference},
 year = {2016}
}


@article{c6MfDdWP,
 abstract = {Disparate areas of machine learning have benefited from models that can take
raw data with little preprocessing as input and learn rich representations of
that raw data in order to perform well on a given prediction task. We evaluate
this approach in healthcare by using longitudinal measurements of lab tests, one of the more raw signals of a patient's health state widely available in
clinical data, to predict disease onsets. In particular, we train a Long
Short-Term Memory (LSTM) recurrent neural network and two novel convolutional
neural networks for multi-task prediction of disease onset for 133 conditions
based on 18 common lab tests measured over time in a cohort of 298K patients
derived from 8 years of administrative claims data. We compare the neural
networks to a logistic regression with several hand-engineered, clinically
relevant features. We find that the representation-based learning approaches
significantly outperform this baseline. We believe that our work suggests a new
avenue for patient risk stratification based solely on lab results.},
 archiveprefix = {arXiv},
 author = {Narges Razavian and Jake Marcus and David Sontag},
 eprint = {1608.00647v3},
 file = {1608.00647v3.pdf},
 link = {http://arxiv.org/abs/1608.00647v3},
 month = {Aug},
 primaryclass = {cs.LG},
 title = {Multi-task Prediction of Disease Onsets from Longitudinal Lab Tests},
 year = {2016}
}


@article{1GwC1ll6h,
 abstract = {MicroRNAs (miRNAs) are short sequences of ribonucleic acids that control the
expression of target messenger RNAs (mRNAs) by binding them. Robust prediction
of miRNA-mRNA pairs is of utmost importance in deciphering gene regulations but
has been challenging because of high false positive rates, despite a deluge of
computational tools that normally require laborious manual feature extraction.
This paper presents an end-to-end machine learning framework for miRNA target
prediction. Leveraged by deep recurrent neural networks-based auto-encoding and
sequence-sequence interaction learning, our approach not only delivers an
unprecedented level of accuracy but also eliminates the need for manual feature
extraction. The performance gap between the proposed method and existing
alternatives is substantial (over 25% increase in F-measure), and deepTarget
delivers a quantum leap in the long-standing challenge of robust miRNA target
prediction.},
 archiveprefix = {arXiv},
 author = {Byunghan Lee and Junghwan Baek and Seunghyun Park and Sungroh Yoon},
 eprint = {1603.09123v2},
 file = {1603.09123v2.pdf},
 link = {http://arxiv.org/abs/1603.09123v2},
 month = {Mar},
 primaryclass = {cs.LG},
 title = {deepTarget: End-to-end Learning Framework for microRNA Target Prediction
using Deep Recurrent Neural Networks},
 year = {2016}
}


@article{1TeyWffV,
 abstract = {Since microRNAs (miRNAs) play a crucial role in post-transcriptional gene
regulation, miRNA identification is one of the most essential problems in
computational biology. miRNAs are usually short in length ranging between 20
and 23 base pairs. It is thus often difficult to distinguish miRNA-encoding
sequences from other non-coding RNAs and pseudo miRNAs that have a similar
length, and most previous studies have recommended using precursor miRNAs
instead of mature miRNAs for robust detection. A great number of conventional
machine-learning-based classification methods have been proposed, but they
often have the serious disadvantage of requiring manual feature engineering, and their performance is limited as well. In this paper, we propose a novel
miRNA precursor prediction algorithm, deepMiRGene, based on recurrent neural
networks, specifically long short-term memory networks. deepMiRGene
automatically learns suitable features from the data themselves without manual
feature engineering and constructs a model that can successfully reflect
structural characteristics of precursor miRNAs. For the performance evaluation
of our approach, we have employed several widely used evaluation metrics on
three recent benchmark datasets and verified that deepMiRGene delivered
comparable performance among the current state-of-the-art tools.},
 archiveprefix = {arXiv},
 author = {Seunghyun Park and Seonwoo Min and Hyunsoo Choi and Sungroh Yoon},
 eprint = {1605.00017v1},
 file = {1605.00017v1.pdf},
 link = {http://arxiv.org/abs/1605.00017v1},
 month = {Apr},
 primaryclass = {cs.LG},
 title = {deepMiRGene: Deep Neural Network based Precursor microRNA Prediction},
 year = {2016}
}


@article{VMkPJjVk,
 abstract = {We present an interpretation of Inception modules in convolutional neural
networks as being an intermediate step in-between regular convolution and the
depthwise separable convolution operation (a depthwise convolution followed by
a pointwise convolution). In this light, a depthwise separable convolution can
be understood as an Inception module with a maximally large number of towers.
This observation leads us to propose a novel deep convolutional neural network
architecture inspired by Inception, where Inception modules have been replaced
with depthwise separable convolutions. We show that this architecture, dubbed
Xception, slightly outperforms Inception V3 on the ImageNet dataset (which
Inception V3 was designed for), and significantly outperforms Inception V3 on a
larger image classification dataset comprising 350 million images and 17,000
classes. Since the Xception architecture has the same number of parameters as
Inception V3, the performance gains are not due to increased capacity but
rather to a more efficient use of model parameters.},
 archiveprefix = {arXiv},
 author = {François Chollet},
 eprint = {1610.02357v3},
 file = {1610.02357v3.pdf},
 link = {http://arxiv.org/abs/1610.02357v3},
 month = {Nov},
 primaryclass = {cs.CV},
 title = {Xception: Deep Learning with Depthwise Separable Convolutions},
 year = {2016}
}


@article{M2OLWojE,
 abstract = {Conversational speech recognition has served as a flagship speech recognition
task since the release of the Switchboard corpus in the 1990s. In this paper, we measure the human error rate on the widely used NIST 2000 test set, and find
that our latest automated system has reached human parity. The error rate of
professional transcribers is 5.9% for the Switchboard portion of the data, in
which newly acquainted pairs of people discuss an assigned topic, and 11.3% for
the CallHome portion where friends and family members have open-ended
conversations. In both cases, our automated system establishes a new state of
the art, and edges past the human benchmark, achieving error rates of 5.8% and
11.0%, respectively. The key to our system's performance is the use of various
convolutional and LSTM acoustic model architectures, combined with a novel
spatial smoothing method and lattice-free MMI acoustic training, multiple
recurrent neural network language modeling approaches, and a systematic use of
system combination.},
 archiveprefix = {arXiv},
 author = {W. Xiong and J. Droppo and X. Huang and F. Seide and M. Seltzer and A. Stolcke and D. Yu and G. Zweig},
 eprint = {1610.05256v2},
 file = {1610.05256v2.pdf},
 link = {http://arxiv.org/abs/1610.05256v2},
 month = {Nov},
 primaryclass = {cs.CL},
 title = {Achieving Human Parity in Conversational Speech Recognition},
 year = {2016}
}


@article{wKioubsT,
 abstract = {One of the most difficult speech recognition tasks is accurate recognition of
human to human communication. Advances in deep learning over the last few years
have produced major speech recognition improvements on the representative
Switchboard conversational corpus. Word error rates that just a few years ago
were 14% have dropped to 8.0%, then 6.6% and most recently 5.8%, and are now
believed to be within striking range of human performance. This then raises two
issues - what IS human performance, and how far down can we still drive speech
recognition error rates? A recent paper by Microsoft suggests that we have
already achieved human performance. In trying to verify this statement, we
performed an independent set of human performance measurements on two
conversational tasks and found that human performance may be considerably
better than what was earlier reported, giving the community a significantly
harder goal to achieve. We also report on our own efforts in this area, presenting a set of acoustic and language modeling techniques that lowered the
word error rate of our own English conversational telephone LVCSR system to the
level of 5.5%/10.3% on the Switchboard/CallHome subsets of the Hub5 2000
evaluation, which - at least at the writing of this paper - is a new
performance milestone (albeit not at what we measure to be human performance!).
On the acoustic side, we use a score fusion of three models: one LSTM with
multiple feature inputs, a second LSTM trained with speaker-adversarial
multi-task learning and a third residual net (ResNet) with 25 convolutional
layers and time-dilated convolutions. On the language modeling side, we use
word and character LSTMs and convolutional WaveNet-style language models.},
 archiveprefix = {arXiv},
 author = {George Saon and Gakuto Kurata and Tom Sercu and Kartik Audhkhasi and Samuel Thomas and Dimitrios Dimitriadis and Xiaodong Cui and Bhuvana Ramabhadran and Michael Picheny and Lynn-Li Lim and Bergul Roomi and Phil Hall},
 eprint = {1703.02136v1},
 file = {1703.02136v1.pdf},
 link = {http://arxiv.org/abs/1703.02136v1},
 month = {Mar},
 primaryclass = {cs.CL},
 title = {English Conversational Telephone Speech Recognition by Humans and
Machines},
 year = {2017}
}


@article{3qm8sXnB,
 abstract = {Stochastic Gradient Descent (SGD) is a popular algorithm that can achieve
state-of-the-art performance on a variety of machine learning tasks. Several
researchers have recently proposed schemes to parallelize SGD, but all require
performance-destroying memory locking and synchronization. This work aims to
show using novel theoretical analysis, algorithms, and implementation that SGD
can be implemented without any locking. We present an update scheme called
HOGWILD! which allows processors access to shared memory with the possibility
of overwriting each other's work. We show that when the associated optimization
problem is sparse, meaning most gradient updates only modify small parts of the
decision variable, then HOGWILD! achieves a nearly optimal rate of convergence.
We demonstrate experimentally that HOGWILD! outperforms alternative schemes
that use locking by an order of magnitude.},
 archiveprefix = {arXiv},
 author = {Feng Niu and Benjamin Recht and Christopher Re and Stephen J. Wright},
 eprint = {1106.5730v2},
 file = {1106.5730v2.pdf},
 link = {http://arxiv.org/abs/1106.5730v2},
 month = {Jun},
 primaryclass = {math.OC},
 title = {HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient
Descent},
 year = {2011}
}


@article{sLPsrfbl,
 abstract = {Melanoma is the deadliest form of skin cancer. While curable with early
detection, only highly trained specialists are capable of accurately
recognizing the disease. As expertise is in limited supply, automated systems
capable of identifying disease could save lives, reduce unnecessary biopsies, and reduce costs. Toward this goal, we propose a system that combines recent
developments in deep learning with established machine learning approaches, creating ensembles of methods that are capable of segmenting skin lesions, as
well as analyzing the detected area and surrounding tissue for melanoma
detection. The system is evaluated using the largest publicly available
benchmark dataset of dermoscopic images, containing 900 training and 379
testing images. New state-of-the-art performance levels are demonstrated, leading to an improvement in the area under receiver operating characteristic
curve of 7.5% (0.843 vs. 0.783), in average precision of 4% (0.649 vs. 0.624), and in specificity measured at the clinically relevant 95% sensitivity
operating point 2.9 times higher than the previous state-of-the-art (36.8%
specificity compared to 12.5%). Compared to the average of 8 expert
dermatologists on a subset of 100 test images, the proposed system produces a
higher accuracy (76% vs. 70.5%), and specificity (62% vs. 59%) evaluated at an
equivalent sensitivity (82%).},
 archiveprefix = {arXiv},
 author = {Noel Codella and Quoc-Bao Nguyen and Sharath Pankanti and David Gutman and Brian Helba and Allan Halpern and John R. Smith},
 eprint = {1610.04662v2},
 file = {1610.04662v2.pdf},
 link = {http://arxiv.org/abs/1610.04662v2},
 month = {Nov},
 note = {IBM Journal of Research and Development, vol. 61, no. 4/5, 2017},
 primaryclass = {cs.CV},
 title = {Deep Learning Ensembles for Melanoma Recognition in Dermoscopy Images},
 year = {2016}
}


@article{1AJUcl1KV,
 abstract = {Melanoma is amongst most aggressive types of cancer. However, it is highly
curable if detected in its early stages. Prescreening of suspicious moles and
lesions for malignancy is of great importance. Detection can be done by images
captured by standard cameras, which are more preferable due to low cost and
availability. One important step in computerized evaluation of skin lesions is
accurate detection of lesion region, i.e. segmentation of an image into two
regions as lesion and normal skin. Accurate segmentation can be challenging due
to burdens such as illumination variation and low contrast between lesion and
healthy skin. In this paper, a method based on deep neural networks is proposed
for accurate extraction of a lesion region. The input image is preprocessed and
then its patches are fed to a convolutional neural network (CNN). Local texture
and global structure of the patches are processed in order to assign pixels to
lesion or normal classes. A method for effective selection of training patches
is used for more accurate detection of a lesion border. The output segmentation
mask is refined by some post processing operations. The experimental results of
qualitative and quantitative evaluations demonstrate that our method can
outperform other state-of-the-art algorithms exist in the literature.},
 archiveprefix = {arXiv},
 author = {Mohammad H. Jafari and Ebrahim Nasr-Esfahani and Nader Karimi and S. M. Reza Soroushmehr and Shadrokh Samavi and Kayvan Najarian},
 doi = {10.1007/s11548-017-1567-8},
 eprint = {1609.02374v1},
 file = {1609.02374v1.pdf},
 link = {http://arxiv.org/abs/1609.02374v1},
 month = {Sep},
 primaryclass = {cs.CV},
 title = {Extraction of Skin Lesions from Non-Dermoscopic Images Using Deep
Learning},
 year = {2016}
}


@article{LL5huVs3,
 abstract = {Deep learning algorithms, in particular convolutional networks, have rapidly
become a methodology of choice for analyzing medical images. This paper reviews
the major deep learning concepts pertinent to medical image analysis and
summarizes over 300 contributions to the field, most of which appeared in the
last year. We survey the use of deep learning for image classification, object
detection, segmentation, registration, and other tasks and provide concise
overviews of studies per application area. Open challenges and directions for
future research are discussed.},
 archiveprefix = {arXiv},
 author = {Geert Litjens and Thijs Kooi and Babak Ehteshami Bejnordi and Arnaud Arindra Adiyoso Setio and Francesco Ciompi and Mohsen Ghafoorian and Jeroen A. W. M. van der Laak and Bram van Ginneken and Clara I. Sánchez},
 eprint = {1702.05747v1},
 file = {1702.05747v1.pdf},
 link = {http://arxiv.org/abs/1702.05747v1},
 month = {Feb},
 primaryclass = {cs.CV},
 title = {A Survey on Deep Learning in Medical Image Analysis},
 year = {2017}
}


@article{1Fel6Bdb8,
 abstract = {Deep neural networks are highly expressive models that have recently achieved
state of the art performance on speech and visual recognition tasks. While
their expressiveness is the reason they succeed, it also causes them to learn
uninterpretable solutions that could have counter-intuitive properties. In this
paper we report two such properties.
First, we find that there is no distinction between individual high level
units and random linear combinations of high level units, according to various
methods of unit analysis. It suggests that it is the space, rather than the
individual units, that contains of the semantic information in the high layers
of neural networks.
Second, we find that deep neural networks learn input-output mappings that
are fairly discontinuous to a significant extend. We can cause the network to
misclassify an image by applying a certain imperceptible perturbation, which is
found by maximizing the network's prediction error. In addition, the specific
nature of these perturbations is not a random artifact of learning: the same
perturbation can cause a different network, that was trained on a different
subset of the dataset, to misclassify the same input.},
 archiveprefix = {arXiv},
 author = {Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian Goodfellow and Rob Fergus},
 eprint = {1312.6199v4},
 file = {1312.6199v4.pdf},
 link = {http://arxiv.org/abs/1312.6199v4},
 month = {12},
 primaryclass = {cs.CV},
 title = {Intriguing properties of neural networks},
 year = {2013}
}


@article{UtcyntjF,
 abstract = {Several machine learning models, including neural networks, consistently
misclassify adversarial examples---inputs formed by applying small but
intentionally worst-case perturbations to examples from the dataset, such that
the perturbed input results in the model outputting an incorrect answer with
high confidence. Early attempts at explaining this phenomenon focused on
nonlinearity and overfitting. We argue instead that the primary cause of neural
networks' vulnerability to adversarial perturbation is their linear nature.
This explanation is supported by new quantitative results while giving the
first explanation of the most intriguing fact about them: their generalization
across architectures and training sets. Moreover, this view yields a simple and
fast method of generating adversarial examples. Using this approach to provide
examples for adversarial training, we reduce the test set error of a maxout
network on the MNIST dataset.},
 archiveprefix = {arXiv},
 author = {Ian J. Goodfellow and Jonathon Shlens and Christian Szegedy},
 eprint = {1412.6572v3},
 file = {1412.6572v3.pdf},
 link = {http://arxiv.org/abs/1412.6572v3},
 month = {12},
 primaryclass = {stat.ML},
 title = {Explaining and Harnessing Adversarial Examples},
 year = {2014}
}


@article{AsLAb71x,
 abstract = {Advances in machine learning (ML) in recent years have enabled a dizzying
array of applications such as data analytics, autonomous systems, and security
diagnostics. ML is now pervasive---new systems and models are being deployed in
every domain imaginable, leading to rapid and widespread deployment of software
based inference and decision making. There is growing recognition that ML
exposes new vulnerabilities in software systems, yet the technical community's
understanding of the nature and extent of these vulnerabilities remains
limited. We systematize recent findings on ML security and privacy, focusing on
attacks identified on these systems and defenses crafted to date. We articulate
a comprehensive threat model for ML, and categorize attacks and defenses within
an adversarial framework. Key insights resulting from works both in the ML and
security communities are identified and the effectiveness of approaches are
related to structural elements of ML algorithms and the data used to train
them. We conclude by formally exploring the opposing relationship between model
accuracy and resilience to adversarial manipulation. Through these
explorations, we show that there are (possibly unavoidable) tensions between
model complexity, accuracy, and resilience that must be calibrated for the
environments in which they will be used.},
 archiveprefix = {arXiv},
 author = {Nicolas Papernot and Patrick McDaniel and Arunesh Sinha and Michael Wellman},
 eprint = {1611.03814v1},
 file = {1611.03814v1.pdf},
 link = {http://arxiv.org/abs/1611.03814v1},
 month = {Dec},
 primaryclass = {cs.CR},
 title = {Towards the Science of Security and Privacy in Machine Learning},
 year = {2016}
}


@article{18lZK7fxH,
 abstract = {Although deep neural networks (DNNs) have achieved great success in many
computer vision tasks, recent studies have shown they are vulnerable to
adversarial examples. Such examples, typically generated by adding small but
purposeful distortions, can frequently fool DNN models. Previous studies to
defend against adversarial examples mostly focused on refining the DNN models.
They have either shown limited success or suffer from the expensive
computation. We propose a new strategy, \emph{feature squeezing}, that can be
used to harden DNN models by detecting adversarial examples. Feature squeezing
reduces the search space available to an adversary by coalescing samples that
correspond to many different feature vectors in the original space into a
single sample. By comparing a DNN model's prediction on the original input with
that on the squeezed input, feature squeezing detects adversarial examples with
high accuracy and few false positives. This paper explores two instances of
feature squeezing: reducing the color bit depth of each pixel and smoothing
using a spatial filter. These strategies are straightforward, inexpensive, and
complementary to defensive methods that operate on the underlying model, such
as adversarial training.},
 archiveprefix = {arXiv},
 author = {Weilin Xu and David Evans and Yanjun Qi},
 eprint = {1704.01155v1},
 file = {1704.01155v1.pdf},
 link = {http://arxiv.org/abs/1704.01155v1},
 month = {Apr},
 primaryclass = {cs.CV},
 title = {Feature Squeezing: Detecting Adversarial Examples in Deep Neural
Networks},
 year = {2017}
}


@article{apBChoyF,
 abstract = {The recent rapid and tremendous success of deep convolutional neural networks
(CNN) on many challenging computer vision tasks largely derives from the
accessibility of the well-annotated ImageNet and PASCAL VOC datasets.
Nevertheless, unsupervised image categorization (i.e., without the ground-truth
labeling) is much less investigated, yet critically important and difficult
when annotations are extremely hard to obtain in the conventional way of
"Google Search" and crowd sourcing. We address this problem by presenting a
looped deep pseudo-task optimization (LDPO) framework for joint mining of deep
CNN features and image labels. Our method is conceptually simple and rests upon
the hypothesized "convergence" of better labels leading to better trained CNN
models which in turn feed more discriminative image representations to
facilitate more meaningful clusters/labels. Our proposed method is validated in
tackling two important applications: 1) Large-scale medical image annotation
has always been a prohibitively expensive and easily-biased task even for
well-trained radiologists. Significantly better image categorization results
are achieved via our proposed approach compared to the previous
state-of-the-art method. 2) Unsupervised scene recognition on representative
and publicly available datasets with our proposed technique is examined. The
LDPO achieves excellent quantitative scene classification results. On the MIT
indoor scene dataset, it attains a clustering accuracy of 75.3%, compared to
the state-of-the-art supervised classification accuracy of 81.0% (when both are
based on the VGG-VD model).},
 archiveprefix = {arXiv},
 author = {Xiaosong Wang and Le Lu and Hoo-chang Shin and Lauren Kim and Mohammadhadi Bagheri and Isabella Nogues and Jianhua Yao and Ronald M. Summers},
 eprint = {1701.06599v1},
 file = {1701.06599v1.pdf},
 link = {http://arxiv.org/abs/1701.06599v1},
 month = {Jan},
 primaryclass = {cs.CV},
 title = {Unsupervised Joint Mining of Deep Features and Image Labels for
Large-scale Radiology Image Categorization and Scene Recognition},
 year = {2017}
}


@article{PGi9g7yV,
 abstract = {The chest X-ray is one of the most commonly accessible radiological
examinations for screening and diagnosis of many lung diseases. A tremendous
number of X-ray imaging studies accompanied by radiological reports are
accumulated and stored in many modern hospitals' Picture Archiving and
Communication Systems (PACS). On the other side, it is still an open question
how this type of hospital-size knowledge database containing invaluable imaging
informatics (i.e., loosely labeled) can be used to facilitate the data-hungry
deep learning paradigms in building truly large-scale high precision
computer-aided diagnosis (CAD) systems.
In this paper, we present a new chest X-ray database, namely "ChestX-ray8", which comprises 108,948 frontal-view X-ray images of 32,717 unique patients
with the text-mined eight disease image labels (where each image can have
multi-labels), from the associated radiological reports using natural language
processing. Importantly, we demonstrate that these commonly occurring thoracic
diseases can be detected and even spatially-located via a unified
weakly-supervised multi-label image classification and disease localization
framework, which is validated using our proposed dataset. Although the initial
quantitative results are promising as reported, deep convolutional neural
network based "reading chest X-rays" (i.e., recognizing and locating the common
disease patterns trained with only image-level labels) remains a strenuous task
for fully-automated high precision CAD systems.},
 archiveprefix = {arXiv},
 author = {Xiaosong Wang and Yifan Peng and Le Lu and Zhiyong Lu and Mohammadhadi Bagheri and Ronald M. Summers},
 eprint = {1705.02315v1},
 file = {1705.02315v1.pdf},
 link = {http://arxiv.org/abs/1705.02315v1},
 month = {May},
 primaryclass = {cs.CV},
 title = {ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on
Weakly-Supervised Classification and Localization of Common Thorax Diseases},
 year = {2017}
}


@article{1ENxzq6pT,
 abstract = {We propose a criterion for discrimination against a specified sensitive
attribute in supervised learning, where the goal is to predict some target
based on available features. Assuming data about the predictor, target, and
membership in the protected group are available, we show how to optimally
adjust any learned predictor so as to remove discrimination according to our
definition. Our framework also improves incentives by shifting the cost of poor
classification from disadvantaged groups to the decision maker, who can respond
by improving the classification accuracy.
In line with other studies, our notion is oblivious: it depends only on the
joint statistics of the predictor, the target and the protected attribute, but
not on interpretation of individualfeatures. We study the inherent limits of
defining and identifying biases based on such oblivious measures, outlining
what can and cannot be inferred from different oblivious tests.
We illustrate our notion using a case study of FICO credit scores.},
 archiveprefix = {arXiv},
 author = {Moritz Hardt and Eric Price and Nathan Srebro},
 eprint = {1610.02413v1},
 file = {1610.02413v1.pdf},
 link = {http://arxiv.org/abs/1610.02413v1},
 month = {Nov},
 primaryclass = {cs.LG},
 title = {Equality of Opportunity in Supervised Learning},
 year = {2016}
}


@article{11aqfNfQx,
 abstract = {Motivated by concerns that automated decision-making procedures can
unintentionally lead to discriminatory behavior, we study a technical
definition of fairness modeled after John Rawls' notion of "fair equality of
opportunity". In the context of a simple model of online decision making, we
give an algorithm that satisfies this fairness constraint, while still being
able to learn at a rate that is comparable to (but necessarily worse than) that
of the best algorithms absent a fairness constraint. We prove a regret bound
for fair algorithms in the linear contextual bandit framework that is a
significant improvement over our technical companion paper [16], which gives
black-box reductions in a more general setting. We analyze our algorithms both
theoretically and experimentally. Finally, we introduce the notion of a
"discrimination index", and show that standard algorithms for our problem
exhibit structured discriminatory behavior, whereas the "fair" algorithms we
develop do not.},
 archiveprefix = {arXiv},
 author = {Matthew Joseph and Michael Kearns and Jamie Morgenstern and Seth Neel and Aaron Roth},
 eprint = {1610.09559v3},
 file = {1610.09559v3.pdf},
 link = {http://arxiv.org/abs/1610.09559v3},
 month = {Nov},
 primaryclass = {cs.LG},
 title = {Rawlsian Fairness for Machine Learning},
 year = {2016}
}


@article{1EayJRsI,
 abstract = {This work introduces a method to tune a sequence-based generative model for
molecular de novo design that through augmented episodic likelihood can learn
to generate structures with certain specified desirable properties. We
demonstrate how this model can execute a range of tasks such as generating
analogues to a query structure and generating compounds predicted to be active
against a biological target. As a proof of principle, the model is first
trained to generate molecules that do not contain sulphur. As a second example, the model is trained to generate analogues to the drug Celecoxib, a technique
that could be used for scaffold hopping or library expansion starting from a
single molecule. Finally, when tuning the model towards generating compounds
predicted to be active against the dopamine receptor D2, the model generates
structures of which more than 95% are predicted to be active, including
experimentally confirmed actives that have not been included in either the
generative model nor the activity prediction model.},
 archiveprefix = {arXiv},
 author = {Marcus Olivecrona and Thomas Blaschke and Ola Engkvist and Hongming Chen},
 eprint = {1704.07555v1},
 file = {1704.07555v1.pdf},
 link = {http://arxiv.org/abs/1704.07555v1},
 month = {Apr},
 primaryclass = {cs.AI},
 title = {Molecular De Novo Design through Deep Reinforcement Learning},
 year = {2017}
}


@article{8LWFFeYg,
 abstract = {In de novo drug design, computational strategies are used to generate novel
molecules with good affinity to the desired biological target. In this work, we
show that recurrent neural networks can be trained as generative models for
molecular structures, similar to statistical language models in natural
language processing. We demonstrate that the properties of the generated
molecules correlate very well with the properties of the molecules used to
train the model. In order to enrich libraries with molecules active towards a
given biological target, we propose to fine-tune the model with small sets of
molecules, which are known to be active against that target.
Against Staphylococcus aureus, the model reproduced 14% of 6051 hold-out test
molecules that medicinal chemists designed, whereas against Plasmodium
falciparum (Malaria) it reproduced 28% of 1240 test molecules. When coupled
with a scoring function, our model can perform the complete de novo drug design
cycle to generate large sets of novel molecules for drug discovery.},
 archiveprefix = {arXiv},
 author = {Marwin H. S. Segler and Thierry Kogej and Christian Tyrchan and Mark P. Waller},
 eprint = {1701.01329v1},
 file = {1701.01329v1.pdf},
 link = {http://arxiv.org/abs/1701.01329v1},
 month = {Jan},
 primaryclass = {cs.NE},
 title = {Generating Focussed Molecule Libraries for Drug Discovery with Recurrent
Neural Networks},
 year = {2017}
}


@article{4TK06zOf,
 abstract = {Neural Machine Translation (NMT) is an end-to-end learning approach for
automated translation, with the potential to overcome many of the weaknesses of
conventional phrase-based translation systems. Unfortunately, NMT systems are
known to be computationally expensive both in training and in translation
inference. Also, most NMT systems have difficulty with rare words. These issues
have hindered NMT's use in practical deployments and services, where both
accuracy and speed are essential. In this work, we present GNMT, Google's
Neural Machine Translation system, which attempts to address many of these
issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder
layers using attention and residual connections. To improve parallelism and
therefore decrease training time, our attention mechanism connects the bottom
layer of the decoder to the top layer of the encoder. To accelerate the final
translation speed, we employ low-precision arithmetic during inference
computations. To improve handling of rare words, we divide words into a limited
set of common sub-word units ("wordpieces") for both input and output. This
method provides a good balance between the flexibility of "character"-delimited
models and the efficiency of "word"-delimited models, naturally handles
translation of rare words, and ultimately improves the overall accuracy of the
system. Our beam search technique employs a length-normalization procedure and
uses a coverage penalty, which encourages generation of an output sentence that
is most likely to cover all the words in the source sentence. On the WMT'14
English-to-French and English-to-German benchmarks, GNMT achieves competitive
results to state-of-the-art. Using a human side-by-side evaluation on a set of
isolated simple sentences, it reduces translation errors by an average of 60%
compared to Google's phrase-based production system.},
 archiveprefix = {arXiv},
 author = {Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and Łukasz Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa and Keith Stevens and George Kurian and Nishant Patil and Wei Wang and Cliff Young and Jason Smith and Jason Riesa and Alex Rudnick and Oriol Vinyals and Greg Corrado and Macduff Hughes and Jeffrey Dean},
 eprint = {1609.08144v2},
 file = {1609.08144v2.pdf},
 link = {http://arxiv.org/abs/1609.08144v2},
 month = {Sep},
 primaryclass = {cs.CL},
 title = {Google's Neural Machine Translation System: Bridging the Gap between
Human and Machine Translation},
 year = {2016}
}


@article{39RPiE10,
 abstract = {Computational prediction of membrane protein (MP) structures is very
challenging partially due to lack of sufficient solved structures for homology
modeling. Recently direct evolutionary coupling analysis (DCA) sheds some light
on protein contact prediction and accordingly, contact-assisted folding, but
DCA is effective only on some very large-sized families since it uses
information only in a single protein family. This paper presents a deep
transfer learning method that can significantly improve MP contact prediction
by learning contact patterns and complex sequence-contact relationship from
thousands of non-membrane proteins (non-MPs). Tested on 510 non-redundant MPs, our deep model (learned from only non-MPs) has top L/10 long-range contact
prediction accuracy 0.69, better than our deep model trained by only MPs (0.63)
and much better than a representative DCA method CCMpred (0.47) and the CASP11
winner MetaPSICOV (0.55). The accuracy of our deep model can be further
improved to 0.72 when trained by a mix of non-MPs and MPs. When only contacts
in transmembrane regions are evaluated, our method has top L/10 long-range
accuracy 0.62, 0.57, and 0.53 when trained by a mix of non-MPs and MPs, by
non-MPs only, and by MPs only, respectively, still much better than MetaPSICOV
(0.45) and CCMpred (0.40). All these results suggest that sequence-structure
relationship learned by our deep model from non-MPs generalizes well to MP
contact prediction. Improved contact prediction also leads to better
contact-assisted folding. Using only top predicted contacts as restraints, our
deep learning method can fold 160 and 200 of 510 MPs with TMscore>0.6 when
trained by non-MPs only and by a mix of non-MPs and MPs, respectively, while
CCMpred and MetaPSICOV can do so for only 56 and 77 MPs, respectively. Our
contact-assisted folding also greatly outperforms homology modeling.},
 archiveprefix = {arXiv},
 author = {Zhen Li and Sheng Wang and Yizhou Yu and Jinbo Xu},
 eprint = {1704.07207v1},
 file = {1704.07207v1.pdf},
 link = {http://arxiv.org/abs/1704.07207v1},
 month = {Apr},
 primaryclass = {q-bio.BM},
 title = {Predicting membrane protein contacts from non-membrane proteins by deep
transfer learning},
 year = {2017}
}

