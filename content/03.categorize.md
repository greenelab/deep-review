## Deep learning and patient categorization

In healthcare, individuals are diagnosed with a disease or condition
based on symptoms, the results of certain diagnostic tests, or other factors.
Once diagnosed with a disease, an individual might be assigned a stage based on
another set of human-defined rules. While these rules are refined over time, the
process is evolutionary and ad hoc, potentially impeding the identification of
underlying biological mechanisms and their corresponding treatment
interventions.

Deep learning methods coupled with a large corpus of patient phenotypes may
provide a more data-driven approach to patient categorization. A deep neural
network has the potential to identify entirely new categories of health or
disease that are only present when data from multiple lab tests are integrated.

As an example, consider the condition Latent Autoimmune Diabetes in Adults
(LADA; reviewed in [@doi:10.2337/diabetes.54.suppl_2.S68]). In the absence of a
pre-specified disease definition, a deep neural network might have identified a
subgroup of individuals with blood glucose levels that indicated diabetes as
well as auto-antibodies, even though the individuals had never been diagnosed
with type 1 diabetes -- the autoimmune form of the disease that arises in young
people. Such a neural network would be identifying patients with LADA. As no
such computational approach existed, LADA was actually identified by Groop et
al. [@doi:10.2337/diab.35.2.237].

One should not regard recapitulation of existing disease categories
as a gold-standard for deep learning results. Instead, a meaningful
contribution to patient categorization would be to identify new shared
mechanisms that would otherwise be obscured due to ad hoc historical definitions
of disease. Perhaps deep neural networks, by reevaluating data without the
context of our assumptions, can reveal novel classes of treatable conditions.

In spite of such optimism, the ability of deep learning models to
indiscriminately extract predictive signals must also be assessed and
operationalized with care. Imagine a deep neural network is provided with
clinical test results gleaned from electronic health records. Because physicians
may order certain tests based on their suspected diagnosis, a deep neural
network may learn to "diagnose" patients simply based on the tests that are
ordered. For some objective functions, such as predicting an International
Classification of Diseases (ICD) code, this may offer good performance even
though it does not provide insight into the underlying disease beyond physician
activity. This challenge is not unique to deep learning approaches; however, it
is important for practitioners to be aware of these challenges and the
possibility in this domain of constructing highly predictive classifiers of
questionable actual utility.

Our goal in this section is to assess the extent to which deep learning is
already contributing to the discovery of novel categories. Where it is not, we
focus on barriers to achieving these goals. We also highlight approaches that
researchers are taking to address challenges within the field, particularly with
regards to data availability and labeling.

### Imaging applications in healthcare

Deep learning methods have transformed the analysis of natural images and video,
and similar examples are beginning to emerge with medical images. Deep learning
has been used to classify lesions and nodules; localize organs, regions,
landmarks and lesions; segment organs, organ substructures and lesions; retrieve
images based on content; generate and enhance images; and combine images with
clinical reports [@tag:Litjens2017_medimage_survey; @tag:Shen2017_medimg_review].

Though there are many commonalities with the analysis of natural images, there
are also key differences. In all cases that we examined, fewer than one million
images were available for training, and datasets are often many orders of
magnitude smaller than collections of natural images. Researchers have developed
subtask-specific strategies to address this challenge.

The first strategy repurposes features extracted from natural images by deep
learning models, such as ImageNet [@tag:Russakovsky2015_imagenet], for new
purposes. Diagnosing diabetic retinopathy through color fundus images became an
area of focus for deep learning researchers after a large labeled image set was
made publicly available during a 2015 Kaggle competition [@tag:Pratt2016_dr].
Most participants trained neural networks from scratch [@tag:Pratt2016_dr;
@tag:Abramoff2016_dr; @tag:Leibig2016_dr], but Gulshan et al.
[@tag:Gulshan2016_dt] repurposed a 48-layer Inception-v3 deep architecture
pre-trained on natural images and surpassed the state-of-the-art specificity and
sensitivity. Such features were also repurposed to detect melanoma, the
deadliest form of skin cancer, from dermoscopic
[@tag:Codella2016_ensemble_melanoma; @tag:Yu2016_melanoma_resnet] and
non-dermoscopic images of skin lesions [@tag:Jafari2016_skin_lesions;
@tag:Esfahani2016_melanoma; @tag:Esteva2017_skin_cancer_nature] as well as
age-related macular degeneration [@tag:Burlina2016_amd]. Pre-training on natural
images can enable very deep networks to succeed without overfitting. For the
melanoma task, reported performance was competitive with or better than a board
of certified dermatologists [@tag:Codella2016_ensemble_melanoma;
@tag:Esteva2017_skin_cancer_nature].

Reusing features from natural images is also an emerging approach for radiographic images,
where datasets are often too small to train large deep neural networks without
these techniques [@tag:Bar2015_nonmed_tl; @tag:Shin2016_cad_tl;
@tag:Rajkomar2017_radiographs; @tag:Lakhani2017_radiography].
A deep CNN trained on natural images
boosts performance in radiographic images [@tag:Rajkomar2017_radiographs]. However, the target task required
either re-training the initial model from scratch with special pre-processing or
fine-tuning of the whole network on radiographs with heavy data augmentation to
avoid overfitting.

The technique of reusing features from a different task falls into the broader
area of transfer learning (see Discussion). Though we've mentioned numerous
successes for the transfer of natural image features to new tasks, we expect
that a lower proportion of negative results have been published. The analysis of
magnetic resonance images (MRIs) is also faced with the challenge of small
training sets. In this domain, Amit et al. [@tag:Amit2017_breast_mri]
investigated the tradeoff between pre-trained models from a different domain and
a small CNN trained only with MRI images. In contrast with the other selected
literature, they found a smaller network trained with data augmentation on few
hundred images from a few dozen patients can outperform a pre-trained
out-of-domain classifier. Data augmentation is a different strategy to deal with
small training sets. The practice is exemplified by a series of papers that
analyze images from mammographies [@tag:Dhungel2015_struct_pred_mamm;
@tag:Dhungel2016_mamm; @tag:Zhu2016_mult_inst_mamm; @tag:Zhu2016_advers_mamm;
@tag:Dhungel2017_mamm_min_interv]. To expand the number and diversity of images,
researchers constructed adversarial examples [@tag:Zhu2016_advers_mamm].
Adversarial examples are constructed by applying a transformation that changes
training images but not their content -- for example by rotating an image by a
random amount. An alternative in the domain is to train towards human-created
features before subsequent fine-tuning [@tag:Dhungel2016_mamm], which can help
to sidestep this challenge though it does give up deep learning techniques'
strength as feature constructors.

Another way of dealing with limited training data is to divide rich data -- e.g.
3D images -- into numerous reduced projections. Shin et al.
[@tag:Shin2016_cad_tl] compared various deep network architectures, dataset
characteristics, and training procedures for computer tomography-based (CT)
abnormality detection. They concluded that networks as deep as 22 layers could
be useful for 3D data, despite the limited size of training datasets.
However, they noted that choice of architecture, parameter setting, and model
fine-tuning needed is very problem- and dataset-specific. Moreover, this type of
task often depends on both lesion localization and appearance, which poses
challenges for CNN-based approaches. Straightforward attempts to capture useful
information from full-size images in all three dimensions simultaneously via
standard neural network architectures were computationally unfeasible. Instead,
two-dimensional models were used to either process image slices individually
(2D), or aggregate information from a number of 2D projections in the native
space (2.5D).

Roth et al. compared 2D, 2.5D, and 3D CNNs on a number of tasks
for computer-aided detection from CT scans and showed that 2.5D CNNs performed
comparably well to 3D analogs, while requiring much less training time,
especially on augmented training sets [@tag:Roth2015_view_agg_cad]. Another
advantage of 2D and 2.5D networks is the wider availability of pre-trained
models. But reducing the dimensionality is not always helpful. Nie et al.
[@tag:Nie2016_3d_survival] showed that multimodal, multi-channel 3D deep
architecture was successful at learning high-level brain tumor appearance
features jointly from MRI, functional MRI, and diffusion MRI images,
outperforming single-modality or 2D models. Overall, the variety of modalities,
properties and sizes of training sets, the dimensionality of input, and the
importance of end goals in medical image analysis are provoking a development of
specialized deep neural network architectures, training and validation
protocols, and input representations that are not characteristic of
widely-studied natural images.

Predictions from deep neural networks can be evaluated for use in workflows that
also incorporate human experts. In a large dataset of mammography images, Kooi
et al. [@tag:Kooi2016_mamm_lesions] demonstrated that deep neural networks
outperform the traditional computer-aided diagnosis system at low sensitivity
and perform comparably at high sensitivity. They also compared network
performance to certified screening radiologists on a patch level and found no
significant difference between the network and the readers. However, using deep
methods for clinical practice is challenged by the difficulty of assigning a
level of confidence to each prediction. Leibig et al. [@tag:Leibig2016_dr]
estimated the uncertainty of deep networks for diabetic retinopathy diagnosis by
linking dropout networks with approximate Bayesian inference. Techniques that
assign confidences to each prediction should aid pathologist-computer
interactions and improve uptake by physicians.

Systems to aid in the analysis of histology slides are also promising use cases
for deep learning [@tag:Litjens2016_histopath_survey]. Ciresan et al.
[@tag:Ciresan2013_mitosis] developed one of the earliest approaches for
histology slides, winning the 2012 International Conference on Pattern
Recognition's Contest on Mitosis Detection while achieving human-competitive
accuracy. In more recent work, Wang et al. [@tag:Wang2016_breast_cancer]
analyzed stained slides of lymph node slices to identify cancers. On this task a
pathologist has about a 3% error rate. The pathologist did not produce any false
positives, but did have a number of false negatives. The algorithm had about
twice the error rate of a pathologist, but the errors were not strongly
correlated.  In this area, these algorithms may be ready to be incorporated into
existing tools to aid pathologists and reduce the false negative rate. Ensembles
of deep learning and human experts may help overcome some of the challenges
presented by data limitations.

One source of training examples with rich clinical annotations is electronic
health records. Recently, Lee et al. [@tag:Lee2016_emr_oct_amd] developed an
approach to distinguish individuals with age-related macular degeneration from
control individuals. They trained a deep neural network on approximately 100,000
images extracted from structured electronic health records, reaching greater
than 93% accuracy. The authors used their test set to evaluate when to stop
training. In other domains, this has resulted in a minimal change in the
estimated accuracy [@tag:Krizhevsky2013_nips_cnn], but we recommend the use of
an independent test set whenever feasible.

Chest X-rays are a common radiological examination for screening and diagnosis
of lung diseases. Although hospitals have accumulated a large number of raw
radiology images and reports in Picture Archiving and Communication Systems and
their related reports in Radiology Information Systems, it is not yet known how
to effectively use them to learn the correlation between pathology categories
and X-rays. In the last few years, deep learning methods showed remarkable
results in chest X-ray image analysis [@arxiv:1701.06599; @arxiv:1705.02315].
However, it is both costly and time-consuming to annotate a large-scale
fully-labeled corpus to facilitate data-intensive deep learning models. As an
alternative, Wang et al. [@arxiv:1705.02315] proposed to use weakly labeled
images. To generate weak labels for X-ray images, they applied a series of
natural language processing (NLP) techniques to the associated chest X-ray
radiological reports. Specifically, they first extracted all diseases mentioned
in the reports using a state-of-the-art NLP tool, then applied a newly-developed
negation and uncertainty detection tool (NegBio) to filter negative and
equivocal findings in the reports. Evaluation on three independent datasets
demonstrated that NegBio is highly accurate for detecting negative and equivocal
findings (~90% in F-measure, which balances precision and recall
[@doi:10.1038/nmeth.3945]). These highly-accurate results meet the need to
generate a corpus with weak labels, which serves as a solid foundation for the
later process of image classification. The resulting dataset
[@url:https://nihcc.app.box.com/v/ChestXray-NIHCC] consists of 112,120
frontal-view chest X-ray images from 30,805 patients, and each image is
associated with one or more weakly-labeled pathology category (e.g. pneumonia
and cardiomegaly) or "normal" otherwise. Further, Wang et al.
[@arxiv:1705.02315] used this dataset with a unified weakly-supervised
multi-label image classification framework, to detect common thoracic diseases.
It showed superior performance over a benchmark using fully-labeled data.

With the exception of natural image-like problems (e.g. melanoma detection),
biomedical imaging poses a number of challenges for deep learning. Datasets are
typically small, annotations can be sparse, and images are often
high-dimensional, multimodal, and multi-channel. Techniques like transfer
learning, heavy dataset augmentation, and the use of multi-view and multi-stream architectures
are more common than in the natural image domain. Furthermore, high model
sensitivity and specificity can translate directly into clinical value. Thus,
prediction evaluation, uncertainty estimation, and model interpretation methods
are also of great importance in this domain (see Discussion). Finally, there is
a need for better pathologist-computer interaction techniques that will allow
combining the power of deep learning methods with human expertise and lead to
better-informed decisions for patient treatment and care.

### Text applications in healthcare

Due to the rapid growth of scholarly publications and electronic medical/health records, biomedical text mining has become increasingly important in recent years.
The main tasks in biological and clinical text mining include, but are not limited to, named entity recognition, relation/event extraction, and information retrieval (Figure @fig:biotm).

![Deep learning applications, tasks and models based on NLP perspectives.](images/biotm.png){#fig:biotm width="100%"}

Machine learning is widely used in many of aforementioned tasks where engineering the optimal set of features is a must.
However it is often difficult to know what features should be extracted, even with domain knowledge.
Deep learning addresses this challenge by enabling "computers to build complex
concepts out of simpler ones [@tag:goodfellow2016deep]".
Moreover, deep learning has shown competitive performance compared to traditional ML methods.
As a result, we have observed growing interests of using deep learning in biomedical text mining applications in recent years.
In this section, we summarize recent studies in text mining with various deep learning methods.
We categorize the body of work by the application domain/text genre (biomedical literature vs. clinical notes) and by the actual task (e.g. concept or relation extraction).

In biomedical text mining, named entity recognition (NER) is a task of identifying text spans that refer to a biological concept of a specific class, such as disease or chemical, in a controlled vocabulary or ontology.
NER is of importance as it is often needed as a first step in many complex text mining systems.
The current state-of-the-art methods typically reformulate the task as a sequence labeling problem and use conditional random fields [@doi:10.1093/bioinformatics/btw343; @doi:10.1093/bioinformatics/btt156; @doi:10.1093/bioinformatics/btt474].
In recent years, word embeddings that contain rich latent semantic information of words have been widely used to improve the NER performance.
Liu et al. studied the effect of word embeddings on drug name recognition and compared them with traditional semantic features [@doi:10.3390/info6040848].
Tang et al. investigated word embeddings in gene, DNA, and cell line mention detection tasks [@doi:10.1155/2014/240403].
Moreover, Wu et al. examined the use of neural word embeddings for clinical abbreviation disambiguation [@doi:10.18653/v1/w15-3822].
Liu et al. exploited task-oriented resources to learn word embeddings for clinical abbreviation expansion [@doi:10.18653/v1/w15-3810].

Relation extraction is a task of detecting and classifying semantic relationship between entities from the literature.
At present, kernel methods or feature-based approaches are commonly applied [@doi:10.1371/journal.pcbi.1000837; @doi:10.1186/s13321-016-0165-z; @doi:10.1093/bioinformatics/btp602].
There are several proposals of using deep learning in the biomedical relation tasks because it can relieve the feature sparsity and engineering problems.
Some studies focused on jointly extracting biomedical entities as well as their relations simultaneously [@tag:li2016joint; @doi:10.1186/s12859-017-1609-9], while others applied deep learning on relation classification with given entities.
For example, Peng and Lu proposed a multichannel dependency-based CNN and Hua and Quan proposed a shortest path based CNN for the sentence-based protein-protein extraction task [@doi:10.18653/v1/w17-2304; @doi:10.1155/2016/8479587; @doi:10.1155/2016/1850404].
Jiang et al. proposed a biomedical domain-specific word embedding model to reduce the manual labor of designing semantic representation for the same task [@doi:10.1504/IJDMB.2016.074878].
Gu et al. employed a maximum entropy model and a CNN model for chemical-induced disease relation extraction at inter- and intra-sentence level, respectively [@doi:10.1093/database/bax024].
For drug-drug interaction, Zhao et al. used a CNN that employs word embeddings with the syntactic information of a sentence as well as features of POS tags and dependency trees [@doi:10.1093/bioinformatics/btw486].
Masaki et al. experimented with an attention CNN, and Yi et al. a recurrent neural network model (RNN) model with multiple attention layers [@doi:10.18653/v1/w17-2302; @arxiv:1705.03261].
In both cases, it is a single model with attention mechanism, which allows the decoder to "attend" to different parts of the source sentence. As a result, it does not require dependency parsing or training multiple models. 
Both attention CNN and RNN have comparable results, but the CNN model has an advantage in that it can be easily computed in parallel, hence making it faster with recent Graphical Processing Units (GPUs).

For biotopes event extraction, Li et al. employed CNN and distributed representation while Mehryary et al. used long-short term memory (LSTM) networks to extract complicate relations [@doi:10.18653/v1/w16-3012; @doi:10.18653/v1/w16-3009].
Li et al. applied word embedding to extract complete events from biomedical text and archived the results comparable to the state-of-the-art systems [@doi:10.18653/v1/w15-3814].
There are also some works that identified event triggers rather than the complete event [@doi:10.1142/S0219720015410012;  @arxiv:1705.09516].
Taken together, it appears that deep learning models outperform traditional kernel methods or feature-based approaches by 1-5% in f-score.
Among various deep learning approaches, CNN stands out as the most popular model both in terms of computational complexity and performance, while RNN has achieved continuous progress.

Information retrieval is a task of finding relevant text that satisfies an information need from within a large document collection.
While deep learning has not yet achieved the same level of success in this area as seen in others, the recent surge of interest and work suggest that this may be quickly changing.
For example, Mohan et al. described a deep learning approach to modeling the relevance of a document's text to a query, applied to the entire biomedical literature [@doi:10.18653/v1/w17-2328].

In the clinical domain, Jagannatha and Yu employed a bidirectional LSTM RNN structure to extract adverse drug events from electronic health records [@pmid:27885364].
Choi et al. developed Doctor AI, a RNN-based model that can learn efficient patient representation from a large amount of longitudinal patient records and predict diagnosis and medication code of EHR [@arxiv:1511.05942].
Minarro-Giménez et al. applied the word2vec [@arxiv:1301.3781] deep learning toolkit to medical corpora and evaluated the efficiency of word2vec in identifying properties of pharmaceuticals based on mid-sized, unstructured medical text corpora without any additional background knowledge [@doi:10.3233/978-1-61499-432-9-584].
Lin et al. investigated using CNN to extract temporal relations [@doi:10.18653/v1/w17-2341].
Karimi et al. investigated the applicability of deep learning at autocoding of radiology reports using the International Classification of Diseases [@doi:10.18653/v1/w17-2342; @tag:world2004international].

To summarize, deep learning has shown promising results in many biomedical text mining tasks and applications.
But to further realize its full potential in this domain, either large size of labeled data and/or technical advancements in current methods coping with limited labeled data are required.

### Electronic health records

EHR data include substantial amounts of free text, which remains challenging to
approach [@doi:10.1136/amiajnl-2011-000501]. Often, researchers developing
algorithms that perform well on specific tasks must design and implement
domain-specific features [@doi:10.1136/amiajnl-2011-000150]. These features
capture unique aspects of the literature being processed. Deep learning methods
are natural feature constructors. In recent work, the authors evaluated the
extent to which deep learning methods could be applied on top of generic
features for domain-specific concept extraction [@arxiv:1611.08373]. They found
that performance was in line with, but lower than the best domain-specific
method [@arxiv:1611.08373]. This raises the possibility that deep learning may
impact the field by reducing the researcher time and cost required to develop
specific solutions, but it may not always lead to performance increases.

In recent work, Yoon et al.[@tag:Yoon2016_cancer_reports] analyzed simple
features using deep neural networks and found that the patterns recognized by
the algorithms could be re-used across tasks. Their aim was to analyze the free
text portions of pathology reports to identify the primary site and laterality
of tumors. The only features the authors supplied to the algorithms were
unigrams (counts for single words) and bigrams (counts for two-word combinations) in a
free text document. They subset the full set of words and word combinations to
the 400 most common. The machine learning algorithms that they employed (naïve
Bayes, logistic regression, and deep neural networks) all performed relatively
similarly on the task of identifying the primary site. However, when the authors
evaluated the more challenging task, evaluating the laterality of each tumor,
the deep neural network outperformed the other methods. Of particular interest,
when the authors first trained a neural network to predict primary site and then
repurposed those features as a component of a secondary neural network trained
to predict laterality, the performance was higher than a laterality-trained
neural network. This demonstrates how deep learning methods can repurpose
features across tasks, improving overall predictions as the field tackles new
challenges. The Discussion further reviews this type of transfer learning.

Several authors have created reusable feature sets for medical terminologies
using natural language processing and neural embedding models, as popularized by
Word2vec [@tag:Word2Vec]. A goal of learning terminologies for different
entities in the same vector space is to find relationships between different
domains (e.g. drugs and the diseases they treat). It is difficult for us to
provide a strong statement on the broad utility of these methods. Manuscripts in
this area tend to compare algorithms applied to the same data but lack a
comparison against overall best-practices for one or more tasks addressed by
these methods. Techniques have been developed for free text medical notes
[@doi:10.1145/2661829.2661974], ICD and National Drug Codes, and claims data
[@doi:10.1145/2939672.2939823]. Methods for neural embeddings learned from
electronic health records have at least some ability to predict disease-disease
associations and implicate genes with a statistical association with a disease
[@doi:10.1038/srep32404]. However, the evaluations performed did not
differentiate between simple predictions (i.e. the same disease in different
sites of the body) and non-intuitive ones. While promising, a lack of rigorous
evaluations of the real-world utility of these kinds of features makes current
contributions in this area difficult to evaluate. To examine the true utility,
comparisons need to be performed against leading approaches (i.e. algorithms and
data) as opposed to simply evaluating multiple algorithms on the same
potentially limited dataset.

Identifying consistent subgroups of individuals and individual health
trajectories from clinical tests is also an active area of research. Approaches
inspired by deep learning have been used for both unsupervised feature
construction and supervised prediction. Early work by Lasko et al.
[@doi:10.1371/journal.pone.0066341], combined sparse autoencoders and Gaussian
processes to distinguish gout from leukemia from uric acid sequences. Later work
showed that unsupervised feature construction of many features via denoising
autoencoder neural networks could dramatically reduce the number of labeled
examples required for subsequent supervised analyses
[@doi:10.1016/j.jbi.2016.10.007]. In addition, it pointed towards features
learned during unsupervised training being useful for visualising and
stratifying subgroups of patients within a single disease. In a concurrent
large-scale analysis of EHR data from 700,000 patients, Miotto et al.
[@doi:10.1038/srep26094] used a deep denoising autoencoder architecture applied
to the number and co-occurrence of clinical events to learn a representation of
patients (DeepPatient). The model was able to predict disease trajectories
within one year with over 90% accuracy and patient-level predictions were
improved by up to 15% when compared to other methods. Choi et
al. [@arxiv:1511.05942] attempted to model the longitudinal structure of EHRs
with a RNN to predict future diagnosis and medication prescriptions on a cohort
of 260,000 patients followed for 8 years (Doctor AI). Pham et al.
[@arxiv:1602.00357] built upon this concept by utilising a RNN with a long 
short-term memory (LSTM) architecture enabling explicit modelling of patient
trajectories through the use of memory cells. The method, DeepCare, performed
better than shallow models or plain RNN when tested on two independent cohorts
for its ability to predict disease progression, intervention recommendation and
future risk prediction. Nguyen et al. [@doi:10.1109/JBHI.2016.2633963] took a
different approach and used word embeddings from EHRs to train a CNN that could
detect and pool local clinical motifs to predict unplanned readmission after six
months, with performance better than the baseline method (Deepr). Razavian et
al. [@arxiv:1608.00647] used a set of 18 common lab tests to predict disease
onset using both CNN and LSTM architectures and demonstrated an improvement over
baseline regression models. However, numerous challenges including data
integration (patient demographics, family history, laboratory tests, text-based
patient records, image analysis, genomic data) and better handling of streaming
temporal data with many features, will need to be overcome before we can fully
assess the potential of deep learning for this application area.

Still, recent work has also revealed domains in which deep networks have proven
superior to traditional methods. Survival analysis models the time leading to an
event of interest from a shared starting point, and in the context of EHR data,
often associates these events to subject covariates. Exploring this relationship
is difficult, however, given that EHR data types are often heterogeneous,
covariates are often missing, and conventional approaches require the
covariate-event relationship be linear and aligned to a specific starting point
[@arxiv:1608.02158]. Early approaches, such as the Faraggi-Simon feed-forward
network, aimed to relax the linearity assumption, but performance gains were
lacking [@tag:Xiang]. Katzman et al. in turn developed a
deep implementation of the Faraggi-Simon network that, in addition to
outperforming Cox regression, was capable of comparing the risk between a given
pair of treatments, thus potentially acting as recommender system
[@arxiv:1606.00931]. To overcome the remaining difficulties, researchers have
turned to deep exponential families, a class of latent generative models that
are constructed from any type of exponential family distributions
[@arxiv:1411.2581v1]. The result was a deep survival analysis model capable of
overcoming challenges posed by missing data and heterogeneous data types, while
uncovering nonlinear relationships between covariates and failure time. They
showed their model more accurately stratified patients as a function of disease
risk score compared to the current clinical implementation.

There is a computational cost for these methods, however, when compared to
traditional, non-neural network approaches. For the exponential family models,
despite their scalability [@arxiv:1206.7051], an important question for the
investigator is whether he or she is interested in estimates of posterior
uncertainty. Given that these models are effectively Bayesian neural networks,
much of their utility simplifies to whether a Bayesian approach is warranted for
a given increase in computational cost. Moreover, as with all variational
methods, future work must continue to explore just how well the posterior
distributions are approximated, especially as model complexity increases
[@arxiv:1511.02386].

### Challenges and opportunities in patient categorization

#### Generating ground-truth labels can be expensive or impossible

A dearth of true labels is perhaps among the biggest obstacles for EHR-based
analyses that employ machine learning. Popular deep learning (and other machine
learning) methods are often used to tackle classification tasks and thus require
ground-truth labels for training.  For EHRs this can mean that researchers must
hire multiple clinicians to manually read and annotate individual patients'
records through a process called chart review. This allows researchers to assign
"true" labels, i.e. those that match our best available knowledge. Depending on
the application, sometimes the features constructed by algorithms also need to
be manually validated and interpreted by clinicians. This can be time consuming
and expensive [@doi:10.1016/j.ijmedinf.2016.09.014]. Because of these costs,
much of this research, including the work cited in this review, skips the
process of expert review. Clinicians' skepticism for research without expert
review may greatly dampen their enthusiasm for the work and consequently reduce
its impact. To date, even well-resourced large national consortia have been
challenged by the task of acquiring enough expert-validated labeled data. For
instance, in the eMERGE consortia and PheKB database
[@url:https://phekb.org/implementations], most samples with expert validation
contain only 100 to 300 patients. These datasets are quite small even for simple
machine learning algorithms. The challenge is greater for deep learning models
with many parameters. While unsupervised and semi-supervised approaches can help
with small sample sizes, the field would benefit greatly from large collections
of anonymized records in which a substantial number of records have undergone
expert review. This challenge is not unique to EHR-based studies. Work on
medical images, omics data in applications for which detailed metadata are
required, and other applications for which labels are costly to obtain will be
hampered as long as abundant curated data are unavailable.

Successful approaches to date in this domain have sidestepped this challenge by
making methodological choices that either reduce the need for labeled examples
or that use transformations to training data to increase the number of times it
can be used before overfitting occurs. For example, the unsupervised and
semi-supervised methods that we have discussed reduce the need for labeled
examples [@doi:10.1016/j.jbi.2016.10.007]. The anchor and learn framework
[@doi:10.1093/jamia/ocw011] uses expert knowledge to identify high-confidence
observations from which labels can be inferred. The adversarial training example
strategies mentioned above can reduce overfitting, if transformations are
available that preserve the meaningful content of the data while transforming
irrelevant features [@doi:10.1101/095786]. While adversarial training examples
can be easily imagined for certain methods that operate on images, it is more
challenging to figure out what an equivalent transformation would be for a
patient's clinical test results. Consequently, it may be hard to employ
adversarial training examples, not to be confused with generative adversarial
neural networks, with other applications. Finally, approaches that transfer
features can also help use valuable training data most efficiently. Rajkomar et
al. trained a deep neural network using generic images before tuning using only
radiology images [@doi:10.1007/s10278-016-9914-9]. Datasets that require many of
the same types of features might be used for initial training, before fine
tuning takes place with the more sparse biomedical examples. Though the analysis
has not yet been attempted, it is possible that analogous strategies may be
possible with electronic health records. For example, features learned from the
electronic health record for one type of clinical test (e.g. a decrease over
time in a lab value) may transfer across phenotypes.

Methods to accomplish more with little high-quality labeled data are also being
applied in other domains and may also be adapted to this challenge, e.g. data
programming [@arxiv:1605.07723]. In data programming, noisy automated labeling
functions are integrated. Numerous writers have described data as the new oil
[@url:http://ana.blogs.com/maestros/2006/11/data_is_the_new.html;
@url:https://medium.com/twenty-one-hundred/data-is-the-new-oil-a-ludicrous-proposition-1d91bba4f294].
The idea behind this metaphor is that data are available in large quantities,
valuable once refined, and the underlying resource that will enable a
data-driven revolution in how work is done. Contrasting with this perspective,
Ratner, Bach, and Ré described labeled training data as "The _New_ New Oil"
[@url:http://hazyresearch.github.io/snorkel/blog/weak_supervision.html]. In this
framing, data are abundant and not a scarce resource. Instead, new approaches to
solving problems arise when labeled training data become sufficient to enable
them. Based on our review of research on deep learning methods to categorize
disease, the latter framing rings true.

We expect improved methods for domains with limited data to play an important
role if deep learning is going to transform how we categorize states of human
health. We don't expect that deep learning methods will replace expert review.
We expect them to complement expert review by allowing more efficient use of the
costly practice of manual annotation.

#### Data sharing is hampered by standardization and privacy considerations

To construct the types of very large datasets that deep learning methods thrive
on, we need robust sharing of large collections of data. This is in part a
cultural challenge. We touch on this challenge in Discussion. Beyond
the cultural hurdles around data sharing, there are also technological and legal
hurdles related to sharing individual health records or deep models built from
such records. This subsection deals primarily with these challenges.

EHRs are designed chiefly for clinical, administrative and financial purposes,
such as patient care, insurance and  billing [@doi:10.1038/nrg3208]. Science is
at best a tertiary priority, presenting challenges to EHR-based research in
general and to deep learning research in particular. Although there is
significant work in the literature around EHR data quality and the impact on
research [@doi:10.1136/amiajnl-2011-000681], we focus on three types of
challenges: local bias, wider standards, and legal issues. Note these problems
are not restricted to EHRs but can also apply to any large biomedical dataset,
e.g. clinical trial data.

Even within the same healthcare system, EHRs can be used differently
[@pmid:24159271; @pmid:21347133]. Individual users have unique documentation and
ordering patterns, with different departments and different hospitals having
different priorities that code patients and introduce missing data in a
non-random fashion [@tag:Serden]. Patient data may be
kept across several "silos" within a single health system (e.g. separate
nursing documentation, registries, etc.). Even the most basic task of matching
patients across systems can be challenging due to data entry issues
[@pmid:27134610]. The situation is further exacerbated by the ongoing
introduction, evolution, and migration of EHR systems, especially where
reorganized and acquired healthcare facilities have to merge. Further, even the
ostensibly least-biased data type, laboratory measurements, can be biased based
by both the healthcare process and patient health state
[@doi:10.1016/j.jbi.2014.03.016]. As a result, EHR data can be less complete and
less objective than expected.

In the wider picture, standards for EHRs are numerous and evolving. Proprietary
systems, indifferent and scattered use of health information standards, and
controlled terminologies makes combining and comparison of data across systems
challenging [@doi:10.1016/j.jbi.2014.10.006]. Further diversity arises from
variation in languages, healthcare practices, and demographics. Merging EHRs
gathered in different systems (and even under different assumptions) is
challenging [@doi:10.1007/978-3-319-44839-8].

Combining or replicating studies across systems thus requires controlling for
both the above biases and dealing with mismatching standards. This has the
practical effect of reducing cohort size, limiting statistical significance,
preventing the detection of weak effects
[@doi:10.1590/2176-9451.19.4.027-029.ebo], and restricting the number of
parameters that can be trained in a model. Further, rules-based algorithms have
been popular in EHR-based research, but because these are developed at a single
institution and trained with a specific patient population, they do not transfer
easily to other healthcare systems [@doi:10.1136/amiajnl-2013-001935]. Genetic
studies using EHR data are subject to even more bias, as the differences in
population ancestry across health centers (e.g. proportion of patients with
African or Asian ancestry) can affect algorithm performance. For example, Wiley
et al. [@doi:10.1142/9789813207813_0050] showed that warfarin dosing algorithms
often under-perform in African Americans, illustrating that some of these issues
are unresolved even at a treatment best practices level. Lack of standardization
also makes it challenging for investigators skilled in deep learning to enter
the field, as numerous data processing steps must be performed before algorithms
are applied.

Finally, even if data were perfectly consistent and compatible across systems,
attempts to share and combine EHR data face considerable legal and ethical
barriers. Patient privacy can severely restrict the sharing and use of EHR data
[@doi:10.1093/ije/dyn022]. Here again, standards are heterogeneous and evolving,
but often EHR data can often not be exported or even accessed directly for
research purposes without appropriate consent. In the United States, research
use of EHR data is subject both to the Common Rule and the Health Insurance
Portability and Accountability Act (HIPAA). Ambiguity in the regulatory language
and individual interpretation of these rules can hamper use of EHR data
[@doi:10.1093/jamia/ocv111].  Once again, this has the effect of making data
gathering more laborious and expensive, reducing sample size and study power.

Several technological solutions have been proposed in this direction, allowing
access to sensitive data satisfying privacy and legal concerns. Software like
DataShield [@doi:10.1093/ije/dyu188] and ViPAR [@doi:10.1093/ije/dyv193],
although not EHR-specific, allows querying and combining of datasets and
calculation of summary statistics across remote sites by "taking the analysis to
the data". The computation is carried out at the remote site. Conversely, the
EH4CR project [@doi:10.1016/j.jbi.2014.10.006] allows analysis of private data
by use of an inter-mediation layer that interprets remote queries across
internal   formats and datastores and returns the results in a de-identified
standard form, thus giving real-time consistent but secure access. Continuous
Analysis [@doi:10.1038/nbt.3780] can allow reproducible computing on private
data. Using such techniques, intermediate results can be automatically tracked
and shared without sharing the original data. While none of these have been used
in deep learning, the potential is there.

Even without sharing data, algorithms trained on confidential patient data may
present security risks or accidentally allow for the exposure of individual
level patient data. Tramer et al. [@arxiv:1609.02943] showed the ability to
steal trained models via public application programming interfaces (APIs). Dwork
and Roth [@doi:10.1561/0400000042] demonstrate the ability to expose individual
level information from accurate answers in a machine learning model. Attackers
can use similar attacks to find out if a particular data instance was present in
the original training set for the machine learning model [@arxiv:1610.05820], in
this case, whether a person's record was present. This presents a potential
hazard for approaches that aim to generate data. Choi et al. propose generative
adversarial neural networks as a tool to make sharable EHR data
[@arxiv:1703.06490v1]; however, the authors did not take steps to protect the
model from such attacks.

There are approaches to protect models, but they pose their own challenges.
Training in a differentially private manner provides a limited guarantee that an
algorithm's output will be equally likely to occur regardless of the
participation of any one individual. The limit is determined by a single
parameter which provides a quantification of privacy. Simmons et al.
[@doi:10.1016/j.cels.2016.04.013] present the ability to perform genome-wide
association studies (GWASs) in a differentially private manner, and Abadi et al.
[@arxiv:1607.00133] show the ability to train deep learning classifiers under
the differential privacy framework. Federated learning
[@url:http://proceedings.mlr.press/v54/mcmahan17a.html] and secure aggregations
[@url:https://eprint.iacr.org/2017/281.pdf; @tag:Papernot2017_pate] are
complementary approaches that reinforce differential privacy. Both aim to
maintain privacy by training deep learning models from decentralized data
sources such as personal mobile devices without transferring actual training
instances. This is becoming of increasing importance with the rapid growth of
mobile health applications. However, the training process in these approaches
places constraints on the algorithms used and can make fitting a model
substantially more challenging. In our own experience, it can be trivial to
train a model without differential privacy, but quite difficult to train one
within the differential privacy framework. The problem can be particularly
pronounced with small sample sizes.

While none of these problems are insurmountable or restricted to deep learning,
they present challenges that cannot be ignored. Technical evolution in EHRs and
data standards will doubtless ease -- although not solve -- the problems of data
sharing and merging. More problematic are the privacy issues. Those applying
deep learning to the domain should consider the potential of inadvertently
disclosing the participants' identities. Techniques that enable training on data
without sharing the raw data may have a part to play. Training within a
differential privacy framework may often be warranted.

#### Discrimination and "right to an explanation" laws

In April 2016, the European Union adopted new rules regarding the use of
personal information, the General Data Protection Regulation
[@arxiv:1606.08813v3]. A component of these rules can be summed up by the phrase
"right to an explanation". Those who use machine learning algorithms must be
able to explain how a decision was reached. For example, a clinician treating a
patient who is aided by a machine learning algorithm may be expected to explain
decisions that use the patient's data. The new rules were designed to target
categorization or recommendation systems, which inherently profile individuals.
Such systems can do so in ways that are discriminatory and unlawful.

As datasets become larger and more complex, we may begin to identify
relationships in data that are important for human health but difficult to
understand. The algorithms described in this review and others like them may
become highly accurate and useful for various purposes, including within medical
practice. However, to discover and avoid discriminatory applications it will be
important to consider interpretability alongside accuracy. A number of
properties of genomic and healthcare data will make this difficult.

First, research samples are frequently non-representative of the general
population of interest; they tend to be disproportionately sick
[@doi:10.1086/512821], male [@doi:10.1016/j.neubiorev.2010.07.002], and European
in ancestry [@doi:10.1371/journal.pbio.1001661]. One well-known consequence of
these biases in genomics is that penetrance is consistently lower in the general
population than would be implied by case-control data, as reviewed in
[@doi:10.1086/512821]. Moreover, real genetic associations found in one
population may not hold in other populations with different patterns of linkage
disequilibrium (even when population stratification is explicitly controlled for
[@doi:10.1038/nrg2813]). As a result, many genomic findings are of limited value
for people of non-European ancestry [@doi:10.1371/journal.pbio.1001661] and may
even lead to worse treatment outcomes for them. Methods have been developed for
mitigating some of these problems in genomic studies [@doi:10.1086/512821;
@doi:10.1038/nrg2813], but it is not clear how easily they can be adapted for
deep models that are designed specifically to extract subtle effects from
high-dimensional data. For example, differences in the equipment that tended to
be used for cases versus controls have led to spurious genetic findings (e.g.
Sebastiani et al.'s retraction [@doi:10.1126/science.333.6041.404-a]).  In some
contexts, it may not be possible to correct for all of these differences to the
degree that a deep network is unable to use them. Moreover, the complexity of
deep networks makes it difficult to determine when their predictions are likely
to be based on such nominally-irrelevant features of the data (called "leakage"
in other fields [@doi:10.1145/2382577.2382579]). When we are not careful with
our data and models, we may inadvertently say more about the way the data was
collected (which may involve a history of unequal access and discrimination)
than about anything of scientific or predictive value. This fact can undermine
the privacy of patient data [@doi:10.1145/2382577.2382579] or lead to severe
discriminatory consequences [@doi:10.1111/j.1740-9713.2016.00960.x].

There is a small but growing literature on the prevention and mitigation of data
leakage [@doi:10.1145/2382577.2382579], as well as a closely-related literature
on discriminatory model behavior [@arxiv:1610.02413], but it remains difficult
to predict when these problems will arise, how to diagnose them, and how to
resolve them in practice. There is even disagreement about which kinds of
algorithmic outcomes should be considered discriminatory [@arxiv:1610.09559].
Despite the difficulties and uncertainties, machine learning practitioners (and
particularly those who use deep neural networks, which are challenging to
interpret) must remain cognizant of these dangers and make every effort to
prevent harm from discriminatory predictions. To reach their potential in this
domain, deep learning methods will need to be interpretable. Researchers need to
consider the extent to which biases may be learned by the model and whether or
not a model is sufficiently interpretable to identify bias. We discuss the
challenge of model interpretability more thoroughly in Discussion.

#### Applications of deep learning to longitudinal analysis

Longitudinal analysis follows a population across time, for example,
prospectively from birth or from the onset of particular conditions. In large
patient populations, longitudinal analyses such as the Framingham Heart Study
[@tag:Mahmood] and the Avon Longitudinal Study of Parents
and Children [@doi:10.1038/484155a] have yielded important discoveries about the
development of disease and the factors contributing to health status. Yet, a
common practice in EHR-based research is to take a snapshot at a point in time and
convert patient data to a traditional vector for machine learning and
statistical analysis. This results in loss of information as timing and order of
events can provide insight into a patient's disease and treatment
[@doi:10.2307/2281868]. Efforts to model sequences of events have shown promise
[@doi:10.1038/ncomms5022] but require exceedingly large patient sizes due to
discrete combinatorial bucketing. Lasko et al.
[@doi:10.1371/journal.pone.0066341] used autoencoders on longitudinal sequences
of serum urine acid measurements to identify population subtypes. More recently,
deep learning has shown promise working with both sequences (CNNs)
[@arxiv:1607.07519] and the incorporation of past and current state (RNNs,
LSTMs) [@arxiv:1602.00357]. This may be a particular area of opportunity for
deep neural networks. The ability to recognize relevant sequences of events from
a large number of trajectories requires powerful and flexible feature
construction methods -- an area in which deep neural networks excel.
