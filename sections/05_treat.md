## The impact of deep learning in treating disease and developing new treatments

*There will be some overlap with the Categorize section, and we may have to
determine which methods categorize individuals and which more directly match
patients with treatments.  The sub-section titles are merely placeholders.*

### Categorizing patients for clinical decision making

*How can deep learning match patients with clinical trails, therapies, or
other interventions?  As an example, [@doi:10.1016/j.jalz.2015.01.010]
predicts individuals who are most likely to decline during a clinical trial
and benefit from the treatment.*

### Effects of drugs on transcriptomic responses

*We discussed a few papers that operate on Library of Network-Based Cellular
Signatures (LINCS) gene expression data.  We could briefly introduce the
goals of that resource and comment on the deep learning applications.  In the
Issues, we had reservations about whether the improvements in expression
prediction are good enough to make a practical difference in the domain and
feature selection and construction.*

### Ligand-Based Prediction of Bioactivity

In the biomedical domain, high-throughput chemical screening aims to improve
therapeutic options and patient treatment options over a long term horizon
`TODO: add general screening reference`.  The objective is to discover which
small molecules (also referred to as chemical compounds or ligands) effectively
and specifically affect the activity of a target, such as a kinase,
protein-protein interaction, or broader cellular phenotype.  This screening
process can serve as the first step in the long drug discovery pipeline,
where novel chemicals are pursued for their ability to inhibit or enhance
disease-relevant biological mechanisms.  The appeal of machine learning in this
domain is the need to improve the efficiency of the initial high-throughput
screens such that sufficient candidate active compounds can be identified
without exhaustively screening libraries of hundreds of thousands or millions
of chemicals.  This task has been treated as a classification, regression, and
ranking problem.  In reality, it does not fit neatly into any of those
categories.  An ideal algorithm will rank a sufficient number of active
compounds before the inactives, but the rankings of actives relative to other
actives and inactives to other inactives is less important [@tag:Swamidass2009_irv].
`TODO: can improve this first attempt at an intro by reviewing more existing
literature on the topic` `TODO: check which other existing reviews should be
referenced`

We primarily focus on ligand-based approaches that train on chemicals' features
without requiring knowledge of the target, as opposed to alternative strategies
that use target features such as the protein structure `TODO: add examples`.
Neural networks have a long history in this domain [@tag:Baskin2015_drug_disc]
`TODO: can add additional references besides this review`, and the 2012 Merck
Molecular Activity Challenge on Kaggle `TODO: need URL?` generated substantial
excitement about the potential for high-parameter deep learning approaches.  The
winning submission was an ensemble that included a multitask multilayer
perceptron network [@tag:Dahl2014_multi_qsar], and the Merck sponsors noted
drastic improvements over a random forest (RF) baseline, remarking "we have
seldom seen any method in the past 10 years that could consistently outperform
RF by such a margin" [@tag:Ma2015_qsar_merck].  Subsequent work explored the
effects of jointly modeling far more targets than the Merck challenge
[@tag:Unterthiner2014_screening @tag:Ramsundar2015_multitask_drug], with
[@tag:Ramsundar2015_multitask_drug] showing that the benefits of multitask
networks had not yet saturated even with 259 targets.  Although a deep learning
approach, DeepTox, was also the overall winner of another competition, the
Toxicology in the 21st Century (Tox21) Data Challenge, it did not dominate
alternative methods as thoroughly as in other domains.  DeepTox was the top
performer on only 9 of 15 targets, and in some cases there was little separation
between DeepTox and the second best method on those 9 tasks.  A reliance on AUC
ROC `TODO: define here?` for the evaluation (see Discussion) further hinders the
ability to declare Tox21 as a deep learning success story.

In retrospect, the nuanced Tox21 performance may be more reflective of the
practical challenges encountered in ligand-based chemical screening than the
extreme enthusiasm generated by the Merck competition.  A study of 22 absorption
distribution, metabolism, excretion, and toxicity (ADMET) tasks demonstrated
that there are limitations to multitask transfer learning that are in part
a consequence of the degree to which tasks are related [@tag:Kearnes2016_admet].
Some of the ADMET datasets showed far superior performance in multitask models
of only the 22 ADMET tasks relative to more expansive multitasks that included
over 500 less-similar tasks.  `TODO: also has a good discussion of information
leakage in cross validation but that may be too detailed` In addition, training
datasets encountered in practical applications may be tiny relative to what is
available in public datasets and organized competitions.  A study of BACE-1
inhibitors included only 1547 compounds [@tag:Subramanian2016_bace1].  Machine
learning models were able to train on this limited dataset, but overfitting was
a challenge and the differences between random forests and a deep neural were
negligible, especially in the classification setting.  Overfitting is still a
problem in larger chemical screening datasets with tens or hundreds of thousands
of compounds because the number of active compounds can be very small, on the
order of 0.1% or 1% of all tested chemicals `TODO: verify those estimates`.
This is consistent with the strong performance of earlier, low-parameter
neural networks that emphasize compound-compound similarity
[@tag:Swamidass2009_irv  @tag:Lusci2015_irv] instead of predicting compound
activity directly from chemical features.

- "Creative experimentation" phase of the field, new ideas for representation
learning and novel approaches including graph convolutions, autoencoders, one
shot learning [@tag:AltaeTran2016_one_shot], and generative models (contrast
with descriptors and fingerprints)
- These "creative" approaches are definitely interesting but aren't necessarily
outperforming existing methods, improvements on the software and
reusability side could be important to help establish more rigorous
benchmarking, DeepChem as example of this
- Future outlook, what would need to happen for the "creative" approaches
to overtake the current state of the art, can representation learning be
improved by incorporating more information about chemical properties or
even more "tasks" during training, how much will future growth depend on
data versus algorithms
- Future outlook part 2, how the above approaches relate to traditional
methods like docking (note neural networks that include docking scores as
features), deep learning efforts in this direction that use structure (e.g.
[@tag:Wallach2015_atom_net @arxiv:1612.02751]), "zero-shot learning",
analogies to other domains where deep learning can capture the behavior
of complex physics (e.g. quantum physics example), maybe briefly mention
other compound-protein interaction-based networks although that doesn't seem
to fit here and is somewhat out of scope
- Future output part 3 (most speculative), what would successful generative
networks mean for the HTS field?

`TODO: other papers to add`

### Modeling Metabolism and Chemical Reactivity

*Add a review here of metabolism and chemical reactivity.*
