## Discussion

*This section provides meta-commentary that spans the Categorize, Study, and
Treat subject areas.  The candidate sub-sections below are initial ideas that
can be further pruned.*

### Evaluation

*What are the challenges in evaluating deep learning models that are specific to
this domain?  This can include a discussion of ROC versus precision-recall
curves for the imbalanced classes often encountered in biomedical datasets.
It could also mention alternative metrics that are used in specific sub-areas
such as enrichment factors in virtual screening.  A lack of true gold standard
data for some problems complicates both training and evaluation.  Confidence-
weighted labels are valuable when available.*

*Is progress in some biomedical areas slowed when new predictions (e.g. from
generative models) cannot be assessed by any human expert and require
experimental testing?  For example, contrast a painting or song generated by
a GAN versus a novel chemical compound.  Related is the idea that on some tasks
(e.g. the recent wave of deep learning versus MD image classification papers)
it is easy to tell when deep learning has produced a breakthrough because
human-level performance is an impressive baseline.  In many tasks we reviewed,
human-level performance is irrelevant.*

### Interpretation

As deep learning models achieve state-of-the-art performance in a variety of
domains, there is a growing need to make the models more
interpretable. Interpretability matters for two main reasons: first, a model
that achieves breakthrough performance may have identified patterns
in the data that practitioners in the field would like to understand
- however, this would not be possible if the model is a black-box. Second,
interpretability is important for trust: if a model making is medical diagnoses,
it is important to ensure the model is making decisions for reliable
reasons and is not focusing on an artifact of the data.
A motivating example of this can be found in Caruana et al.
(https://dl.acm.org/citation.cfm?id=2788613),
where a model trained to predict the likelihood of death from pneumonia assigned
lower risk to patients with asthma - but only because such patients were
treated as higher priority by the hospital. In the context of deep learning,
understanding the basis of a model's output is particularly important
as deep learning models are unusually susceptible to adversarial examples
[@tag:Nguyen2014_adversarial] and can output confidence scores over 99.99%
for samples that resemble pure noise. 

As the concept of interpretability is quite broad, many methods described as
improving the interpretability of deep learning models take disparate and
often complementary approaches. Some key themes are discussed below. 

#### Assigning example-specific importance scores

Several approaches ascribe importance on an example-specific basis 
to the parts of the input that are responsible for a particular output.
These can be broadly divided
into perturbation-based approaches and backpropagation-based approaches.

##### Perturbation-based approaches

These approaches make perturbations to individual inputs and observes
the impact on the output of the network. Zhou & Troyanskaya <cite> scored
genomic sequences by introducing virtual
mutations at each position and quantifying the change in the output.
<LIME people> introduced LIME which constructs a linear model to
locally approximate the output
of the network on perturbed versions of the input and assigned importance
scores accordingly. For analyzing images, Zeiler & Fergus <cite> applied
constant-value masks to different input patches and studied the changes in
the activations of later layers. As an alternative to using masks, which can
produce misleading results, Zintgraf et al. proposed a novel strategy based
on marginalizing over plausible values of an input patch to more accurately
estimate its contribution.

A common drawback to perturbation-based approaches is computational efficiency:
each perturbed version of an input requires a separate forward propagation
through the network to compute the output. As noted by Shrikumar et al., such
methods may also underestimate the impact of features that have saturated their
contribution to the output, as can happen when multiple redundant features
are present.

To reduce the computational overhead of perturbation-based approaches,
Ruth & Vedaldi (2017) <cite> solve an optimization problem using
gradient descent to discover a minimal subset
of inputs to perturb in order to decrease the predicted probability of a
selected class. When tested on image data,
their method took about 300 iterations to converge, compared to the
~5000 iterations used by LIME. One drawback of this approach is that
the use of gradient descent requires the perturbation to have a
differentiable form.

##### Backpropagation-based approaches

A second strategy for addressing the computational inefficiency of
perturbation-based approaches is to propagate an important signal from
a target output neuron backwards through the layers to the input layer
in a single backpropagation-like pass. A classic example of this
calculating the gradients of the output w.r.t. the input <cite Simonyan>
to compute a 'saliency map'. Bach et al. 2015
 <cite> proposed a strategy called Layerwise Relevance Propagation, which was
 shown to be equivalent to the elementwise product of the gradient and
 input <cite>. Several variants of gradients exist which differ
in their handling of the ReLU nonlinearity: while gradients zero-out the
importance signal at ReLUs if the input to the ReLU is negative,
deconvolutional networks <cite> zero-out the importance signal if
 the signal itself is negative. Guided backpropagation <cite>
 combines the two strategies to zero-out the importance signal if either
 the input to ReLU is negative or the importance signal is negative,
 in effect discarding negative gradients.
 However, <cite> showed that while guided backpropagation excelled at
 identifying salient features in the input image, these features showed little
 class-specificity, producing very similar saliency maps regardless
 of the class under consideration. <cite> attempted to alleviate this
 by combining gradients and guided backpropagation
 in Guided Grad-CAM: feature maps in the last convolutional layer were
 associated with classes using gradients, and the weighted activation of
 these feature maps was multiplied with the result of guided backpropagation
 to introduce more class specificity. Note that these approaches still would
 not highlight features that have saturated their contribution to the output,
 as the gradients w.r.t. such features would be zero at the input. 

To address the saturation failure mode, strategies have been developed
 to consider how the output changes between some reference input
 and the actual input, where the reference input represents a 'null' input that it
 is informative to measure differences against.
 Sundararajan et al., 2016 <cite> integrated the
 gradients as the input was linearly increased from the reference to its
 actual value (in their examples, which were on image-like data, they
 used a reference of all zeros). While the numerical integration adds computational
 overhead, the method is still more efficient on average than perturbation approaches.
 Further, by relying only on the gradients, the method is a fully black-box
 approach that is guaranteed to give the same answer for functionally
 equivalent networks. Shrikumar et al., 2017 <cite> developed DeepLIFT,
 a strategy that used the difference
 between a neuron's activation on the reference input compared to its
 activation on theactual
 input to improve the backpropagation of importance scores. DeepLIFT is
 a white-box method that requires knowledge of the network architecture,
 but it is more computationally efficient than integrated gradients.
 Lundberg & Lee., 2016 <cite> noted that
 several importance scoring methods, including DeepLIFT,
 integrated gradients and LIME,
 could all be considered
 approximations to the Shapely values <cite>, which have a long history
 in game theory for assigning contributions to players in cooperative games. 
 DeepLIFT introduced a modification which treated positive and negative
 contributions separately to address some failure cases of
 integrated gradients; the modification can be understood as an improved
 approximation of the Shapely values.

#### Matching or exaggerating the hidden representation 

Another approach to understanding the network's predictions
is to find artifical inputs that produce similar hidden
representations to a chosen example. This can elucidate the features
that the network uses for prediction and drop the features that the
network is insensitive to. In the context of natural images,
Mahendran & Vedaldi, 2015
<cite; 'Understanding deep image representations
by inverting them'>
introduced the "inversion" visualization which uses gradient descent and
backpropagation to reconstruct the input from its hidden representation.
The method required placing a prior on the input to favour results which
resemble natural images. 
For genomic sequence, <cite> used a MCMC algorithm to find the
maximum-entropy distribution of inputs that produced a similar hidden
representation to the chosen input. 

A related idea is 'caricaturization', where an initial image is altered to
exaggerate patterns that the net searches for
<cite 'Visualizing Deep Convolutional Neural Networks Using Natural
Pre-images'>. This is done by maximizing
the response of neurons that are active in the network, subject to some
regularizing constraints. <cite deep dream people> leveraged caricaturization to
generate aesthetically pleasing images using neural networks.

#### Activation maximization

Activation maximization can reveal patterns
detected by an individual neuron in the network by generating
images which maximally activate that neuron, subject
to some regularizing constraints. This technique was first introduced
in Ehran et al., (2009) and applied in Simonyan et al. (2014) <cite>,
Mordvintsev et al. (2015) <cite>,
Yosinksi et al. (2015) and Mahendran & Vedaldi (2016). Lanchatin et al. (2016)
<cite https://arxiv.org/abs/1608.03644> applied activation maximization to
genomic sequence. One drawback of
this approach is that neural networks often learn highly distributed
representations where several neurons cooperatively describe a pattern
of interest - thus, visualizing patterns learned by individual neurons
may not always be informative.

#### RNN-specific approaches

Several interpretation methods are specifically tailored to
recurrent neural network architecutres. A few key approaches are summarised
 below.

The most common form of interpretability provided by RNNs is through
attention mechanisms, which have been used in diverse problems such
as image captioning and
machine translation to select portions of the input to focus on for generating
a particular output <cite NMT: https://arxiv.org/abs/1409.0473,
and image captioning: https://arxiv.org/pdf/1502.03044.pdf>.
 Deming et al., <cite: https://arxiv.org/abs/1605.07156>
 applied the attention mechanism to models trained on genomic sequence.
 Attention mechanisms provide insight into the model's
 decision-making process by revealing which portions of the input are
 used by different outputs.
 In the clinical domain,
 Choi et al. (2016)
 <cite https://arxiv.org/pdf/1608.05745.pdf>
 leveraged attention mechanisms to highlight which aspects of
 a patient's medical history were most relevant for making
 diagnoses. Choi et al. [@tag:Choi2016_gram] later extended this work to
 take into account the structure of
 disease ontologies and found that the concepts represented by the model
 were aligned with medical knowledge.
 Note that interpretation strategies that rely on an attention mechanism
 do not provide insight into the internal logic
 used by the attention layer to decide which inputs to attend to.

Visualizing the activation patterns of the hidden state of a
 recurrent neural network can also be instructive.
 Early work by Ghosh & Karamcheti (1992)
 <cite http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=993054>
 used cluster analysis to study comparatively small hidden states of
 networks trained to recognise strings from a finite state machine.
 More recently, Karpathy et al. (2015)
<cite https://arxiv.org/abs/1506.02078> showed the existence of individual
cells in LSTMs that kept track of quotes and brackets in character-level
 language models. To facilitate such analyses, LSTM vis
 <cite https://arxiv.org/abs/1606.07461> allows
 interactive exploration of the hidden state of LSTMs on different inputs.

Another strategy, adopted by Lanchatin et al., 2016
<cite https://arxiv.org/pdf/1608.03644.pdf> looks at how the output of
 a recurrent neural network changes as longer and longer subsequences
 are supplied as input to the network,  where the subsequences begin
 with just the first position and end with the entire sequence.
 In a binary classification task, this can identify those positions
which are responsible for flipping
 the output of the network from negative to positive. If the RNN is
 bidirectional, the same process can be repeated on the reverse sequence.
 As noted by the authors, this approach was less effective at
 identifying motifs compared to the gradient-based
  backpropagation approach of Simonyan et al., illustrating the need
 for more sophisticated strategies to assign importance scores in
 recurrent neural networks.

Murdoch & Szlam <cite https://arxiv.org/pdf/1702.02540.pdf>
 showed that the output of an LSTM can be decomposed into a product
 of factors where each factor can be interpreted as the contribution
 at a particular
 timestep. The contribution scores were then used to identify
 key phrases from a model trained to do sentiment analysis, and obtained
 superior results compared to a gradient-based approach.

####Other

Toward quantifying the uncertainty of
predictions, there has been a renewed interest in confidence intervals for
deep neural networks. Early work from Chryssolouris et al
[@tag:Chryssolouris1996_confidence] provided confidence intervals under the
 assumption of normally distributed error. A more recent technique
 known as test-time dropout <cite https://arxiv.org/abs/1506.02142>
can also be
used to obtain a probabilistic interpretation  of a network's outputs.

It can often be informative to understand how the training data
 affects the learning of a model. Toward this end,
 Koh & Liang <cite https://arxiv.org/pdf/1703.04730.pdf> used
 influence functions, a technique from robust statistics, to
 trace a model's predictions back through the learning algorithm to identify
 the datapoints in the training set that had the most impact on a given
 prediction.

A more free-form approach to interpretability is to visualise
 the activation patterns of the network on individual inputs and on
 subsets of the data. ActiVis, developed at Facebook,
 <cite https://arxiv.org/pdf/1704.01942.pdf> enables interactive
 visualisation and exploration of industry-scale
 deep learning models.

An orthogonal strategy is to use a knowledge distillation approach
 to replace a deep learning model with a more interpretable model
 that achieves comparable performance. Towards this end,
 Che et al [@tag:Che2015_distill] used gradient boosted trees to learn
 interpretable healthcare features from trained deep models.

Finally, it is sometimes possible to train the model to
 provide justifications for its predictions. Lei et al.,
 <cite https://arxiv.org/abs/1606.04155> used a generator to identify
 "rationales", which are short and coherent pieces of the input text that
 produce similar results to the whole input when passed through an encoder.
 The authors applied their approach to a sentiment analysis task
 and obtained substantially superior results compared to an attention-based
 method.


### Data limitations

*Related to evaluation, are there data quality issues in genomic, clinical, and
other data that make this domain particularly challenging?  Are these worse than
what is faced in other non-biomedical domains?*

*Many applications have used relatively small training datasets.  We might
discuss workarounds (e.g. semi-synthetic data, splitting instances, etc.) and
how this could impact future progress.  Might this be why some studies have
resorted to feature engineering instead of learning representations from low-
level features?  Is there still work to be done in finding the right low-level
features in some problems?*

###### Biomedical data is often "Wide"

*Biomedical studies typically deal with relatively small sample sizes but each
sample may have millions of measurements (genotypes and other omics data, lab
tests etc).*

*Classical machine learning recommendations were to have 10x samples per number
of parameters in the model.*

*Number of parameters in an MLP. Convolutions and similar strategies help but do
not solve*

*Bengio diet networks paper*


### Hardware limitations and scaling

*Several papers have stated that memory or other hardware limitations
artificially restricted the number of training instances, model inputs/outputs,
hidden layers, etc.  Is this a general problem worth discussing or will it be
solved naturally as hardware improves and/or groups move to distributed deep
learning frameworks?  Does hardware limit what types of problems are accessible
to the average computational group, and if so, will that limit future progress?
For instance, some hyperparameter search strategies are not feasible for a lab
with only a couple GPUs.*

*Some of this is also outlined in the Categorize section.  We can decide where
it best fits.*

Efficiently scaling deep learning is challenging, and there is a high
computational cost (e.g., time, memory, energy) associated with training neural
networks and using them for classification. As such, neural networks
have only recently found widespread use [@tag:Schmidhuber2014_dnn_overview].

Many have sought to curb the costs of deep learning, with methods ranging from
the very applied (e.g., reduced numerical precision [@tag:Gupta2015_prec
@tag:Bengio2015_prec @tag:Sa2015_buckwild @tag:Hubara2016_qnn]) to the exotic
and theoretic (e.g., training small networks to mimic large networks and
ensembles [@tag:Caruana2014_need @tag:Hinton2015_dark_knowledge]). The largest
gains in efficiency have come from computation with graphics processing units
(GPUs) [@tag:Raina2009_gpu @tag:Vanhoucke2011_cpu @tag:Seide2014_parallel
@tag:Hadjas2015_cc @tag:Edwards2015_growing_pains
@tag:Schmidhuber2014_dnn_overview], which excel at the matrix and vector
operations so central to deep learning. The massively parallel nature of GPUs
allows additional optimizations, such as accelerated mini-batch gradient
descent [@tag:Vanhoucke2011_cpu @tag:Seide2014_parallel @tag:Su2015_gpu
@tag:Li2014_minibatch]. However, GPUs also have a limited quantity of memory,
making it difficult to implement networks of significant size and
complexity on a single GPU or machine [@tag:Raina2009_gpu
@tag:Krizhevsky2013_nips_cnn]. This restriction has sometimes forced
computational biologists to use workarounds or limit the size of an analysis.
For example, Chen et al. [@tag:Chen2016_gene_expr] aimed to infer the
expression level of all genes with a single neural network, but due to
memory restrictions they randomly partitioned genes into two halves and
analyzed each separately. In other cases, researchers limited the size
of their neural network [@tag:Wang2016_protein_contact
@tag:Gomezb2016_automatic]. Some have also chosen to use slower
CPU implementations rather than sacrifice network size or performance
[@tag:Yasushi2016_cgbvs_dnn].

Steady improvements in GPU hardware may alleviate this issue somewhat, but it
is not clear whether they can occur quickly enough to keep up with the growing
amount of available biological data or increasing network sizes. Much has
been done to minimize the memory
requirements of neural networks [@tag:CudNN @tag:Caruana2014_need
@tag:Gupta2015_prec @tag:Bengio2015_prec @tag:Sa2015_buckwild
@tag:Chen2015_hashing @tag:Hubara2016_qnn], but there is also growing
interest in specialized hardware, such as field-programmable gate arrays
(FPGAs) [@tag:Edwards2015_growing_pains @tag:Lacey2016_dl_fpga] and
application-specific integrated circuits (ASICs). Specialized hardware promises
improvements in deep learning at reduced time, energy, and memory
[@tag:Edwards2015_growing_pains]. Logically, there is less software for highly
specialized hardware [@tag:Lacey2016_dl_fpga], and it could be a difficult
investment for those not solely interested in deep learning. However, it is
likely that such options will find increased support as they become a more
popular platform for deep learning and general computation.

Distributed computing is a general solution to intense computational
requirements, and has enabled many large-scale deep learning efforts. Early
approaches to distributed computation [@tag:Mapreduce @tag:Graphlab] were
not suitable for deep learning [@tag:Dean2012_nips_downpour],
but significant progress has been made. There
now exist a number of algorithms [@tag:Dean2012_nips_downpour @tag:Dogwild
@tag:Sa2015_buckwild], tools [@tag:Moritz2015_sparknet @tag:Meng2016_mllib
@tag:TensorFlow], and high-level libraries [@tag:Keras @tag:Elephas] for deep
learning in a distributed environment, and it is possible to train very complex
networks with limited infrastructure [@tag:Coates2013_cots_hpc]. Besides
handling very large networks, distributed or parallelized approaches offer
other advantages, such as improved ensembling [@tag:Sun2016_ensemble] or
accelerated hyperparameter optimization [@tag:Bergstra2011_hyper
@tag:Bergstra2012_random].

Cloud computing, which has already seen adoption in genomics
[@tag:Schatz2010_dna_cloud], could facilitate easier sharing of the large
datasets common to biology [@tag:Gerstein2016_scaling @tag:Stein2010_cloud],
and may be key to scaling deep learning. Cloud computing affords researchers
significant flexibility, and enables the use of specialized hardware (e.g.,
FPGAs, ASICs, GPUs) without significant investment. With such flexibility, it
could be easier to address the different challenges associated with the
multitudinous layers and architectures available
[@tag:Krizhevsky2014_weird_trick]. Though many are reluctant to store sensitive
data (e.g., patient electronic health records) in the cloud,
secure/regulation-compliant cloud services do exist [@tag:RAD2010_view_cc].

*TODO: Write the transition once more of the Discussion section has been
fleshed out.*

### Code, data, and model sharing

*Reproducibiliy is important for science to progress. In the context of deep
learning applied to advance human healthcare, does reproducibility have
different requirements or alternative connotations? With vast hyperparameter
spaces, massively heterogeneous and noisy biological data sets, and black box
interpretability problems, how can we best ensure reproducible models? What
might a clinician, or policy maker, need to see in a deep model in order to
influence healthcare decisions? Or, is deep learning a hypothesis generation
machine that requires manual validation? DeepChem and DragoNN are worth
discussing here.*

### Transfer learning/transferability of features

* https://github.com/greenelab/deep-review/issues/139#issuecomment-268901804
