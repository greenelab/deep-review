<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Travers Ching" />
  <meta name="author" content="Daniel S. Himmelstein" />
  <meta name="author" content="Brett K. Beaulieu-Jones" />
  <meta name="author" content="Alexandr A. Kalinin" />
  <meta name="author" content="Brian T. Do" />
  <meta name="author" content="Gregory P. Way" />
  <meta name="author" content="Enrico Ferrero" />
  <meta name="author" content="Paul-Michael Agapow" />
  <meta name="author" content="Michael Zietz" />
  <meta name="author" content="Michael M. Hoffman" />
  <meta name="author" content="Wei Xie" />
  <meta name="author" content="Gail L. Rosen" />
  <meta name="author" content="Benjamin J. Lengerich" />
  <meta name="author" content="Johnny Israeli" />
  <meta name="author" content="Jack Lanchantin" />
  <meta name="author" content="Stephen Woloszynek" />
  <meta name="author" content="Anne E. Carpenter" />
  <meta name="author" content="Avanti Shrikumar" />
  <meta name="author" content="Jinbo Xu" />
  <meta name="author" content="Evan M. Cofer" />
  <meta name="author" content="Christopher A. Lavender" />
  <meta name="author" content="Srinivas C. Turaga" />
  <meta name="author" content="Amr M. Alexandari" />
  <meta name="author" content="Zhiyong Lu" />
  <meta name="author" content="David J. Harris" />
  <meta name="author" content="Dave DeCaprio" />
  <meta name="author" content="Yanjun Qi" />
  <meta name="author" content="Anshul Kundaje" />
  <meta name="author" content="Yifan Peng" />
  <meta name="author" content="Laura K. Wiley" />
  <meta name="author" content="Marwin H.S. Segler" />
  <meta name="author" content="Simina M. Boca" />
  <meta name="author" content="S. Joshua Swamidass" />
  <meta name="author" content="Austin Huang" />
  <meta name="author" content="Anthony Gitter" />
  <meta name="author" content="Casey S. Greene" />
  <meta name="dcterms.date" content="2019-07-23" />
  <meta name="keywords" content="deep learning, review, precision medicine, genomics, machine learning, neural networks, collaborative, manubot" />
  <title>Opportunities and obstacles for deep learning in biology and medicine: 2019 update</title>
  <style>
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Opportunities and obstacles for deep learning in biology and medicine: 2019 update</h1>
</header>
<p><em>The published version of this manuscript is available at <a href="https://doi.org/10.1098/rsif.2017.0387" class="uri">https://doi.org/10.1098/rsif.2017.0387</a></em>.</p>
<p><small><em> This manuscript (<a href="https://greenelab.github.io/deep-review/v/f000c4c7a6ff03605235db8f1b3fd6f6ab5b47ad/">permalink</a>) was automatically generated from <a href="https://github.com/greenelab/deep-review/tree/f000c4c7a6ff03605235db8f1b3fd6f6ab5b47ad">greenelab/deep-review@f000c4c</a> on July 23, 2019. </em></small></p>
<h2 id="authors">Authors</h2>
<p><a href="https://orcid.org/0000-0002-5577-3516"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> Travers Ching<sup>1,☯</sup>, <a href="https://orcid.org/0000-0002-3012-7446"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> Daniel S. Himmelstein<sup>2</sup>, <a href="https://orcid.org/0000-0002-6700-1468"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> Brett K. Beaulieu-Jones<sup>3</sup>, <a href="https://orcid.org/0000-0003-4563-3226"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> Alexandr A. Kalinin<sup>4</sup>, <a href="https://orcid.org/0000-0003-4992-2623"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> Brian T. Do<sup>5</sup>, <a href="https://orcid.org/0000-0002-0503-9348"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> Gregory P. Way<sup>2</sup>, <a href="https://orcid.org/0000-0002-8362-100X"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> Enrico Ferrero<sup>6</sup>, <a href="https://orcid.org/0000-0003-1126-1479"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> Paul-Michael Agapow<sup>7</sup>, <a href="https://orcid.org/0000-0003-0539-630X"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> Michael Zietz<sup>2</sup>, <a href="https://orcid.org/0000-0002-4517-1562"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> Michael M. Hoffman<sup>8,9,10</sup>, <a href="https://orcid.org/0000-0002-1871-6846"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> Wei Xie<sup>11</sup>, <a href="https://orcid.org/0000-0003-1763-5750"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> Gail L. Rosen<sup>12</sup>, <a href="https://orcid.org/0000-0001-8690-9554"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> Benjamin J. Lengerich<sup>13</sup>, <a href="https://orcid.org/0000-0003-1633-5780"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> Johnny Israeli<sup>14</sup>, <a href="https://orcid.org/0000-0003-0811-0944"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> Jack Lanchantin<sup>15</sup>, <a href="https://orcid.org/0000-0003-0568-298X"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> Stephen Woloszynek<sup>12</sup>, <a href="https://orcid.org/0000-0003-1555-8261"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> Anne E. Carpenter<sup>16</sup>, <a href="https://orcid.org/0000-0002-6443-4671"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> Avanti Shrikumar<sup>17</sup>, <a href="https://orcid.org/0000-0001-7111-4839"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> Jinbo Xu<sup>18</sup>, <a href="https://orcid.org/0000-0003-3877-0433"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> Evan M. Cofer<sup>19,20</sup>, <a href="https://orcid.org/0000-0002-7762-1089"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> Christopher A. Lavender<sup>21</sup>, <a href="https://orcid.org/0000-0003-3247-6487"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> Srinivas C. Turaga<sup>22</sup>, <a href="https://orcid.org/0000-0001-8655-8109"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> Amr M. Alexandari<sup>17</sup>, <a href="https://orcid.org/0000-0001-9998-916X"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> Zhiyong Lu<sup>23</sup>, <a href="https://orcid.org/0000-0003-3332-9307"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> David J. Harris<sup>24</sup>, <a href="https://orcid.org/0000-0001-8931-9461"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> Dave DeCaprio<sup>25</sup>, <a href="https://orcid.org/0000-0002-5796-7453"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> Yanjun Qi<sup>15</sup>, <a href="https://orcid.org/0000-0003-3084-2287"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> Anshul Kundaje<sup>17,26</sup>, <a href="https://orcid.org/0000-0001-9309-8331"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> Yifan Peng<sup>23</sup>, <a href="https://orcid.org/0000-0001-6681-9754"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> Laura K. Wiley<sup>27</sup>, <a href="https://orcid.org/0000-0001-8008-0546"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> Marwin H.S. Segler<sup>28</sup>, <a href="https://orcid.org/0000-0002-1400-3398"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> Simina M. Boca<sup>29</sup>, <a href="https://orcid.org/0000-0003-2191-0778"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> S. Joshua Swamidass<sup>30</sup>, <a href="https://orcid.org/0000-0003-1349-4030"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> Austin Huang<sup>31</sup>, <a href="https://orcid.org/0000-0002-5324-9833"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> Anthony Gitter<sup>32,33,†</sup>, <a href="https://orcid.org/0000-0001-8713-9213"><img src="images/orcid.svg" alt="ORCID icon" width="11" height="11" /></a> Casey S. Greene<sup>2,†</sup></p>
<p><sup>☯</sup> — Author order was determined with a randomized algorithm<br> <sup>†</sup> — To whom correspondence should be addressed: gitter@biostat.wisc.edu (A.G.) and greenescientist@gmail.com (C.S.G.) <small></p>
<ol type="1">
<li>Molecular Biosciences and Bioengineering Graduate Program, University of Hawaii at Manoa, Honolulu, HI</li>
<li>Department of Systems Pharmacology and Translational Therapeutics, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA</li>
<li>Genomics and Computational Biology Graduate Group, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA</li>
<li>Department of Computational Medicine and Bioinformatics, University of Michigan Medical School, Ann Arbor, MI</li>
<li>Harvard Medical School, Boston, MA</li>
<li>Computational Biology and Stats, Target Sciences, GlaxoSmithKline, Stevenage, United Kingdom</li>
<li>Data Science Institute, Imperial College London, London, United Kingdom</li>
<li>Princess Margaret Cancer Centre, Toronto, ON, Canada</li>
<li>Department of Medical Biophysics, University of Toronto, Toronto, ON, Canada</li>
<li>Department of Computer Science, University of Toronto, Toronto, ON, Canada</li>
<li>Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN</li>
<li>Ecological and Evolutionary Signal-processing and Informatics Laboratory, Department of Electrical and Computer Engineering, Drexel University, Philadelphia, PA</li>
<li>Computational Biology Department, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA</li>
<li>Biophysics Program, Stanford University, Stanford, CA</li>
<li>Department of Computer Science, University of Virginia, Charlottesville, VA</li>
<li>Imaging Platform, Broad Institute of Harvard and MIT, Cambridge, MA</li>
<li>Department of Computer Science, Stanford University, Stanford, CA</li>
<li>Toyota Technological Institute at Chicago, Chicago, IL</li>
<li>Department of Computer Science, Trinity University, San Antonio, TX</li>
<li>Lewis-Sigler Institute for Integrative Genomics, Princeton University, Princeton, NJ</li>
<li>Integrative Bioinformatics, National Institute of Environmental Health Sciences, National Institutes of Health, Research Triangle Park, NC</li>
<li>Howard Hughes Medical Institute, Janelia Research Campus, Ashburn, VA</li>
<li>National Center for Biotechnology Information and National Library of Medicine, National Institutes of Health, Bethesda, MD</li>
<li>Department of Wildlife Ecology and Conservation, University of Florida, Gainesville, FL</li>
<li>ClosedLoop.ai, Austin, TX</li>
<li>Department of Genetics, Stanford University, Stanford, CA</li>
<li>Division of Biomedical Informatics and Personalized Medicine, University of Colorado School of Medicine, Aurora, CO</li>
<li>Institute of Organic Chemistry, Westfälische Wilhelms-Universität Münster, Münster, Germany</li>
<li>Innovation Center for Biomedical Informatics, Georgetown University Medical Center, Washington, DC</li>
<li>Department of Pathology and Immunology, Washington University in Saint Louis, Saint Louis, MO</li>
<li>Department of Medicine, Brown University, Providence, RI</li>
<li>Department of Biostatistics and Medical Informatics, University of Wisconsin-Madison, Madison, WI</li>
<li>Morgridge Institute for Research, Madison, WI</li>
</ol>
<p></small></p>
<h2 id="abstract" class="page_break_before">Abstract</h2>
<p>Deep learning describes a class of machine learning algorithms that are capable of combining raw inputs into layers of intermediate features. These algorithms have recently shown impressive results across a variety of domains. Biology and medicine are data-rich disciplines, but the data are complex and often ill-understood. Hence, deep learning techniques may be particularly well-suited to solve problems of these fields. We examine applications of deep learning to a variety of biomedical problems—patient classification, fundamental biological processes, and treatment of patients—and discuss whether deep learning will be able to transform these tasks or if the biomedical sphere poses unique challenges. Following from an extensive literature review, we find that deep learning has yet to revolutionize biomedicine or definitively resolve any of the most pressing challenges in the field, but promising advances have been made on the prior state of the art. Even though improvements over previous baselines have been modest in general, the recent progress indicates that deep learning methods will provide valuable means for speeding up or aiding human investigation. Though progress has been made linking a specific neural network’s prediction to input features, understanding how users should interpret these models to make testable hypotheses about the system under study remains an open challenge. Furthermore, the limited amount of labeled data for training presents problems in some domains, as do legal and privacy constraints on work with sensitive health records. Nonetheless, we foresee deep learning enabling changes at both bench and bedside with the potential to transform several areas of biology and medicine.</p>
<h2 id="introduction-to-deep-learning">Introduction to deep learning</h2>
<p>Biology and medicine are rapidly becoming data-intensive. A recent comparison of genomics with social media, online videos, and other data-intensive disciplines suggests that genomics alone will equal or surpass other fields in data generation and analysis within the next decade <span class="citation" data-cites="13bxiY1vo">[<a href="#ref-13bxiY1vo" role="doc-biblioref">1</a>]</span>. The volume and complexity of these data present new opportunities, but also pose new challenges. Automated algorithms that extract meaningful patterns could lead to actionable knowledge and change how we develop treatments, categorize patients, or study diseases, all within privacy-critical environments.</p>
<p>The term <em>deep learning</em> has come to refer to a collection of new techniques that, together, have demonstrated breakthrough gains over existing best-in-class machine learning algorithms across several fields. For example, over the past five years these methods have revolutionized image classification and speech recognition due to their flexibility and high accuracy <span class="citation" data-cites="BeijBSRE">[<a href="#ref-BeijBSRE" role="doc-biblioref">2</a>]</span>. More recently, deep learning algorithms have shown promise in fields as diverse as high-energy physics <span class="citation" data-cites="TDruxF1s">[<a href="#ref-TDruxF1s" role="doc-biblioref">3</a>]</span>, computational chemistry <span class="citation" data-cites="zCt6PUXj">[<a href="#ref-zCt6PUXj" role="doc-biblioref">4</a>]</span>, dermatology <span class="citation" data-cites="XnYNYoYB">[<a href="#ref-XnYNYoYB" role="doc-biblioref">5</a>]</span>, and translation among written languages <span class="citation" data-cites="4TK06zOf">[<a href="#ref-4TK06zOf" role="doc-biblioref">6</a>]</span>. Across fields, “off-the-shelf” implementations of these algorithms have produced comparable or higher accuracy than previous best-in-class methods that required years of extensive customization, and specialized implementations are now being used at industrial scales.</p>
<p>Deep learning approaches grew from research on artificial neurons, which were first proposed in 1943 <span class="citation" data-cites="1HVDhhwpK">[<a href="#ref-1HVDhhwpK" role="doc-biblioref">7</a>]</span> as a model for how the neurons in a biological brain process information. The history of artificial neural networks—referred to as “neural networks” throughout this article—is interesting in its own right <span class="citation" data-cites="1G5eCiq4d">[<a href="#ref-1G5eCiq4d" role="doc-biblioref">8</a>]</span>. In neural networks, inputs are fed into the input layer, which feeds into one or more hidden layers, which eventually link to an output layer. A layer consists of a set of nodes, sometimes called “features” or “units,” which are connected via edges to the immediately earlier and the immediately deeper layers. In some special neural network architectures, nodes can connect to themselves with a delay. The nodes of the input layer generally consist of the variables being measured in the dataset of interest—for example, each node could represent the intensity value of a specific pixel in an image or the expression level of a gene in a specific transcriptomic experiment. The neural networks used for deep learning have multiple hidden layers. Each layer essentially performs feature construction for the layers before it. The training process used often allows layers deeper in the network to contribute to the refinement of earlier layers. For this reason, these algorithms can automatically engineer features that are suitable for many tasks and customize those features for one or more specific tasks.</p>
<p>Deep learning does many of the same things as more familiar machine learning approaches. In particular, deep learning approaches can be used both in <em>supervised</em> applications—where the goal is to accurately predict one or more labels or outcomes associated with each data point—in the place of regression approaches, as well as in <em>unsupervised</em>, or “exploratory” applications—where the goal is to summarize, explain, or identify interesting patterns in a data set—as a form of clustering. Deep learning methods may in fact combine both of these steps. When sufficient data are available and labeled, these methods construct features tuned to a specific problem and combine those features into a predictor. In fact, if the dataset is “labeled” with binary classes, a simple neural network with no hidden layers and no cycles between units is equivalent to logistic regression if the output layer is a sigmoid (logistic) function of the input layer. Similarly, for continuous outcomes, linear regression can be seen as a single-layer neural network. Thus, in some ways, supervised deep learning approaches can be seen as an extension of regression models that allow for greater flexibility and are especially well-suited for modeling non-linear relationships among the input features. Recently, hardware improvements and very large training datasets have allowed these deep learning techniques to surpass other machine learning algorithms for many problems. In a famous and early example, scientists from Google demonstrated that a neural network “discovered” that cats, faces, and pedestrians were important components of online videos <span class="citation" data-cites="IiNJE32f">[<a href="#ref-IiNJE32f" role="doc-biblioref">9</a>]</span> without being told to look for them. What if, more generally, deep learning takes advantage of the growth of data in biomedicine to tackle challenges in this field? Could these algorithms identify the “cats” hidden in our data—the patterns unknown to the researcher—and suggest ways to act on them? In this review, we examine deep learning’s application to biomedical science and discuss the unique challenges that biomedical data pose for deep learning methods.</p>
<p>Several important advances make the current surge of work done in this area possible. Easy-to-use software packages have brought the techniques of the field out of the specialist’s toolkit to a broad community of computational scientists. Additionally, new techniques for fast training have enabled their application to larger datasets <span class="citation" data-cites="3qm8sXnB">[<a href="#ref-3qm8sXnB" role="doc-biblioref">10</a>]</span>. Dropout of nodes, edges, and layers makes networks more robust, even when the number of parameters is very large. Finally, the larger datasets now available are also sufficient for fitting the many parameters that exist for deep neural networks. The convergence of these factors currently makes deep learning extremely adaptable and capable of addressing the nuanced differences of each domain to which it is applied.</p>
<figure>
<img src="https://user-images.githubusercontent.com/542643/34995383-52363dc0-faa4-11e7-8cdd-47966c1a2a0f.png" alt="Figure 1: Neural networks come in many different forms. Left: a key for the various types of nodes used in neural networks. Simple FFNN: a feed forward neural network in which inputs are connected via some function to an output node and the model is trained to produce some output for a set of inputs. MLP: the multi-layer perceptron is a feed forward neural network in which there is at least one hidden layer between the input and output nodes. CNN: the convolutional neural network is a feed forward neural network in which the inputs are grouped spatially into hidden nodes. In the case of this example, each input node is only connected to hidden nodes alongside their neighboring input node. Autoencoder: a type of MLP in which the neural network is trained to produce an output that matches the input to the network. RNN: a deep recurrent neural network is used to allow the neural network to retain memory over time or sequential inputs. This figure was inspired by the Neural Network Zoo by Fjodor Van Veen." id="fig:nn-petting-zoo" class="white" /><figcaption><span>Figure 1:</span> Neural networks come in many different forms. Left: a key for the various types of nodes used in neural networks. Simple FFNN: a feed forward neural network in which inputs are connected via some function to an output node and the model is trained to produce some output for a set of inputs. MLP: the multi-layer perceptron is a feed forward neural network in which there is at least one hidden layer between the input and output nodes. CNN: the convolutional neural network is a feed forward neural network in which the inputs are grouped spatially into hidden nodes. In the case of this example, each input node is only connected to hidden nodes alongside their neighboring input node. Autoencoder: a type of MLP in which the neural network is trained to produce an output that matches the input to the network. RNN: a deep recurrent neural network is used to allow the neural network to retain memory over time or sequential inputs. This figure was inspired by the <a href="http://www.asimovinstitute.org/neural-network-zoo/">Neural Network Zoo</a> by Fjodor Van Veen.</figcaption>
</figure>
<p>This review discusses recent work in the biomedical domain, and most successful applications select neural network architectures that are well suited to the problem at hand. We sketch out a few simple example architectures in Figure <a href="#fig:nn-petting-zoo">1</a>. If data have a natural adjacency structure, a convolutional neural network (CNN) can take advantage of that structure by emphasizing local relationships, especially when convolutional layers are used in early layers of the neural network. Other neural network architectures such as autoencoders require no labels and are now regularly used for unsupervised tasks. In this review, we do not exhaustively discuss the different types of deep neural network architectures; an overview of the principal terms used herein is given in Table <a href="#tbl:glossary">1</a>. Table <a href="#tbl:glossary">1</a> also provides select example applications, though in practice each neural network architecture has been broadly applied across multiple types of biomedical data. A recent book from Goodfellow et al. covers neural network architectures in detail <span class="citation" data-cites="yg8NW0K7">[<a href="#ref-yg8NW0K7" role="doc-biblioref">11</a>]</span>, and LeCun et al. provide a more general introduction <span class="citation" data-cites="BeijBSRE">[<a href="#ref-BeijBSRE" role="doc-biblioref">2</a>]</span>.</p>
<a name="tbl:glossary"></a>
<table>
<caption><span>Table 1:</span> Glossary. </caption>
<colgroup>
<col style="width: 20%" />
<col style="width: 40%" />
<col style="width: 40%" />
</colgroup>
<thead>
<tr class="header">
<th>Term</th>
<th>Definition</th>
<th>Example applications</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Supervised learning</td>
<td>Machine-learning approaches with goal of prediction of labels or outcomes</td>
<td></td>
</tr>
<tr class="even">
<td>Unsupervised learning</td>
<td>Machine-learning approaches with goal of data summarization or pattern identification</td>
<td></td>
</tr>
<tr class="odd">
<td>Neural network (NN)</td>
<td>Machine-learning approach inspired by biological neurons where inputs are fed into one or more layers, producing an output layer</td>
<td></td>
</tr>
<tr class="even">
<td>Deep neural network</td>
<td>NN with multiple hidden layers. Training happens over the network, and consequently such architectures allow for feature construction to occur alongside optimization of the overall training objective.</td>
<td></td>
</tr>
<tr class="odd">
<td>Feed-forward neural network (FFNN)</td>
<td>NN that does not have cycles between nodes in the same layer</td>
<td>Most of the examples below are special cases of FFNNs, except recurrent neural networks.</td>
</tr>
<tr class="even">
<td>Multi-layer perceptron (MLP)</td>
<td>Type of FFNN with at least one hidden layer where each deeper layer is a nonlinear function of each earlier layer</td>
<td>MLPs do not impose structure and are frequently used when there is no natural ordering of the inputs (e.g. as with gene expression measurements).</td>
</tr>
<tr class="odd">
<td>Convolutional neural network (CNN)</td>
<td>A NN with layers in which connectivity preserves local structure. <em>If the data meet the underlying assumptions</em> performance is often good, and such networks can require fewer examples to train effectively because they have fewer parameters and also provide improved efficiency.</td>
<td>CNNs are used for sequence data—such as DNA sequences—or grid data—such as medical and microscopy images.</td>
</tr>
<tr class="even">
<td>Recurrent neural network (RNN)</td>
<td>A neural network with cycles between nodes within a hidden layer.</td>
<td>The RNN architecture is used for sequential data—such as clinical time series and text or genome sequences.</td>
</tr>
<tr class="odd">
<td>Long short-term memory (LSTM) neural network</td>
<td>This special type of RNN has features that enable models to capture longer-term dependencies.</td>
<td>LSTMs are gaining a substantial foothold in the analysis of natural language, and may become more widely applied to biological sequence data.</td>
</tr>
<tr class="even">
<td>Autoencoder (AE)</td>
<td>A NN where the training objective is to minimize the error between the output layer and the input layer. Such neural networks are unsupervised and are often used for dimensionality reduction.</td>
<td>Autoencoders have been used for unsupervised analysis of gene expression data as well as data extracted from the electronic health record.</td>
</tr>
<tr class="odd">
<td>Variational autoencoder (VAE)</td>
<td>This special type of generative AE learns a probabilistic latent variable model.</td>
<td>VAEs have been shown to often produce meaningful reduced representations in the imaging domain, and some early publications have used VAEs to analyze gene expression data.</td>
</tr>
<tr class="even">
<td>Denoising autoencoder (DA)</td>
<td>This special type of AE includes a step where noise is added to the input during the training process. The denoising step acts as smoothing and may allow for effective use on input data that is inherently noisy.</td>
<td>Like AEs, DAs have been used for unsupervised analysis of gene expression data as well as data extracted from the electronic health record.</td>
</tr>
<tr class="odd">
<td>Generative neural network</td>
<td>Neural networks that fall into this class can be used to generate data similar to input data. These models can be sampled to produce hypothetical examples.</td>
<td>A number of the unsupervised learning neural network architectures that are summarized here can be used in a generative fashion.</td>
</tr>
<tr class="even">
<td>Restricted Boltzmann machine (RBM)</td>
<td>A generative NN that forms the building block for many deep learning approaches, having a single input layer and a single hidden layer, with no connections between the nodes within each layer</td>
<td>RBMs have been applied to combine multiple types of omic data (e.g. DNA methylation, mRNA expression, and miRNA expression).</td>
</tr>
<tr class="odd">
<td>Deep belief network (DBN)</td>
<td>Generative NN with several hidden layers, which can be obtained from combining multiple RBMs</td>
<td>DBNs can be used to predict new relationships in a drug-target interaction network.</td>
</tr>
<tr class="even">
<td>Generative adversarial network (GAN)</td>
<td>A generative NN approach where two neural networks are trained. One neural network, the generator, is provided with a set of randomly generated inputs and tasked with generating samples. The second, the discriminator, is trained to differentiate real and generated samples. After the two neural networks are trained against each other, the resulting generator can be used to produce new examples.</td>
<td>GANs can synthesize new examples with the same statistical properties of datasets that contain individual-level records and are subject to sharing restrictions. They have also been applied to generate microscopy images.</td>
</tr>
<tr class="odd">
<td>Adversarial training</td>
<td>A process by which artificial training examples are maliciously designed to fool a NN and then input as training examples to make the resulting NN robust (no relation to GANs)</td>
<td>Adversarial training has been used in image analysis.</td>
</tr>
<tr class="even">
<td>Data augmentation</td>
<td>A process by which transformations that do not affect relevant properties of the input data (e.g. arbitrary rotations of histopathology images) are applied to training examples to increase the size of the training set.</td>
<td>Data augmentation is widely used in the analysis of images because rotation transformations for biomedical images often do not change relevant properties of the image.</td>
</tr>
</tbody>
</table>
<p>While deep learning shows increased flexibility over other machine learning approaches, as seen in the remainder of this review, it requires large training sets in order to fit the hidden layers, as well as accurate labels for the supervised learning applications. For these reasons, deep learning has recently become popular in some areas of biology and medicine, while having lower adoption in other areas. At the same time, this highlights the potentially even larger role that it may play in future research, given the increases in data in all biomedical fields. It is also important to see it as a branch of machine learning and acknowledge that it has the same limitations as other approaches in that field. In particular, the results are still dependent on the underlying study design and the usual caveats of correlation versus causation still apply—a more precise answer is only better than a less precise one if it answers the correct question.</p>
<h3 id="will-deep-learning-transform-the-study-of-human-disease">Will deep learning transform the study of human disease?</h3>
<p>With this review, we ask the question: what is needed for deep learning to transform how we categorize, study, and treat individuals to maintain or restore health? We choose a high bar for “transform.” Andrew Grove, the former CEO of Intel, coined the term Strategic Inflection Point to refer to a change in technologies or environment that requires a business to be fundamentally reshaped <span class="citation" data-cites="mAXsmd43">[<a href="#ref-mAXsmd43" role="doc-biblioref">12</a>]</span>. Here, we seek to identify whether deep learning is an innovation that can induce a Strategic Inflection Point in the practice of biology or medicine.</p>
<p>There are already a number of reviews focused on applications of deep learning in biology <span class="citation" data-cites="yXqhuueV 1VZjheOA irSe12Sm G00xvi94 MmRGFVUu">[<a href="#ref-yXqhuueV" role="doc-biblioref">13</a>,<a href="#ref-1VZjheOA" role="doc-biblioref">14</a>,<a href="#ref-irSe12Sm" role="doc-biblioref">15</a>,<a href="#ref-G00xvi94" role="doc-biblioref">16</a>,<a href="#ref-MmRGFVUu" role="doc-biblioref">17</a>]</span>, healthcare <span class="citation" data-cites="11I7bLcP3 1FkYUUryG brPjEjYw">[<a href="#ref-11I7bLcP3" role="doc-biblioref">18</a>,<a href="#ref-1FkYUUryG" role="doc-biblioref">19</a>,<a href="#ref-brPjEjYw" role="doc-biblioref">20</a>]</span>, and drug discovery <span class="citation" data-cites="gJE0ExFr zCt6PUXj 1DTUK3YyI xPkT1z7D">[<a href="#ref-gJE0ExFr" role="doc-biblioref">21</a>,<a href="#ref-1DTUK3YyI" role="doc-biblioref">22</a>,<a href="#ref-xPkT1z7D" role="doc-biblioref">23</a>,<a href="#ref-zCt6PUXj" role="doc-biblioref">4</a>]</span>. Under our guiding question, we sought to highlight cases where deep learning enabled researchers to solve challenges that were previously considered infeasible or makes difficult, tedious analyses routine. We also identified approaches that researchers are using to sidestep challenges posed by biomedical data. We find that domain-specific considerations have greatly influenced how to best harness the power and flexibility of deep learning. Model interpretability is often critical. Understanding the patterns in data may be just as important as fitting the data. In addition, there are important and pressing questions about how to build networks that efficiently represent the underlying structure and logic of the data. Domain experts can play important roles in designing networks to represent data appropriately, encoding the most salient prior knowledge and assessing success or failure. There is also great potential to create deep learning systems that augment biologists and clinicians by prioritizing experiments or streamlining tasks that do not require expert judgment. We have divided the large range of topics into three broad classes: Disease and Patient Categorization, Fundamental Biological Study, and Treatment of Patients. Below, we briefly introduce the types of questions, approaches and data that are typical for each class in the application of deep learning.</p>
<h4 id="disease-and-patient-categorization">Disease and patient categorization</h4>
<p>A key challenge in biomedicine is the accurate classification of diseases and disease subtypes. In oncology, current “gold standard” approaches include histology, which requires interpretation by experts, or assessment of molecular markers such as cell surface receptors or gene expression. One example is the PAM50 approach to classifying breast cancer where the expression of 50 marker genes divides breast cancer patients into four subtypes. Substantial heterogeneity still remains within these four subtypes <span class="citation" data-cites="lnK82Ey6 pEIw87Mp">[<a href="#ref-lnK82Ey6" role="doc-biblioref">24</a>,<a href="#ref-pEIw87Mp" role="doc-biblioref">25</a>]</span>. Given the increasing wealth of molecular data available, a more comprehensive subtyping seems possible. Several studies have used deep learning methods to better categorize breast cancer patients: For instance, denoising autoencoders, an unsupervised approach, can be used to cluster breast cancer patients <span class="citation" data-cites="PBiRSdXv">[<a href="#ref-PBiRSdXv" role="doc-biblioref">26</a>]</span>, and CNNs can help count mitotic divisions, a feature that is highly correlated with disease outcome in histological images <span class="citation" data-cites="koEdZRcY">[<a href="#ref-koEdZRcY" role="doc-biblioref">27</a>]</span>. Despite these recent advances, a number of challenges exist in this area of research, most notably the integration of molecular and imaging data with other disparate types of data such as electronic health records (EHRs).</p>
<h4 id="fundamental-biological-study">Fundamental biological study</h4>
<p>Deep learning can be applied to answer more fundamental biological questions; it is especially suited to leveraging large amounts of data from high-throughput “omics” studies. One classic biological problem where machine learning, and now deep learning, has been extensively applied is molecular target prediction. For example, deep recurrent neural networks (RNNs) have been used to predict gene targets of microRNAs <span class="citation" data-cites="YUms527e">[<a href="#ref-YUms527e" role="doc-biblioref">28</a>]</span>, and CNNs have been applied to predict protein residue-residue contacts and secondary structure <span class="citation" data-cites="BhfjKSY3 ZzaRyGuJ UO8L6nd">[<a href="#ref-BhfjKSY3" role="doc-biblioref">29</a>,<a href="#ref-ZzaRyGuJ" role="doc-biblioref">30</a>,<a href="#ref-UO8L6nd" role="doc-biblioref">31</a>]</span>. Other recent exciting applications of deep learning include recognition of functional genomic elements such as enhancers and promoters <span class="citation" data-cites="s5sy4AOi 17B2QAA1k 12aqvAgz6">[<a href="#ref-s5sy4AOi" role="doc-biblioref">32</a>,<a href="#ref-17B2QAA1k" role="doc-biblioref">33</a>,<a href="#ref-12aqvAgz6" role="doc-biblioref">34</a>]</span> and prediction of the deleterious effects of nucleotide polymorphisms <span class="citation" data-cites="15E5yG1Ho">[<a href="#ref-15E5yG1Ho" role="doc-biblioref">35</a>]</span>.</p>
<h4 id="treatment-of-patients">Treatment of patients</h4>
<p>Although the application of deep learning to patient treatment is just beginning, we expect new methods to recommend patient treatments, predict treatment outcomes, and guide the development of new therapies. One type of effort in this area aims to identify drug targets and interactions or predict drug response. Another uses deep learning on protein structures to predict drug interactions and drug bioactivity <span class="citation" data-cites="Z7fd0BYf">[<a href="#ref-Z7fd0BYf" role="doc-biblioref">36</a>]</span>. Drug repositioning using deep learning on transcriptomic data is another exciting area of research <span class="citation" data-cites="EMDwvRGb">[<a href="#ref-EMDwvRGb" role="doc-biblioref">37</a>]</span>. Restricted Boltzmann machines (RBMs) can be combined into deep belief networks (DBNs) to predict novel drug-target interactions and formulate drug repositioning hypotheses <span class="citation" data-cites="1AU7wzPqa oTF8O79C">[<a href="#ref-1AU7wzPqa" role="doc-biblioref">38</a>,<a href="#ref-oTF8O79C" role="doc-biblioref">39</a>]</span>. Finally, deep learning is also prioritizing chemicals in the early stages of drug discovery for new targets <span class="citation" data-cites="xPkT1z7D">[<a href="#ref-xPkT1z7D" role="doc-biblioref">23</a>]</span>.</p>
<h2 id="deep-learning-and-patient-categorization">Deep learning and patient categorization</h2>
<p>In healthcare, individuals are diagnosed with a disease or condition based on symptoms, the results of certain diagnostic tests, or other factors. Once diagnosed with a disease, an individual might be assigned a stage based on another set of human-defined rules. While these rules are refined over time, the process is evolutionary and ad hoc, potentially impeding the identification of underlying biological mechanisms and their corresponding treatment interventions.</p>
<p>Deep learning methods applied to a large corpus of patient phenotypes may provide a meaningful and more data-driven approach to patient categorization. For example, they may identify new shared mechanisms that would otherwise be obscured due to ad hoc historical definitions of disease. Perhaps deep neural networks, by reevaluating data without the context of our assumptions, can reveal novel classes of treatable conditions.</p>
<p>In spite of such optimism, the ability of deep learning models to indiscriminately extract predictive signals must also be assessed and operationalized with care. Imagine a deep neural network is provided with clinical test results gleaned from electronic health records. Because physicians may order certain tests based on their suspected diagnosis, a deep neural network may learn to “diagnose” patients simply based on the tests that are ordered. For some objective functions, such as predicting an International Classification of Diseases (ICD) code, this may offer good performance even though it does not provide insight into the underlying disease beyond physician activity. This challenge is not unique to deep learning approaches; however, it is important for practitioners to be aware of these challenges and the possibility in this domain of constructing highly predictive classifiers of questionable utility.</p>
<p>Our goal in this section is to assess the extent to which deep learning is already contributing to the discovery of novel categories. Where it is not, we focus on barriers to achieving these goals. We also highlight approaches that researchers are taking to address challenges within the field, particularly with regards to data availability and labeling.</p>
<h3 id="imaging-applications-in-healthcare">Imaging applications in healthcare</h3>
<p>Deep learning methods have transformed the analysis of natural images and video, and similar examples are beginning to emerge with medical images. Deep learning has been used to classify lesions and nodules; localize organs, regions, landmarks and lesions; segment organs, organ substructures and lesions; retrieve images based on content; generate and enhance images; and combine images with clinical reports <span class="citation" data-cites="1FkYUUryG yEstnIOT">[<a href="#ref-1FkYUUryG" role="doc-biblioref">19</a>,<a href="#ref-yEstnIOT" role="doc-biblioref">40</a>]</span>.</p>
<p>Though there are many commonalities with the analysis of natural images, there are also key differences. In all cases that we examined, fewer than one million images were available for training, and datasets are often many orders of magnitude smaller than collections of natural images. Researchers have developed subtask-specific strategies to address this challenge.</p>
<p>Data augmentation provides an effective strategy for working with small training sets. The practice is exemplified by a series of papers that analyze images from mammographies <span class="citation" data-cites="VFw1VXDP JK8NuXy3 9G9Hv1Pp Xxb4t3zO 5kfDbGhA">[<a href="#ref-VFw1VXDP" role="doc-biblioref">41</a>,<a href="#ref-JK8NuXy3" role="doc-biblioref">42</a>,<a href="#ref-9G9Hv1Pp" role="doc-biblioref">43</a>,<a href="#ref-Xxb4t3zO" role="doc-biblioref">44</a>,<a href="#ref-5kfDbGhA" role="doc-biblioref">45</a>]</span>. To expand the number and diversity of images, researchers constructed adversarial <span class="citation" data-cites="Xxb4t3zO">[<a href="#ref-Xxb4t3zO" role="doc-biblioref">44</a>]</span> or augmented <span class="citation" data-cites="5kfDbGhA">[<a href="#ref-5kfDbGhA" role="doc-biblioref">45</a>]</span> examples. Adversarial training examples are constructed by selecting targeted small transformations to input data that cause a model to produce very different outputs. Augmented training applies perturbations to the input data that do not change the underlying meaning, such as rotations for pathology images. An alternative in the domain is to train towards human-created features before subsequent fine-tuning <span class="citation" data-cites="JK8NuXy3">[<a href="#ref-JK8NuXy3" role="doc-biblioref">42</a>]</span>, which can help to sidestep this challenge though it does give up deep learning techniques’ strength as feature constructors.</p>
<p>A second strategy repurposes features extracted from natural images by deep learning models, such as ImageNet <span class="citation" data-cites="cBVeXnZx">[<a href="#ref-cBVeXnZx" role="doc-biblioref">46</a>]</span>, for new purposes. Diagnosing diabetic retinopathy through color fundus images became an area of focus for deep learning researchers after a large labeled image set was made publicly available during a 2015 Kaggle competition <span class="citation" data-cites="ayTsooEM">[<a href="#ref-ayTsooEM" role="doc-biblioref">47</a>]</span>. Most participants trained neural networks from scratch <span class="citation" data-cites="ayTsooEM e3vyHBV2 14Ovc5nPg">[<a href="#ref-ayTsooEM" role="doc-biblioref">47</a>,<a href="#ref-e3vyHBV2" role="doc-biblioref">48</a>,<a href="#ref-14Ovc5nPg" role="doc-biblioref">49</a>]</span>, but Gulshan et al. <span class="citation" data-cites="1mJW6umJ">[<a href="#ref-1mJW6umJ" role="doc-biblioref">50</a>]</span> repurposed a 48-layer Inception-v3 deep architecture pre-trained on natural images and surpassed the state-of-the-art specificity and sensitivity. Such features were also repurposed to detect melanoma, the deadliest form of skin cancer, from dermoscopic <span class="citation" data-cites="sLPsrfbl phRCihNB">[<a href="#ref-sLPsrfbl" role="doc-biblioref">51</a>,<a href="#ref-phRCihNB" role="doc-biblioref">52</a>]</span> and non-dermoscopic images of skin lesions <span class="citation" data-cites="18f8olBNy O39LDkX XnYNYoYB">[<a href="#ref-XnYNYoYB" role="doc-biblioref">5</a>,<a href="#ref-18f8olBNy" role="doc-biblioref">53</a>,<a href="#ref-O39LDkX" role="doc-biblioref">54</a>]</span> as well as age-related macular degeneration <span class="citation" data-cites="iBPOt78R">[<a href="#ref-iBPOt78R" role="doc-biblioref">55</a>]</span>. Pre-training on natural images can enable very deep networks to succeed without overfitting. For the melanoma task, reported performance was competitive with or better than a board of certified dermatologists <span class="citation" data-cites="sLPsrfbl XnYNYoYB">[<a href="#ref-XnYNYoYB" role="doc-biblioref">5</a>,<a href="#ref-sLPsrfbl" role="doc-biblioref">51</a>]</span>. Reusing features from natural images is also an emerging approach for radiographic images, where datasets are often too small to train large deep neural networks without these techniques <span class="citation" data-cites="1Fy5bcnCI 1GAyqYBNZ x6HXFAS4 Qve94Jra">[<a href="#ref-1Fy5bcnCI" role="doc-biblioref">56</a>,<a href="#ref-1GAyqYBNZ" role="doc-biblioref">57</a>,<a href="#ref-x6HXFAS4" role="doc-biblioref">58</a>,<a href="#ref-Qve94Jra" role="doc-biblioref">59</a>]</span>. A deep CNN trained on natural images boosts performance in radiographic images <span class="citation" data-cites="x6HXFAS4">[<a href="#ref-x6HXFAS4" role="doc-biblioref">58</a>]</span>. However, the target task required either re-training the initial model from scratch with special pre-processing or fine-tuning of the whole network on radiographs with heavy data augmentation to avoid overfitting.</p>
<p>The technique of reusing features from a different task falls into the broader area of transfer learning (see Discussion). Though we’ve mentioned numerous successes for the transfer of natural image features to new tasks, we expect that a lower proportion of negative results have been published. The analysis of magnetic resonance images (MRIs) is also faced with the challenge of small training sets. In this domain, Amit et al. <span class="citation" data-cites="SOi9mAC2">[<a href="#ref-SOi9mAC2" role="doc-biblioref">60</a>]</span> investigated the tradeoff between pre-trained models from a different domain and a small CNN trained only with MRI images. In contrast with the other selected literature, they found a smaller network trained with data augmentation on a few hundred images from a few dozen patients can outperform a pre-trained out-of-domain classifier.</p>
<p>Another way of dealing with limited training data is to divide rich data—e.g. 3D images—into numerous reduced projections. Shin et al. <span class="citation" data-cites="1GAyqYBNZ">[<a href="#ref-1GAyqYBNZ" role="doc-biblioref">57</a>]</span> compared various deep network architectures, dataset characteristics, and training procedures for computer tomography-based (CT) abnormality detection. They concluded that networks as deep as 22 layers could be useful for 3D data, despite the limited size of training datasets. However, they noted that choice of architecture, parameter setting, and model fine-tuning needed is very problem- and dataset-specific. Moreover, this type of task often depends on both lesion localization and appearance, which poses challenges for CNN-based approaches. Straightforward attempts to capture useful information from full-size images in all three dimensions simultaneously via standard neural network architectures were computationally unfeasible. Instead, two-dimensional models were used to either process image slices individually (2D) or aggregate information from a number of 2D projections in the native space (2.5D).</p>
<p>Roth et al. compared 2D, 2.5D, and 3D CNNs on a number of tasks for computer-aided detection from CT scans and showed that 2.5D CNNs performed comparably well to 3D analogs, while requiring much less training time, especially on augmented training sets <span class="citation" data-cites="KseoWN2w">[<a href="#ref-KseoWN2w" role="doc-biblioref">61</a>]</span>. Another advantage of 2D and 2.5D networks is the wider availability of pre-trained models. However, reducing the dimensionality is not always helpful. Nie et al. <span class="citation" data-cites="18EpaZ7QB">[<a href="#ref-18EpaZ7QB" role="doc-biblioref">62</a>]</span> showed that multimodal, multi-channel 3D deep architecture was successful at learning high-level brain tumor appearance features jointly from MRI, functional MRI, and diffusion MRI images, outperforming single-modality or 2D models. Overall, the variety of modalities, properties and sizes of training sets, the dimensionality of input, and the importance of end goals in medical image analysis are provoking a development of specialized deep neural network architectures, training and validation protocols, and input representations that are not characteristic of widely-studied natural images.</p>
<p>Predictions from deep neural networks can be evaluated for use in workflows that also incorporate human experts. In a large dataset of mammography images, Kooi et al. <span class="citation" data-cites="18cZbigDD">[<a href="#ref-18cZbigDD" role="doc-biblioref">63</a>]</span> demonstrated that deep neural networks outperform a traditional computer-aided diagnosis system at low sensitivity and perform comparably at high sensitivity. They also compared network performance to certified screening radiologists on a patch level and found no significant difference between the network and the readers. However, using deep methods for clinical practice is challenged by the difficulty of assigning a level of confidence to each prediction. Leibig et al. <span class="citation" data-cites="14Ovc5nPg">[<a href="#ref-14Ovc5nPg" role="doc-biblioref">49</a>]</span> estimated the uncertainty of deep networks for diabetic retinopathy diagnosis by linking dropout networks with approximate Bayesian inference. Techniques that assign confidences to each prediction should aid physician-computer interactions and improve uptake by physicians.</p>
<p>Systems to aid in the analysis of histology slides are also promising use cases for deep learning <span class="citation" data-cites="dQCjqq0Q">[<a href="#ref-dQCjqq0Q" role="doc-biblioref">64</a>]</span>. Ciresan et al. <span class="citation" data-cites="koEdZRcY">[<a href="#ref-koEdZRcY" role="doc-biblioref">27</a>]</span> developed one of the earliest approaches for histology slides, winning the 2012 International Conference on Pattern Recognition’s Contest on Mitosis Detection while achieving human-competitive accuracy. In more recent work, Wang et al. <span class="citation" data-cites="mbEp6jNr">[<a href="#ref-mbEp6jNr" role="doc-biblioref">65</a>]</span> analyzed stained slides of lymph node slices to identify cancers. On this task a pathologist has about a 3% error rate. The pathologist did not produce any false positives, but did have a number of false negatives. The algorithm had about twice the error rate of a pathologist, but the errors were not strongly correlated. Combining pre-trained deep network architectures with multiple augmentation techniques enabled accurate detection of breast cancer from a very small set of histology images with less than 100 images per class <span class="citation" data-cites="4L1QgpXP">[<a href="#ref-4L1QgpXP" role="doc-biblioref">66</a>]</span>. In this area, these algorithms may be ready to be incorporated into existing tools to aid pathologists and reduce the false negative rate. Ensembles of deep learning and human experts may help overcome some of the challenges presented by data limitations.</p>
<p>One source of training examples with rich phenotypical annotations is the EHR. Billing information in the form of ICD codes are simple annotations but phenotypic algorithms can combine laboratory tests, medication prescriptions, and patient notes to generate more reliable phenotypes. Recently, Lee et al. <span class="citation" data-cites="SxsZyrVM">[<a href="#ref-SxsZyrVM" role="doc-biblioref">67</a>]</span> developed an approach to distinguish individuals with age-related macular degeneration from control individuals. They trained a deep neural network on approximately 100,000 images extracted from structured electronic health records, reaching greater than 93% accuracy. The authors used their test set to evaluate when to stop training. In other domains, this has resulted in a minimal change in the estimated accuracy <span class="citation" data-cites="CCS5KSIM">[<a href="#ref-CCS5KSIM" role="doc-biblioref">68</a>]</span>, but we recommend the use of an independent test set whenever feasible.</p>
<p>Rich clinical information is stored in EHRs. However, manually annotating a large set requires experts and is time consuming. For chest X-ray studies, a radiologist usually spends a few minutes per example. Generating the number of examples needed for deep learning is infeasibly expensive. Instead, researchers may benefit from using text mining to generate annotations <span class="citation" data-cites="rw9nA3Y7">[<a href="#ref-rw9nA3Y7" role="doc-biblioref">69</a>]</span>, even if those annotations are of modest accuracy. Wang et al. <span class="citation" data-cites="PGi9g7yV">[<a href="#ref-PGi9g7yV" role="doc-biblioref">70</a>]</span> proposed to build predictive deep neural network models through the use of images with <em>weak labels</em>. Such labels are automatically generated and not verified by humans, so they may be noisy or incomplete. In this case, they applied a series of natural language processing (NLP) techniques to the associated chest X-ray radiological reports. They first extracted all diseases mentioned in the reports using a state-of-the-art NLP tool, then applied a new method, NegBio <span class="citation" data-cites="gYxBO26g">[<a href="#ref-gYxBO26g" role="doc-biblioref">71</a>]</span>, to filter negative and equivocal findings in the reports. Evaluation on four independent datasets demonstrated that NegBio is highly accurate for detecting negative and equivocal findings (~90% in F₁ score, which balances precision and recall <span class="citation" data-cites="JmHFuXEM">[<a href="#ref-JmHFuXEM" role="doc-biblioref">72</a>]</span>). The resulting dataset <span class="citation" data-cites="odFR7ptt">[<a href="#ref-odFR7ptt" role="doc-biblioref">73</a>]</span> consisted of 112,120 frontal-view chest X-ray images from 30,805 patients, and each image was associated with one or more <em>text-mined</em> (weakly-labeled) pathology categories (e.g. pneumonia and cardiomegaly) or “no finding” otherwise. Further, Wang et al. <span class="citation" data-cites="PGi9g7yV">[<a href="#ref-PGi9g7yV" role="doc-biblioref">70</a>]</span> used this dataset with a unified weakly-supervised multi-label image classification framework to detect common thoracic diseases. It showed superior performance over a benchmark using fully-labeled data.</p>
<p>Another example of semi-automated label generation for hand radiograph segmentation employed positive mining, an iterative procedure that combines manual labeling with automatic processing <span class="citation" data-cites="jTh3Ds6m">[<a href="#ref-jTh3Ds6m" role="doc-biblioref">74</a>]</span>. First, the initial training set was created by manually labeling 100 of 12,600 unlabeled radiographs that were used to train a model and predict labels for the rest of the dataset. Then, poor quality predictions were discarded through manual inspection, the initial training set was expanded with the acceptable segmentations, and the process was repeated. This procedure had to be repeated six times to obtain good quality segmentation labeling for all radiographs, except for 100 corner cases that still required manual annotation. These annotations allowed accurate segmentation of all hand images in the test set and boosted the final performance in radiograph classification <span class="citation" data-cites="jTh3Ds6m">[<a href="#ref-jTh3Ds6m" role="doc-biblioref">74</a>]</span>.</p>
<p>With the exception of natural image-like problems (e.g. melanoma detection), biomedical imaging poses a number of challenges for deep learning. Datasets are typically small, annotations can be sparse, and images are often high-dimensional, multimodal, and multi-channel. Techniques like transfer learning, heavy dataset augmentation, and the use of multi-view and multi-stream architectures are more common than in the natural image domain. Furthermore, high model sensitivity and specificity can translate directly into clinical value. Thus, prediction evaluation, uncertainty estimation, and model interpretation methods are also of great importance in this domain (see Discussion). Finally, there is a need for better pathologist-computer interaction techniques that will allow combining the power of deep learning methods with human expertise and lead to better-informed decisions for patient treatment and care.</p>
<h3 id="text-applications-in-healthcare">Text applications in healthcare</h3>
<p>Due to the rapid growth of scholarly publications and EHRs, biomedical text mining has become increasingly important in recent years. The main tasks in biological and clinical text mining include, but are not limited to, named entity recognition, relation/event extraction, and information retrieval (Figure <a href="#fig:biotm">2</a>). Deep learning is appealing in this domain because of its competitive performance versus traditional methods and ability to overcome challenges in feature engineering. Relevant applications can be stratified by the application domain (biomedical literature vs. clinical notes) and the actual task (e.g. concept or relation extraction).</p>
<figure>
<img src="images/biotm.png" alt="Figure 2: Deep learning applications, tasks, and models based on NLP perspectives." id="fig:biotm" class="white" style="width:100.0%" /><figcaption><span>Figure 2:</span> Deep learning applications, tasks, and models based on NLP perspectives.</figcaption>
</figure>
<p>Named entity recognition (NER) is a task of identifying text spans that refer to a biological concept of a specific class, such as disease or chemical, in a controlled vocabulary or ontology. NER is often needed as a first step in many complex text mining systems. The current state-of-the-art methods typically reformulate the task as a sequence labeling problem and use conditional random fields <span class="citation" data-cites="11YUuHulp 19G2RXgfp vtuZ3Wx7">[<a href="#ref-11YUuHulp" role="doc-biblioref">75</a>,<a href="#ref-19G2RXgfp" role="doc-biblioref">76</a>,<a href="#ref-vtuZ3Wx7" role="doc-biblioref">77</a>]</span>. In recent years, word embeddings that contain rich latent semantic information of words have been widely used to improve the NER performance. Liu et al. studied the effect of word embeddings on drug name recognition and compared them with traditional semantic features <span class="citation" data-cites="OuoHShLI">[<a href="#ref-OuoHShLI" role="doc-biblioref">78</a>]</span>. Tang et al. investigated word embeddings in gene, DNA, and cell line mention detection tasks <span class="citation" data-cites="14uLNRP38">[<a href="#ref-14uLNRP38" role="doc-biblioref">79</a>]</span>. Moreover, Wu et al. examined the use of neural word embeddings for clinical abbreviation disambiguation <span class="citation" data-cites="pWpK5WUq">[<a href="#ref-pWpK5WUq" role="doc-biblioref">80</a>]</span>. Liu et al. exploited task-oriented resources to learn word embeddings for clinical abbreviation expansion <span class="citation" data-cites="LeLKNlsR">[<a href="#ref-LeLKNlsR" role="doc-biblioref">81</a>]</span>.</p>
<p>Relation extraction involves detecting and classifying semantic relationships between entities from the literature. At present, kernel methods or feature-based approaches are commonly applied <span class="citation" data-cites="c6gbPCdT Sn8TwSQK X909h5sz">[<a href="#ref-c6gbPCdT" role="doc-biblioref">82</a>,<a href="#ref-Sn8TwSQK" role="doc-biblioref">83</a>,<a href="#ref-X909h5sz" role="doc-biblioref">84</a>]</span>. Deep learning can relieve the feature sparsity and engineering problems. Some studies focused on jointly extracting biomedical entities and relations simultaneously <span class="citation" data-cites="17ydlqmVI 1F5aZYjOB">[<a href="#ref-17ydlqmVI" role="doc-biblioref">85</a>,<a href="#ref-1F5aZYjOB" role="doc-biblioref">86</a>]</span>, while others applied deep learning on relation classification given the relevant entities. For example, both multichannel dependency-based CNNs <span class="citation" data-cites="1H4fyFU1f">[<a href="#ref-1H4fyFU1f" role="doc-biblioref">87</a>]</span> and shortest path-based CNNs <span class="citation" data-cites="19r6xFsZQ ULZPgbOq">[<a href="#ref-19r6xFsZQ" role="doc-biblioref">88</a>,<a href="#ref-ULZPgbOq" role="doc-biblioref">89</a>]</span> are well-suited for sentence-based protein-protein extraction. Jiang et al. proposed a biomedical domain-specific word embedding model to reduce the manual labor of designing semantic representation for the same task <span class="citation" data-cites="MY6FXgFn">[<a href="#ref-MY6FXgFn" role="doc-biblioref">90</a>]</span>. Gu et al. employed a maximum entropy model and a CNN model for chemical-induced disease relation extraction at the inter- and intra-sentence level, respectively <span class="citation" data-cites="14afj7TT1">[<a href="#ref-14afj7TT1" role="doc-biblioref">91</a>]</span>. For drug-drug interactions, Zhao et al. used a CNN that employs word embeddings with the syntactic information of a sentence as well as features of part-of-speech tags and dependency trees <span class="citation" data-cites="8NrcroGt">[<a href="#ref-8NrcroGt" role="doc-biblioref">92</a>]</span>. Asada et al. experimented with an attention CNN <span class="citation" data-cites="zUPTZa6w">[<a href="#ref-zUPTZa6w" role="doc-biblioref">93</a>]</span>, and Yi et al. proposed an RNN model with multiple attention layers <span class="citation" data-cites="M6JCKCLX">[<a href="#ref-M6JCKCLX" role="doc-biblioref">94</a>]</span>. In both cases, it is a single model with attention mechanism, which allows the decoder to focus on different parts of the source sentence. As a result, it does not require dependency parsing or training multiple models. Both attention CNN and RNN have comparable results, but the CNN model has an advantage in that it can be easily computed in parallel, hence making it faster with recent graphics processing units (GPUs).</p>
<p>For biotopes event extraction, Li et al. employed CNNs and distributed representation <span class="citation" data-cites="ztw1ugBP">[<a href="#ref-ztw1ugBP" role="doc-biblioref">95</a>]</span> while Mehryary et al. used long short-term memory (LSTM) networks to extract complicated relations <span class="citation" data-cites="1AkznVzFs">[<a href="#ref-1AkznVzFs" role="doc-biblioref">96</a>]</span>. Li et al. applied word embedding to extract complete events from biomedical text and achieved results comparable to the state-of-the-art systems <span class="citation" data-cites="E0XYyYBe">[<a href="#ref-E0XYyYBe" role="doc-biblioref">97</a>]</span>. There are also approaches that identify event triggers rather than the complete event <span class="citation" data-cites="XbkgrcMy z8hmfrmY">[<a href="#ref-XbkgrcMy" role="doc-biblioref">98</a>,<a href="#ref-z8hmfrmY" role="doc-biblioref">99</a>]</span>. Taken together, deep learning models outperform traditional kernel methods or feature-based approaches by 1–5% in f-score. Among various deep learning approaches, CNNs stand out as the most popular model both in terms of computational complexity and performance, while RNNs have achieved continuous progress.</p>
<p>Information retrieval is a task of finding relevant text that satisfies an information need from within a large document collection. While deep learning has not yet achieved the same level of success in this area as seen in others, the recent surge of interest and work suggest that this may be quickly changing. For example, Mohan et al. described a deep learning approach to modeling the relevance of a document’s text to a query, which they applied to the entire biomedical literature <span class="citation" data-cites="zPnsAqyX">[<a href="#ref-zPnsAqyX" role="doc-biblioref">100</a>]</span>.</p>
<p>To summarize, deep learning has shown promising results in many biomedical text mining tasks and applications. However, to realize its full potential in this domain, either large amounts of labeled data or technical advancements in current methods coping with limited labeled data are required.</p>
<h3 id="electronic-health-records">Electronic health records</h3>
<p>EHR data include substantial amounts of free text, which remains challenging to approach <span class="citation" data-cites="uDaRUyh9">[<a href="#ref-uDaRUyh9" role="doc-biblioref">101</a>]</span>. Often, researchers developing algorithms that perform well on specific tasks must design and implement domain-specific features <span class="citation" data-cites="sG3iVOTS">[<a href="#ref-sG3iVOTS" role="doc-biblioref">102</a>]</span>. These features capture unique aspects of the literature being processed. Deep learning methods are natural feature constructors. In recent work, Chalapathy et al. evaluated the extent to which deep learning methods could be applied on top of generic features for domain-specific concept extraction <span class="citation" data-cites="dO844vZn">[<a href="#ref-dO844vZn" role="doc-biblioref">103</a>]</span>. They found that performance was in line with, but lower than the best domain-specific method <span class="citation" data-cites="dO844vZn">[<a href="#ref-dO844vZn" role="doc-biblioref">103</a>]</span>. This raises the possibility that deep learning may impact the field by reducing the researcher time and cost required to develop specific solutions, but it may not always lead to performance increases.</p>
<p>In recent work, Yoon et al. <span class="citation" data-cites="yUgE09ve">[<a href="#ref-yUgE09ve" role="doc-biblioref">104</a>]</span> analyzed simple features using deep neural networks and found that the patterns recognized by the algorithms could be re-used across tasks. Their aim was to analyze the free text portions of pathology reports to identify the primary site and laterality of tumors. The only features the authors supplied to the algorithms were unigrams (counts for single words) and bigrams (counts for two-word combinations) in a free text document. They subset the full set of words and word combinations to the 400 most common. The machine learning algorithms that they employed (naïve Bayes, logistic regression, and deep neural networks) all performed relatively similarly on the task of identifying the primary site. However, when the authors evaluated the more challenging task, evaluating the laterality of each tumor, the deep neural network outperformed the other methods. Of particular interest, when the authors first trained a neural network to predict the primary site and then repurposed those features as a component of a secondary neural network trained to predict laterality, the performance was higher than a laterality-trained neural network. This demonstrates how deep learning methods can repurpose features across tasks, improving overall predictions as the field tackles new challenges. The Discussion further reviews this type of transfer learning.</p>
<p>Several authors have created reusable feature sets for medical terminologies using natural language processing and neural embedding models, as popularized by word2vec <span class="citation" data-cites="1GhHIDxuW">[<a href="#ref-1GhHIDxuW" role="doc-biblioref">105</a>]</span>. Minarro-Giménez et al. <span class="citation" data-cites="sePDg4mZ">[<a href="#ref-sePDg4mZ" role="doc-biblioref">106</a>]</span> applied the word2vec deep learning toolkit to medical corpora and evaluated the efficiency of word2vec in identifying properties of pharmaceuticals based on mid-sized, unstructured medical text corpora without any additional background knowledge. A goal of learning terminologies for different entities in the same vector space is to find relationships between different domains (e.g. drugs and the diseases they treat). It is difficult for us to provide a strong statement on the broad utility of these methods. Manuscripts in this area tend to compare algorithms applied to the same data but lack a comparison against overall best-practices for one or more tasks addressed by these methods. Techniques have been developed for free text medical notes <span class="citation" data-cites="XQtuRkTU">[<a href="#ref-XQtuRkTU" role="doc-biblioref">107</a>]</span>, ICD and National Drug Codes <span class="citation" data-cites="4QDXEv4C zVoUcFPZ">[<a href="#ref-4QDXEv4C" role="doc-biblioref">108</a>,<a href="#ref-zVoUcFPZ" role="doc-biblioref">109</a>]</span>, and claims data <span class="citation" data-cites="TwvauiTv">[<a href="#ref-TwvauiTv" role="doc-biblioref">110</a>]</span>. Methods for neural embeddings learned from electronic health records have at least some ability to predict disease-disease associations and implicate genes with a statistical association with a disease <span class="citation" data-cites="1G2xP5yOM">[<a href="#ref-1G2xP5yOM" role="doc-biblioref">111</a>]</span>, but the evaluations performed did not differentiate between simple predictions (i.e. the same disease in different sites of the body) and non-intuitive ones. Jagannatha and Yu <span class="citation" data-cites="rqGoVCuH">[<a href="#ref-rqGoVCuH" role="doc-biblioref">112</a>]</span> further employed a bidirectional LSTM structure to extract adverse drug events from electronic health records, and Lin et al. <span class="citation" data-cites="8YmxYueq">[<a href="#ref-8YmxYueq" role="doc-biblioref">113</a>]</span> investigated using CNNs to extract temporal relations. While promising, a lack of rigorous evaluation of the real-world utility of these kinds of features makes current contributions in this area difficult to evaluate. Comparisons need to be performed to examine the true utility against leading approaches (i.e. algorithms and data) as opposed to simply evaluating multiple algorithms on the same potentially limited dataset.</p>
<p>Identifying consistent subgroups of individuals and individual health trajectories from clinical tests is also an active area of research. Approaches inspired by deep learning have been used for both unsupervised feature construction and supervised prediction. Early work by Lasko et al. <span class="citation" data-cites="FLX0o7bL">[<a href="#ref-FLX0o7bL" role="doc-biblioref">114</a>]</span>, combined sparse autoencoders and Gaussian processes to distinguish gout from leukemia from uric acid sequences. Later work showed that unsupervised feature construction of many features via denoising autoencoder neural networks could dramatically reduce the number of labeled examples required for subsequent supervised analyses <span class="citation" data-cites="5x3uMSKi">[<a href="#ref-5x3uMSKi" role="doc-biblioref">115</a>]</span>. In addition, it pointed towards features learned during unsupervised training being useful for visualizing and stratifying subgroups of patients within a single disease. In a concurrent large-scale analysis of EHR data from 700,000 patients, Miotto et al. <span class="citation" data-cites="WrNCJ9sO">[<a href="#ref-WrNCJ9sO" role="doc-biblioref">116</a>]</span> used a deep denoising autoencoder architecture applied to the number and co-occurrence of clinical events to learn a representation of patients (DeepPatient). The model was able to predict disease trajectories within one year with over 90% accuracy, and patient-level predictions were improved by up to 15% when compared to other methods. Choi et al. <span class="citation" data-cites="11tMRPqto">[<a href="#ref-11tMRPqto" role="doc-biblioref">117</a>]</span> attempted to model the longitudinal structure of EHRs with an RNN to predict future diagnosis and medication prescriptions on a cohort of 260,000 patients followed for 8 years (Doctor AI). Pham et al. <span class="citation" data-cites="HRXii6Ni">[<a href="#ref-HRXii6Ni" role="doc-biblioref">118</a>]</span> built upon this concept by using an RNN with a LSTM architecture enabling explicit modelling of patient trajectories through the use of memory cells. The method, DeepCare, performed better than shallow models or plain RNN when tested on two independent cohorts for its ability to predict disease progression, intervention recommendation and future risk prediction. Nguyen et al. <span class="citation" data-cites="1Fiy543WZ">[<a href="#ref-1Fiy543WZ" role="doc-biblioref">119</a>]</span> took a different approach and used word embeddings from EHRs to train a CNN that could detect and pool local clinical motifs to predict unplanned readmission after six months, with performance better than the baseline method (Deepr). Razavian et al. <span class="citation" data-cites="c6MfDdWP">[<a href="#ref-c6MfDdWP" role="doc-biblioref">120</a>]</span> used a set of 18 common lab tests to predict disease onset using both CNN and LSTM architectures and demonstrated an improvement over baseline regression models. However, numerous challenges including data integration (patient demographics, family history, laboratory tests, text-based patient records, image analysis, genomic data) and better handling of streaming temporal data with many features will need to be overcome before we can fully assess the potential of deep learning for this application area.</p>
<p>Still, recent work has also revealed domains in which deep networks have proven superior to traditional methods. Survival analysis models the time leading to an event of interest from a shared starting point, and in the context of EHR data, often associates these events to subject covariates. Exploring this relationship is difficult, however, given that EHR data types are often heterogeneous, covariates are often missing, and conventional approaches require the covariate-event relationship be linear and aligned to a specific starting point <span class="citation" data-cites="qXdO2aMm">[<a href="#ref-qXdO2aMm" role="doc-biblioref">121</a>]</span>. Early approaches, such as the Faraggi-Simon feed-forward network, aimed to relax the linearity assumption, but performance gains were lacking <span class="citation" data-cites="1921Mctzh">[<a href="#ref-1921Mctzh" role="doc-biblioref">122</a>]</span>. Katzman et al. in turn developed a deep implementation of the Faraggi-Simon network that, in addition to outperforming Cox regression, was capable of comparing the risk between a given pair of treatments, thus potentially acting as recommender system <span class="citation" data-cites="1FE0F2pQ">[<a href="#ref-1FE0F2pQ" role="doc-biblioref">123</a>]</span>. To overcome the remaining difficulties, researchers have turned to deep exponential families, a class of latent generative models that are constructed from any type of exponential family distributions <span class="citation" data-cites="pxdeuhMS">[<a href="#ref-pxdeuhMS" role="doc-biblioref">124</a>]</span>. The result was a deep survival analysis model capable of overcoming challenges posed by missing data and heterogeneous data types, while uncovering nonlinear relationships between covariates and failure time. They showed their model more accurately stratified patients as a function of disease risk score compared to the current clinical implementation.</p>
<p>There is a computational cost for these methods, however, when compared to traditional, non-neural network approaches. For the exponential family models, despite their scalability <span class="citation" data-cites="8RAYEOPl">[<a href="#ref-8RAYEOPl" role="doc-biblioref">125</a>]</span>, an important question for the investigator is whether he or she is interested in estimates of posterior uncertainty. Given that these models are effectively Bayesian neural networks, much of their utility simplifies to whether a Bayesian approach is warranted for a given increase in computational cost. Moreover, as with all variational methods, future work must continue to explore just how well the posterior distributions are approximated, especially as model complexity increases <span class="citation" data-cites="15lbUf0as">[<a href="#ref-15lbUf0as" role="doc-biblioref">126</a>]</span>.</p>
<h3 id="challenges-and-opportunities-in-patient-categorization">Challenges and opportunities in patient categorization</h3>
<h4 id="generating-ground-truth-labels-can-be-expensive-or-impossible">Generating ground-truth labels can be expensive or impossible</h4>
<p>A dearth of true labels is perhaps among the biggest obstacles for EHR-based analyses that employ machine learning. Popular deep learning (and other machine learning) methods are often used to tackle classification tasks and thus require ground-truth labels for training. For EHRs this can mean that researchers must hire multiple clinicians to manually read and annotate individual patients’ records through a process called chart review. This allows researchers to assign “true” labels, i.e. those that match our best available knowledge. Depending on the application, sometimes the features constructed by algorithms also need to be manually validated and interpreted by clinicians. This can be time consuming and expensive <span class="citation" data-cites="1Ar4f4vfR">[<a href="#ref-1Ar4f4vfR" role="doc-biblioref">127</a>]</span>. Because of these costs, much of this research, including the work cited in this review, skips the process of expert review. Clinicians’ skepticism for research without expert review may greatly dampen their enthusiasm for the work and consequently reduce its impact. To date, even well-resourced large national consortia have been challenged by the task of acquiring enough expert-validated labeled data. For instance, in the eMERGE consortia and PheKB database <span class="citation" data-cites="ziudr6hx">[<a href="#ref-ziudr6hx" role="doc-biblioref">128</a>]</span>, most samples with expert validation contain only 100 to 300 patients. These datasets are quite small even for simple machine learning algorithms. The challenge is greater for deep learning models with many parameters. While unsupervised and semi-supervised approaches can help with small sample sizes, the field would benefit greatly from large collections of anonymized records in which a substantial number of records have undergone expert review. This challenge is not unique to EHR-based studies. Work on medical images, omics data in applications for which detailed metadata are required, and other applications for which labels are costly to obtain will be hampered as long as abundant curated data are unavailable.</p>
<p>Successful approaches to date in this domain have sidestepped this challenge by making methodological choices that either reduce the need for labeled examples or that use transformations to training data to increase the number of times it can be used before overfitting occurs. For example, the unsupervised and semi-supervised methods that we have discussed reduce the need for labeled examples <span class="citation" data-cites="5x3uMSKi">[<a href="#ref-5x3uMSKi" role="doc-biblioref">115</a>]</span>. The anchor and learn framework <span class="citation" data-cites="A9JeoGV8">[<a href="#ref-A9JeoGV8" role="doc-biblioref">129</a>]</span> uses expert knowledge to identify high-confidence observations from which labels can be inferred. If transformations are available that preserve the meaningful content of the data, the adversarial and augmented training techniques discussed above can reduce overfitting. While these can be easily imagined for certain methods that operate on images, it is more challenging to figure out equivalent transformations for a patient’s clinical test results. Consequently, it may be hard to employ such training examples with other applications. Finally, approaches that transfer features can also help use valuable training data most efficiently. Rajkomar et al. trained a deep neural network using generic images before tuning using only radiology images <span class="citation" data-cites="x6HXFAS4">[<a href="#ref-x6HXFAS4" role="doc-biblioref">58</a>]</span>. Datasets that require many of the same types of features might be used for initial training, before fine tuning takes place with the more sparse biomedical examples. Though the analysis has not yet been attempted, it is possible that analogous strategies may be possible with electronic health records. For example, features learned from the electronic health record for one type of clinical test (e.g. a decrease over time in a lab value) may transfer across phenotypes. Methods to accomplish more with little high-quality labeled data arose in other domains and may also be adapted to this challenge, e.g. data programming <span class="citation" data-cites="5Il3kN32">[<a href="#ref-5Il3kN32" role="doc-biblioref">130</a>]</span>. In data programming, noisy automated labeling functions are integrated.</p>
<p>Numerous commentators have described data as the new oil <span class="citation" data-cites="6fE0Vrba o8mib4CN">[<a href="#ref-6fE0Vrba" role="doc-biblioref">131</a>,<a href="#ref-o8mib4CN" role="doc-biblioref">132</a>]</span>. The idea behind this metaphor is that data are available in large quantities, valuable once refined, and this underlying resource will enable a data-driven revolution in how work is done. Contrasting with this perspective, Ratner, Bach, and Ré described labeled training data, instead of data, as “The <em>New</em> New Oil” <span class="citation" data-cites="hfcf5Hmi">[<a href="#ref-hfcf5Hmi" role="doc-biblioref">133</a>]</span>. In this framing, data are abundant and not a scarce resource. Instead, new approaches to solving problems arise when labeled training data become sufficient to enable them. Based on our review of research on deep learning methods to categorize disease, the latter framing rings true.</p>
<p>We expect improved methods for domains with limited data to play an important role if deep learning is going to transform how we categorize states of human health. We don’t expect that deep learning methods will replace expert review. We expect them to complement expert review by allowing more efficient use of the costly practice of manual annotation.</p>
<h4 id="data-sharing-is-hampered-by-standardization-and-privacy-considerations">Data sharing is hampered by standardization and privacy considerations</h4>
<p>To construct the types of very large datasets that deep learning methods thrive on, we need robust sharing of large collections of data. This is in part a cultural challenge. We touch on this challenge in the Discussion section. Beyond the cultural hurdles around data sharing, there are also technological and legal hurdles related to sharing individual health records or deep models built from such records. This subsection deals primarily with these challenges.</p>
<p>EHRs are designed chiefly for clinical, administrative and financial purposes, such as patient care, insurance, and billing <span class="citation" data-cites="FkSZ1qmz">[<a href="#ref-FkSZ1qmz" role="doc-biblioref">134</a>]</span>. Science is at best a tertiary priority, presenting challenges to EHR-based research in general and to deep learning research in particular. Although there is significant work in the literature around EHR data quality and the impact on research <span class="citation" data-cites="odRiFxnB">[<a href="#ref-odRiFxnB" role="doc-biblioref">135</a>]</span>, we focus on three types of challenges: local bias, wider standards, and legal issues. Note these problems are not restricted to EHRs but can also apply to any large biomedical dataset, e.g. clinical trial data.</p>
<p>Even within the same healthcare system, EHRs can be used differently <span class="citation" data-cites="7s3dpUrT RoOhUFKU">[<a href="#ref-7s3dpUrT" role="doc-biblioref">136</a>,<a href="#ref-RoOhUFKU" role="doc-biblioref">137</a>]</span>. Individual users have unique documentation and ordering patterns, with different departments and different hospitals having different priorities that code patients and introduce missing data in a non-random fashion <span class="citation" data-cites="7BctyA7f">[<a href="#ref-7BctyA7f" role="doc-biblioref">138</a>]</span>. Patient data may be kept across several “silos” within a single health system (e.g. separate nursing documentation, registries, etc.). Even the most basic task of matching patients across systems can be challenging due to data entry issues <span class="citation" data-cites="eAP2kxzn">[<a href="#ref-eAP2kxzn" role="doc-biblioref">139</a>]</span>. The situation is further exacerbated by the ongoing introduction, evolution, and migration of EHR systems, especially where reorganized and acquired healthcare facilities have to merge. Further, even the ostensibly least-biased data type, laboratory measurements, can be biased based by both the healthcare process and patient health state <span class="citation" data-cites="1C97CTU9S">[<a href="#ref-1C97CTU9S" role="doc-biblioref">140</a>]</span>. As a result, EHR data can be less complete and less objective than expected.</p>
<p>In the wider picture, standards for EHRs are numerous and evolving. Proprietary systems, indifferent and scattered use of health information standards, and controlled terminologies makes combining and comparison of data across systems challenging <span class="citation" data-cites="13filvWwr">[<a href="#ref-13filvWwr" role="doc-biblioref">141</a>]</span>. Further diversity arises from variation in languages, healthcare practices, and demographics. Merging EHRs gathered in different systems (and even under different assumptions) is challenging <span class="citation" data-cites="1CWhXZxos">[<a href="#ref-1CWhXZxos" role="doc-biblioref">142</a>]</span>.</p>
<p>Combining or replicating studies across systems thus requires controlling for both the above biases and dealing with mismatching standards. This has the practical effect of reducing cohort size, limiting statistical significance, preventing the detection of weak effects <span class="citation" data-cites="Fx5qVQlk">[<a href="#ref-Fx5qVQlk" role="doc-biblioref">143</a>]</span>, and restricting the number of parameters that can be trained in a model. Further, rule-based algorithms have been popular in EHR-based research, but because these are developed at a single institution and trained with a specific patient population, they do not transfer easily to other healthcare systems <span class="citation" data-cites="11OyzMl87">[<a href="#ref-11OyzMl87" role="doc-biblioref">144</a>]</span>. Genetic studies using EHR data are subject to even more bias, as the differences in population ancestry across health centers (e.g. proportion of patients with African or Asian ancestry) can affect algorithm performance. For example, Wiley et al. <span class="citation" data-cites="qe90c1CL">[<a href="#ref-qe90c1CL" role="doc-biblioref">145</a>]</span> showed that warfarin dosing algorithms often under-perform in African Americans, illustrating that some of these issues are unresolved even at a treatment best practices level. Lack of standardization also makes it challenging for investigators skilled in deep learning to enter the field, as numerous data processing steps must be performed before algorithms are applied.</p>
<p>Finally, even if data were perfectly consistent and compatible across systems, attempts to share and combine EHR data face considerable legal and ethical barriers. Patient privacy can severely restrict the sharing and use of EHR data <span class="citation" data-cites="CVnO5njl">[<a href="#ref-CVnO5njl" role="doc-biblioref">146</a>]</span>. Here again, standards are heterogeneous and evolving, but often EHR data cannot be exported or even accessed directly for research purposes without appropriate consent. In the United States, research use of EHR data is subject both to the Common Rule and the Health Insurance Portability and Accountability Act (HIPAA). Ambiguity in the regulatory language and individual interpretation of these rules can hamper use of EHR data <span class="citation" data-cites="2cYqPKf1">[<a href="#ref-2cYqPKf1" role="doc-biblioref">147</a>]</span>. Once again, this has the effect of making data gathering more laborious and expensive, reducing sample size and study power.</p>
<p>Several technological solutions have been proposed in this direction, allowing access to sensitive data satisfying privacy and legal concerns. Software like DataShield <span class="citation" data-cites="SfxIiPJ1">[<a href="#ref-SfxIiPJ1" role="doc-biblioref">148</a>]</span> and ViPAR <span class="citation" data-cites="1D6b3tMu9">[<a href="#ref-1D6b3tMu9" role="doc-biblioref">149</a>]</span>, although not EHR-specific, allow querying and combining of datasets and calculation of summary statistics across remote sites by “taking the analysis to the data”. The computation is carried out at the remote site. Conversely, the EH4CR project <span class="citation" data-cites="13filvWwr">[<a href="#ref-13filvWwr" role="doc-biblioref">141</a>]</span> allows analysis of private data by use of an inter-mediation layer that interprets remote queries across internal formats and datastores and returns the results in a de-identified standard form, thus giving real-time consistent but secure access. Continuous Analysis <span class="citation" data-cites="Qh7xTLwz">[<a href="#ref-Qh7xTLwz" role="doc-biblioref">150</a>]</span> can allow reproducible computing on private data. Using such techniques, intermediate results can be automatically tracked and shared without sharing the original data. While none of these have been used in deep learning, the potential is there.</p>
<p>Even without sharing data, algorithms trained on confidential patient data may present security risks or accidentally allow for the exposure of individual level patient data. Tramer et al. <span class="citation" data-cites="ULSPV0rh">[<a href="#ref-ULSPV0rh" role="doc-biblioref">151</a>]</span> showed the ability to steal trained models via public application programming interfaces (APIs). Dwork and Roth <span class="citation" data-cites="v8Lp4ibI">[<a href="#ref-v8Lp4ibI" role="doc-biblioref">152</a>]</span> demonstrate the ability to expose individual level information from accurate answers in a machine learning model. Attackers can use similar attacks to find out if a particular data instance was present in the original training set for the machine learning model <span class="citation" data-cites="1HbRTExaU">[<a href="#ref-1HbRTExaU" role="doc-biblioref">153</a>]</span>, in this case, whether a person’s record was present. To protect against these attacks, Simmons et al. <span class="citation" data-cites="6XtEfQMC">[<a href="#ref-6XtEfQMC" role="doc-biblioref">154</a>]</span> developed the ability to perform genome-wide association studies (GWASs) in a differentially private manner, and Abadi et al. <span class="citation" data-cites="LiCxcgZp">[<a href="#ref-LiCxcgZp" role="doc-biblioref">155</a>]</span> show the ability to train deep learning classifiers under the differential privacy framework.</p>
<p>These attacks also present a potential hazard for approaches that aim to generate data. Choi et al. propose generative adversarial neural networks (GANs) as a tool to make sharable EHR data <span class="citation" data-cites="xl1ijigK">[<a href="#ref-xl1ijigK" role="doc-biblioref">156</a>]</span>, and Esteban et al. <span class="citation" data-cites="1988BRJe3">[<a href="#ref-1988BRJe3" role="doc-biblioref">157</a>]</span> showed that recurrent GANs could be used for time series data. However, in both cases the authors did not take steps to protect the model from such attacks. There are approaches to protect models, but they pose their own challenges. Training in a differentially private manner provides a limited guarantee that an algorithm’s output will be equally likely to occur regardless of the participation of any one individual. The limit is determined by parameters which provide a quantification of privacy. Beaulieu-Jones et al. demonstrated the ability to generate data that preserved properties of the SPRINT clinical trial with GANs under the differential privacy framework <span class="citation" data-cites="fbIH12yd">[<a href="#ref-fbIH12yd" role="doc-biblioref">158</a>]</span>. Both Beaulieu-Jones et al. and Esteban et al. train models on synthetic data generated under differential privacy and observe performance from a transfer learning evaluation that is only slightly below models trained on the original, real data. Taken together, these results suggest that differentially private GANs may be an attractive way to generate sharable datasets for downstream reanalysis.</p>
<p>Federated learning <span class="citation" data-cites="U0ySdznJ">[<a href="#ref-U0ySdznJ" role="doc-biblioref">159</a>]</span> and secure aggregations <span class="citation" data-cites="1GprsH3DV b8DJ1u6W">[<a href="#ref-1GprsH3DV" role="doc-biblioref">160</a>,<a href="#ref-b8DJ1u6W" role="doc-biblioref">161</a>]</span> are complementary approaches that reinforce differential privacy. Both aim to maintain privacy by training deep learning models from decentralized data sources such as personal mobile devices without transferring actual training instances. This is becoming of increasing importance with the rapid growth of mobile health applications. However, the training process in these approaches places constraints on the algorithms used and can make fitting a model substantially more challenging. It can be trivial to train a model without differential privacy, but quite difficult to train one within the differential privacy framework <span class="citation" data-cites="fbIH12yd">[<a href="#ref-fbIH12yd" role="doc-biblioref">158</a>]</span>. This problem can be particularly pronounced with small sample sizes.</p>
<p>While none of these problems are insurmountable or restricted to deep learning, they present challenges that cannot be ignored. Technical evolution in EHRs and data standards will doubtless ease—although not solve—the problems of data sharing and merging. More problematic are the privacy issues. Those applying deep learning to the domain should consider the potential of inadvertently disclosing the participants’ identities. Techniques that enable training on data without sharing the raw data may have a part to play. Training within a differential privacy framework may often be warranted.</p>
<h4 id="discrimination-and-right-to-an-explanation-laws">Discrimination and “right to an explanation” laws</h4>
<p>In April 2016, the European Union adopted new rules regarding the use of personal information, the General Data Protection Regulation <span class="citation" data-cites="7yE9K08a">[<a href="#ref-7yE9K08a" role="doc-biblioref">162</a>]</span>. A component of these rules can be summed up by the phrase “right to an explanation”. Those who use machine learning algorithms must be able to explain how a decision was reached. For example, a clinician treating a patient who is aided by a machine learning algorithm may be expected to explain decisions that use the patient’s data. The new rules were designed to target categorization or recommendation systems, which inherently profile individuals. Such systems can do so in ways that are discriminatory and unlawful.</p>
<p>As datasets become larger and more complex, we may begin to identify relationships in data that are important for human health but difficult to understand. The algorithms described in this review and others like them may become highly accurate and useful for various purposes, including within medical practice. However, to discover and avoid discriminatory applications it will be important to consider interpretability alongside accuracy. A number of properties of genomic and healthcare data will make this difficult.</p>
<p>First, research samples are frequently non-representative of the general population of interest; they tend to be disproportionately sick <span class="citation" data-cites="10shRODux">[<a href="#ref-10shRODux" role="doc-biblioref">163</a>]</span>, male <span class="citation" data-cites="sOBzMC57">[<a href="#ref-sOBzMC57" role="doc-biblioref">164</a>]</span>, and European in ancestry <span class="citation" data-cites="dKwyEWWF">[<a href="#ref-dKwyEWWF" role="doc-biblioref">165</a>]</span>. One well-known consequence of these biases in genomics is that penetrance is consistently lower in the general population than would be implied by case-control data, as reviewed in <span class="citation" data-cites="10shRODux">[<a href="#ref-10shRODux" role="doc-biblioref">163</a>]</span>. Moreover, real genetic associations found in one population may not hold in other populations with different patterns of linkage disequilibrium (even when population stratification is explicitly controlled for <span class="citation" data-cites="T3GG8iJN">[<a href="#ref-T3GG8iJN" role="doc-biblioref">166</a>]</span>). As a result, many genomic findings are of limited value for people of non-European ancestry <span class="citation" data-cites="dKwyEWWF">[<a href="#ref-dKwyEWWF" role="doc-biblioref">165</a>]</span> and may even lead to worse treatment outcomes for them. Methods have been developed for mitigating some of these problems in genomic studies <span class="citation" data-cites="10shRODux T3GG8iJN">[<a href="#ref-10shRODux" role="doc-biblioref">163</a>,<a href="#ref-T3GG8iJN" role="doc-biblioref">166</a>]</span>, but it is not clear how easily they can be adapted for deep models that are designed specifically to extract subtle effects from high-dimensional data. For example, differences in the equipment that tended to be used for cases versus controls have led to spurious genetic findings (e.g. Sebastiani et al.’s retraction <span class="citation" data-cites="DmQPI43R">[<a href="#ref-DmQPI43R" role="doc-biblioref">167</a>]</span>). In some contexts, it may not be possible to correct for all of these differences to the degree that a deep network is unable to use them. Moreover, the complexity of deep networks makes it difficult to determine when their predictions are likely to be based on such nominally-irrelevant features of the data (called “leakage” in other fields <span class="citation" data-cites="889LsjDi">[<a href="#ref-889LsjDi" role="doc-biblioref">168</a>]</span>). When we are not careful with our data and models, we may inadvertently say more about the way the data was collected (which may involve a history of unequal access and discrimination) than about anything of scientific or predictive value. This fact can undermine the privacy of patient data <span class="citation" data-cites="889LsjDi">[<a href="#ref-889LsjDi" role="doc-biblioref">168</a>]</span> or lead to severe discriminatory consequences <span class="citation" data-cites="6co0adq">[<a href="#ref-6co0adq" role="doc-biblioref">169</a>]</span>.</p>
<p>There is a small but growing literature on the prevention and mitigation of data leakage <span class="citation" data-cites="889LsjDi">[<a href="#ref-889LsjDi" role="doc-biblioref">168</a>]</span>, as well as a closely-related literature on discriminatory model behavior <span class="citation" data-cites="1ENxzq6pT">[<a href="#ref-1ENxzq6pT" role="doc-biblioref">170</a>]</span>, but it remains difficult to predict when these problems will arise, how to diagnose them, and how to resolve them in practice. There is even disagreement about which kinds of algorithmic outcomes should be considered discriminatory <span class="citation" data-cites="11aqfNfQx">[<a href="#ref-11aqfNfQx" role="doc-biblioref">171</a>]</span>. Despite the difficulties and uncertainties, machine learning practitioners (and particularly those who use deep neural networks, which are challenging to interpret) must remain cognizant of these dangers and make every effort to prevent harm from discriminatory predictions. To reach their potential in this domain, deep learning methods will need to be interpretable (see Discussion). Researchers need to consider the extent to which biases may be learned by the model and whether or not a model is sufficiently interpretable to identify bias. We discuss the challenge of model interpretability more thoroughly in Discussion.</p>
<h4 id="applications-of-deep-learning-to-longitudinal-analysis">Applications of deep learning to longitudinal analysis</h4>
<p>Longitudinal analysis follows a population across time, for example, prospectively from birth or from the onset of particular conditions. In large patient populations, longitudinal analyses such as the Framingham Heart Study <span class="citation" data-cites="N96QKgly">[<a href="#ref-N96QKgly" role="doc-biblioref">172</a>]</span> and the Avon Longitudinal Study of Parents and Children <span class="citation" data-cites="1FjSxrV1k">[<a href="#ref-1FjSxrV1k" role="doc-biblioref">173</a>]</span> have yielded important discoveries about the development of disease and the factors contributing to health status. Yet, a common practice in EHR-based research is to take a snapshot at a point in time and convert patient data to a traditional vector for machine learning and statistical analysis. This results in loss of information as timing and order of events can provide insight into a patient’s disease and treatment <span class="citation" data-cites="6RHepB1T">[<a href="#ref-6RHepB1T" role="doc-biblioref">174</a>]</span>. Efforts to model sequences of events have shown promise <span class="citation" data-cites="ogs3PPp7">[<a href="#ref-ogs3PPp7" role="doc-biblioref">175</a>]</span> but require exceedingly large patient sizes due to discrete combinatorial bucketing. Lasko et al. <span class="citation" data-cites="FLX0o7bL">[<a href="#ref-FLX0o7bL" role="doc-biblioref">114</a>]</span> used autoencoders on longitudinal sequences of serum uric acid measurements to identify population subtypes. More recently, deep learning has shown promise working with both sequences (CNNs) <span class="citation" data-cites="Ohd1Q9Xw">[<a href="#ref-Ohd1Q9Xw" role="doc-biblioref">176</a>]</span> and the incorporation of past and current state (RNNs, LSTMs) <span class="citation" data-cites="HRXii6Ni">[<a href="#ref-HRXii6Ni" role="doc-biblioref">118</a>]</span>. This may be a particular area of opportunity for deep neural networks. The ability to recognize relevant sequences of events from a large number of trajectories requires powerful and flexible feature construction methods—an area in which deep neural networks excel.</p>
<h2 id="deep-learning-to-study-the-fundamental-biological-processes-underlying-human-disease">Deep learning to study the fundamental biological processes underlying human disease</h2>
<p>The study of cellular structure and core biological processes—transcription, translation, signaling, metabolism, etc.—in humans and model organisms will greatly impact our understanding of human disease over the long horizon <span class="citation" data-cites="ru0hjGeQ">[<a href="#ref-ru0hjGeQ" role="doc-biblioref">177</a>]</span>. Predicting how cellular systems respond to environmental perturbations and are altered by genetic variation remain daunting tasks. Deep learning offers new approaches for modeling biological processes and integrating multiple types of omic data <span class="citation" data-cites="8SMDF816">[<a href="#ref-8SMDF816" role="doc-biblioref">178</a>]</span>, which could eventually help predict how these processes are disrupted in disease. Recent work has already advanced our ability to identify and interpret genetic variants, study microbial communities, and predict protein structures, which also relates to the problems discussed in the drug development section. In addition, unsupervised deep learning has enormous potential for discovering novel cellular states from gene expression, fluorescence microscopy, and other types of data that may ultimately prove to be clinically relevant.</p>
<p>Progress has been rapid in genomics and imaging, fields where important tasks are readily adapted to well-established deep learning paradigms. One-dimensional convolutional and recurrent neural networks are well-suited for tasks related to DNA- and RNA-binding proteins, epigenomics, and RNA splicing. Two dimensional CNNs are ideal for segmentation, feature extraction, and classification in fluorescence microscopy images <span class="citation" data-cites="MmRGFVUu">[<a href="#ref-MmRGFVUu" role="doc-biblioref">17</a>]</span>. Other areas, such as cellular signaling, are biologically important but studied less-frequently to date, with some exceptions <span class="citation" data-cites="rmjDc5rm">[<a href="#ref-rmjDc5rm" role="doc-biblioref">179</a>]</span>. This may be a consequence of data limitations or greater challenges in adapting neural network architectures to the available data. Here, we highlight several areas of investigation and assess how deep learning might move these fields forward.</p>
<h3 id="gene-expression">Gene expression</h3>
<p>Gene expression technologies characterize the abundance of many thousands of RNA transcripts within a given organism, tissue, or cell. This characterization can represent the underlying state of the given system and can be used to study heterogeneity across samples as well as how the system reacts to perturbation. While gene expression measurements were traditionally made by quantitative polymerase chain reaction (qPCR), low-throughput fluorescence-based methods, and microarray technologies, the field has shifted in recent years to primarily performing RNA sequencing (RNA-seq) to catalog whole transcriptomes. As RNA-seq continues to fall in price and rise in throughput, sample sizes will increase and training deep models to study gene expression will become even more useful.</p>
<p>Already several deep learning approaches have been applied to gene expression data with varying aims. For instance, many researchers have applied unsupervised deep learning models to extract meaningful representations of gene modules or sample clusters. Denoising autoencoders have been used to cluster yeast expression microarrays into known modules representing cell cycle processes <span class="citation" data-cites="AnenJOuU">[<a href="#ref-AnenJOuU" role="doc-biblioref">180</a>]</span> and to stratify yeast strains based on chemical and mutational perturbations <span class="citation" data-cites="yVBx9Qx4">[<a href="#ref-yVBx9Qx4" role="doc-biblioref">181</a>]</span>. Shallow (one hidden layer) denoising autoencoders have also been fruitful in extracting biological insight from thousands of <em>Pseudomonas aeruginosa</em> experiments <span class="citation" data-cites="1CFhfCyWN zuLdSQx3">[<a href="#ref-1CFhfCyWN" role="doc-biblioref">182</a>,<a href="#ref-zuLdSQx3" role="doc-biblioref">183</a>]</span> and in aggregating features relevant to specific breast cancer subtypes <span class="citation" data-cites="PBiRSdXv">[<a href="#ref-PBiRSdXv" role="doc-biblioref">26</a>]</span>. These unsupervised approaches applied to gene expression data are powerful methods for identifying gene signatures that may otherwise be overlooked. An additional benefit of unsupervised approaches is that ground truth labels, which are often difficult to acquire or are incorrect, are nonessential. However, the genes that have been aggregated into features must be interpreted carefully. Attributing each node to a single specific biological function risks over-interpreting models. Batch effects could cause models to discover non-biological features, and downstream analyses should take this into consideration.</p>
<p>Deep learning approaches are also being applied to gene expression prediction tasks. For example, a deep neural network with three hidden layers outperformed linear regression in inferring the expression of over 20,000 target genes based on a representative, well-connected set of about 1,000 landmark genes <span class="citation" data-cites="12QQw9p7v">[<a href="#ref-12QQw9p7v" role="doc-biblioref">184</a>]</span>. However, while the deep learning model outperformed existing algorithms in nearly every scenario, the model still displayed poor performance. The paper was also limited by computational bottlenecks that required data to be split randomly into two distinct models and trained separately. It is unclear how much performance would have increased if not for computational restrictions.</p>
<p>Epigenomic data, combined with deep learning, may have sufficient explanatory power to infer gene expression. For instance, the DeepChrome CNN <span class="citation" data-cites="G10wkFHt">[<a href="#ref-G10wkFHt" role="doc-biblioref">185</a>]</span> improved prediction accuracy of high or low gene expression from histone modifications over existing methods. AttentiveChrome <span class="citation" data-cites="16MNknNBL">[<a href="#ref-16MNknNBL" role="doc-biblioref">186</a>]</span> added a deep attention model to further enhance DeepChrome. Deep learning can also integrate different data types. For example, Liang et al. combined RBMs to integrate gene expression, DNA methylation, and miRNA data to define ovarian cancer subtypes <span class="citation" data-cites="1EtavGKI4">[<a href="#ref-1EtavGKI4" role="doc-biblioref">187</a>]</span>. While these approaches are promising, many convert gene expression measurements to categorical or binary variables, thus ablating many complex gene expression signatures present in intermediate and relative numbers.</p>
<p>Deep learning applied to gene expression data is still in its infancy, but the future is bright. Many previously untestable hypotheses can now be interrogated as deep learning enables analysis of increasing amounts of data generated by new technologies. For example, the effects of cellular heterogeneity on basic biology and disease etiology can now be explored by single-cell RNA-seq and high-throughput fluorescence-based imaging, techniques we discuss below that will benefit immensely from deep learning approaches.</p>
<h3 id="dna-methylation">DNA Methylation</h3>
<h4 id="inference-imputation-and-prediction">Inference, Imputation, and Prediction</h4>
<p>Deep learning approaches are beginning to help address some of the current limitations of feature-by-feature analysis approaches to DNA methylation data, and may help uncover additional important features necessary to understand the biological underpinnings behind different pathological states. One of the more popular applications is imputing the degree of methylation at CpG sites that are within a few thousand base pairs of measured sites or present in similar samples. DeepSignal employs a convolutional neural network to construct features from raw electrical Nanopore signals from sites near a methylated base, and concatenates uses a bi-directional recurrent neural network on DNA sequences of the aligned signals to detect methylation <span class="citation" data-cites="17sl0Ti0w">[<a href="#ref-17sl0Ti0w" role="doc-biblioref">188</a>]</span>. DeepCpG applies a similar method using scBS-Seq, DNA sequence and Bidirectional GRUs <span class="citation" data-cites="19EJTHByG">[<a href="#ref-19EJTHByG" role="doc-biblioref">189</a>]</span> and methods like DAPL, MRCNN and DeepMethyl incorporate sequence and topological structure <span class="citation" data-cites="WWSBQM3">[<a href="#ref-WWSBQM3" role="doc-biblioref">190</a>]</span> <span class="citation" data-cites="NzYX9e9i">[<a href="#ref-NzYX9e9i" role="doc-biblioref">191</a>]</span> <span class="citation" data-cites="gXQfsrAl">[<a href="#ref-gXQfsrAl" role="doc-biblioref">192</a>]</span> <span class="citation" data-cites="1AfEMOdlu">[<a href="#ref-1AfEMOdlu" role="doc-biblioref">193</a>]</span> <span class="citation" data-cites="fUGhxx3W">[<a href="#ref-fUGhxx3W" role="doc-biblioref">194</a>]</span>. In addition to this, Gene expression has been used to infer and impute methylation states <span class="citation" data-cites="L0qjpS0U">[<a href="#ref-L0qjpS0U" role="doc-biblioref">195</a>]</span> <span class="citation" data-cites="1szm3mxk">[<a href="#ref-1szm3mxk" role="doc-biblioref">196</a>]</span>, methylation of genes predicted from promoter methylation <span class="citation" data-cites="GbgdMBjb">[<a href="#ref-GbgdMBjb" role="doc-biblioref">197</a>]</span>, and convolutional models have been able to predict methylation status from images <span class="citation" data-cites="DSTA8ohX">[<a href="#ref-DSTA8ohX" role="doc-biblioref">198</a>]</span> <span class="citation" data-cites="54x3LbFb">[<a href="#ref-54x3LbFb" role="doc-biblioref">199</a>]</span>. While these examples of methylation imputation and inference methods have value it is imperative to recognize limitations of imputing cytosine modifications. Imputing DNA methylation has complexities above and beyond genotype imputation: correlation of DNA methylation marks can depend on cell types and other factors that can vary by sample. As the number of tissue types and cell types with whole-genome bisulfite sequencing (and oxidative bisulfite sequencing) grows, the accuracy of DNA methylation imputation is expected to increase. While these methods reduce the computational overhead at comparable performance to other popular methylation imputation methods such as K-Nearest Neighbors, Random Forest, Singular Value Decomposition and Multiple Imputation by Chained Equations, the software implementations will need to become more user-friendly to gain widespread adoption.</p>
<p>Once DNA methylation is measured, deep learning approaches can also be used to perform classification and regression tasks. For instance, Deep Neural Networks (DNN) have been employed on DNA methylation data to predict triglyceride concentrations pre- and post-treatment <span class="citation" data-cites="1GRpO6k1o">[<a href="#ref-1GRpO6k1o" role="doc-biblioref">200</a>]</span> <span class="citation" data-cites="bnD7Q1cQ">[<a href="#ref-bnD7Q1cQ" role="doc-biblioref">201</a>]</span> and differentiate cancer subtypes <span class="citation" data-cites="GLXmFOus">[<a href="#ref-GLXmFOus" role="doc-biblioref">202</a>]</span> <span class="citation" data-cites="EXGRa0DD">[<a href="#ref-EXGRa0DD" role="doc-biblioref">203</a>]</span> while outperforming other methods such as Support Vector Machine (SVM). Modular approaches to methylation prediction, such as MethylNet, have been able to predict age, cellular proportions and cancer subtypes, outperforming SVM and Elastic Net models while remaining concordant with expected biology <span class="citation" data-cites="epufolpH">[<a href="#ref-epufolpH" role="doc-biblioref">204</a>]</span>. These approaches aim to make embedding, hyperparameter selection, regression, classification and model interpretation tasks more tractable for epigenetics researchers and machine learning scientists.</p>
<h4 id="latent-space-construction">Latent Space Construction</h4>
<p>Unsupervised discovery of biologically-significant features is another major area of interest for researchers using DNA methylation data. A consistent theme of these methods is that they construct a low-dimensional space that semantically encodes biologically important features from methylation profiles. As with other applications, these low-dimensional representations are thought to capture a set of important, unmeasured sources of biological variability in the data, and that projection into these spaces results in biologically-similar examples being close together. For this reason, they are often termed latent spaces. One method used several stacked binary restricted Boltzmann machines (forming a deep neural network) to learn a low-dimensional subspace representation of the methylation profiles of 5000 CpG sites with highest variance across 136 women breast tissue samples, 113 breast cancer samples and 23 non-cancerous samples, and samples in the latent space were clustered (via self-organizing maps) to show that the latent space could differentiate breast cancer samples from non-neoplastic samples. Furthermore, the latent space was visualized using t-SNE (t-distributed stochastic neighbor embedding) <span class="citation" data-cites="1CvlEI6Kb">[<a href="#ref-1CvlEI6Kb" role="doc-biblioref">205</a>]</span>. Titus et. al. <span class="citation" data-cites="71snAYRy">[<a href="#ref-71snAYRy" role="doc-biblioref">206</a>]</span> adapted a VAE strategy developed by Way et. al. <span class="citation" data-cites="7V2oCvtf">[<a href="#ref-7V2oCvtf" role="doc-biblioref">207</a>]</span> to methylation data. The VAE was modified to perform dimensionality reduction on 300,000 PAM50-assigned CpG features to 100 latent features in 862 samples. The authors performed t-SNE visualization, clustering, and classified tumor subtypes from a Breast Cancer dataset from TCGA. In an subsequent extension of this work <span class="citation" data-cites="7pBVQEZ4">[<a href="#ref-7pBVQEZ4" role="doc-biblioref">208</a>]</span>, the authors constructed a 100-dimensional latent space of 100k CpG sites across around 1200 samples, and selected latent space dimensions that were the most highly associated with the differentiation between estrogen-response (ER) positive and negative tumor samples in breast cancer patients to determine the extent to which the latent space could predict responses to endocrine therapy. Certain latent space dimensions differentiated tumors based on their ER status and provided biologically-plausible hypotheses, which suggests that VAE-derived models may have a place in summarizing DNA methylation profiles into composite features that can aid in predicting treatment response. Another study explored the latent features of lung cancer methylation profiles that were extracted using variational autoencoders. After constructing a latent space representations of TCGA lung cancer samples, the authors used a logistic regression classifier on the latent dimentions to accturately classify cancer subtypes <span class="citation" data-cites="s3I1CsTr">[<a href="#ref-s3I1CsTr" role="doc-biblioref">209</a>]</span>. These studies, along with the growing body of work using VAEs and other latent representations of genomic and epigenomic data demonstrate a suite of tools to explore the unmeasured aspects of biology. Techniques that produce these representations provide the opportunity to discover important biological features that were previously missed. The power of unsupervised deep learning models for this task comes from their ability to learn high-dimensional non-linear relationships among data.</p>
<p>Important applications in the future include predicting methylation and pathological states based on methylation profiles uncovered from datasets with more noise, such as solid tissue samples over blood samples. Unsupervised deep learning approaches such as variational autoencoders, which leverage measured points to produce a generative, low-dimensional representation, may provide a more complete understanding of the biological processes underlying cell types, transitions in cell dynamics, and subject phenotypes. In addition, latent representations may assist with biological hypothesis generation and have the ability to stratify patients by predicted risk. While neural-network embeddings can outperform traditional embeddings, it is important to be aware that many of these methods can be highly sensitive to hyperparameter tuning and an evaluation of the impact of hyperparameter tuning should be included <span class="citation" data-cites="5CsWRjfp">[<a href="#ref-5CsWRjfp" role="doc-biblioref">210</a>]</span>.</p>
<h3 id="splicing">Splicing</h3>
<p>Pre-mRNA transcripts can be spliced into different isoforms by retaining or skipping subsets of exons or including parts of introns, creating enormous spatiotemporal flexibility to generate multiple distinct proteins from a single gene. This remarkable complexity can lend itself to defects that underlie many diseases. For instance, splicing mutations in the lamin A (<em>LMNA</em>) gene can lead to specific variants of dilated cardiomyopathy and limb girdle muscular dystrophy <span class="citation" data-cites="QFK6GapR">[<a href="#ref-QFK6GapR" role="doc-biblioref">211</a>]</span>. A recent study found that quantitative trait loci that affect splicing in lymphoblastoid cell lines are enriched within risk loci for schizophrenia, multiple sclerosis, and other immune diseases, implicating mis-splicing as a more widespread feature of human pathologies than previously thought <span class="citation" data-cites="b6p6wxpC">[<a href="#ref-b6p6wxpC" role="doc-biblioref">212</a>]</span>. Therapeutic strategies that aim to modulate splicing are also currently being considered for disorders such as Duchenne muscular dystrophy and spinal muscular atrophy <span class="citation" data-cites="QFK6GapR">[<a href="#ref-QFK6GapR" role="doc-biblioref">211</a>]</span>.</p>
<p>Sequencing studies routinely return thousands of unannotated variants, but which cause functional changes in splicing and how are those changes manifested? Prediction of a “splicing code” has been a goal of the field for the past decade. Initial machine learning approaches used a naïve Bayes model and a 2-layer Bayesian neural network with thousands of hand-derived sequence-based features to predict the probability of exon skipping <span class="citation" data-cites="11ETDdRKr 8VPGUHcf">[<a href="#ref-11ETDdRKr" role="doc-biblioref">213</a>,<a href="#ref-8VPGUHcf" role="doc-biblioref">214</a>]</span>. With the advent of deep learning, more complex models provided better predictive accuracy <span class="citation" data-cites="17sgPdcMT N0HBi8MH">[<a href="#ref-17sgPdcMT" role="doc-biblioref">215</a>,<a href="#ref-N0HBi8MH" role="doc-biblioref">216</a>]</span>. Importantly, these new approaches can take in multiple kinds of epigenomic measurements as well as tissue identity and RNA binding partners of splicing factors. Deep learning is critical in furthering these kinds of integrative studies where different data types and inputs interact in unpredictable (often nonlinear) ways to create higher-order features. Moreover, as in gene expression network analysis, interrogating the hidden nodes within neural networks could potentially illuminate important aspects of splicing behavior. For instance, tissue-specific splicing mechanisms could be inferred by training networks on splicing data from different tissues, then searching for common versus distinctive hidden nodes, a technique employed by Qin et al. for tissue-specific transcription factor (TF) binding predictions <span class="citation" data-cites="Qbtqlmhf">[<a href="#ref-Qbtqlmhf" role="doc-biblioref">217</a>]</span>.</p>
<p>A parallel effort has been to use more data with simpler models. An exhaustive study using readouts of splicing for millions of synthetic intronic sequences uncovered motifs that influence the strength of alternative splice sites <span class="citation" data-cites="mlqKTlZY">[<a href="#ref-mlqKTlZY" role="doc-biblioref">218</a>]</span>. The authors built a simple linear model using hexamer motif frequencies that successfully generalized to exon skipping. In a limited analysis using single nucleotide polymorphisms (SNPs) from three genes, it predicted exon skipping with three times the accuracy of an existing deep learning-based framework <span class="citation" data-cites="17sgPdcMT">[<a href="#ref-17sgPdcMT" role="doc-biblioref">215</a>]</span>. This case is instructive in that clever sources of data, not just more descriptive models, are still critical.</p>
<p>We already understand how mis-splicing of a single gene can cause diseases such as limb girdle muscular dystrophy. The challenge now is to uncover how genome-wide alternative splicing underlies complex, non-Mendelian diseases such as autism, schizophrenia, Type 1 diabetes, and multiple sclerosis <span class="citation" data-cites="CNz9HwZ3">[<a href="#ref-CNz9HwZ3" role="doc-biblioref">219</a>]</span>. As a proof of concept, Xiong et al. <span class="citation" data-cites="17sgPdcMT">[<a href="#ref-17sgPdcMT" role="doc-biblioref">215</a>]</span> sequenced five autism spectrum disorder and 12 control samples, each with an average of 42,000 rare variants, and identified mis-splicing in 19 genes with neural functions. Such methods may one day enable scientists and clinicians to rapidly profile thousands of unannotated variants for functional effects on splicing and nominate candidates for further investigation. Moreover, these nonlinear algorithms can deconvolve the effects of multiple variants on a single splice event without the need to perform combinatorial <em>in vitro</em> experiments. The ultimate goal is to predict an individual’s tissue-specific, exon-specific splicing patterns from their genome sequence and other measurements to enable a new branch of precision diagnostics that also stratifies patients and suggests targeted therapies to correct splicing defects. However, to achieve this we expect that methods to interpret the “black box” of deep neural networks and integrate diverse data sources will be required.</p>
<h3 id="transcription-factors">Transcription factors</h3>
<p>Transcription factors are proteins that bind regulatory DNA in a sequence-specific manner to modulate the activation and repression of gene transcription. High-throughput <em>in vitro</em> experimental assays that quantitatively measure the binding specificity of a TF to a large library of short oligonucleotides <span class="citation" data-cites="15c9V9Hm1">[<a href="#ref-15c9V9Hm1" role="doc-biblioref">220</a>]</span> provide rich datasets to model the naked DNA sequence affinity of individual TFs in isolation. However, <em>in vivo</em> TF binding is affected by a variety of other factors beyond sequence affinity, such as competition and cooperation with other TFs, TF concentration, and chromatin state (chemical modifications to DNA and other packaging proteins that DNA is wrapped around) <span class="citation" data-cites="15c9V9Hm1">[<a href="#ref-15c9V9Hm1" role="doc-biblioref">220</a>]</span>. TFs can thus exhibit highly variable binding landscapes across the same genomic DNA sequence across diverse cell types and states. Several experimental approaches such as chromatin immunoprecipitation followed by sequencing (ChIP-seq) have been developed to profile <em>in vivo</em> binding maps of TFs <span class="citation" data-cites="15c9V9Hm1">[<a href="#ref-15c9V9Hm1" role="doc-biblioref">220</a>]</span>. Large reference compendia of ChIP-seq data are now freely available for a large collection of TFs in a small number of reference cell states in humans and a few other model organisms <span class="citation" data-cites="15J98V2qM">[<a href="#ref-15J98V2qM" role="doc-biblioref">221</a>]</span>. Due to fundamental material and cost constraints, it is infeasible to perform these experiments for all TFs in every possible cellular state and species. Hence, predictive computational models of TF binding are essential to understand gene regulation in diverse cellular contexts.</p>
<p>Several machine learning approaches have been developed to learn generative and discriminative models of TF binding from <em>in vitro</em> and <em>in vivo</em> TF binding datasets that associate collections of synthetic DNA sequences or genomic DNA sequences to binary labels (bound/unbound) or continuous measures of binding. The most common class of TF binding models in the literature are those that only model the DNA sequence affinity of TFs from <em>in vitro</em> and <em>in vivo</em> binding data. The earliest models were based on deriving simple, compact, interpretable sequence motif representations such as position weight matrices (PWMs) and other biophysically inspired models <span class="citation" data-cites="ywDQIvZJ dwj6qSn3 fv4f2GgS">[<a href="#ref-ywDQIvZJ" role="doc-biblioref">222</a>,<a href="#ref-dwj6qSn3" role="doc-biblioref">223</a>,<a href="#ref-fv4f2GgS" role="doc-biblioref">224</a>]</span>. These models were outperformed by general k-mer based models including support vector machines (SVMs) with string kernels <span class="citation" data-cites="EPo6SPty JxuQvvyk">[<a href="#ref-EPo6SPty" role="doc-biblioref">225</a>,<a href="#ref-JxuQvvyk" role="doc-biblioref">226</a>]</span>.</p>
<p>In 2015, Alipanahi et al. developed DeepBind, the first CNN to classify bound DNA sequences based on <em>in vitro</em> and <em>in vivo</em> assays against random DNA sequences matched for dinucleotide sequence composition <span class="citation" data-cites="jJHZHWrl">[<a href="#ref-jJHZHWrl" role="doc-biblioref">227</a>]</span>. The convolutional layers learn pattern detectors reminiscent of PWMs from a one-hot encoding of the raw input DNA sequences. DeepBind outperformed several state-of-the-art methods from the DREAM5 <em>in vitro</em> TF-DNA motif recognition challenge <span class="citation" data-cites="fv4f2GgS">[<a href="#ref-fv4f2GgS" role="doc-biblioref">224</a>]</span>. Although DeepBind was also applied to RNA-binding proteins, in general RNA binding is a separate problem <span class="citation" data-cites="qnKdqG0P">[<a href="#ref-qnKdqG0P" role="doc-biblioref">228</a>]</span> and accurate models will need to account for RNA secondary structure. Following DeepBind, several optimized convolutional and recurrent neural network architectures as well as novel hybrid approaches that combine kernel methods with neural networks have been proposed that further improve performance <span class="citation" data-cites="182UhQqzp Dwi2eAvT 6Nw5JrLI 1GOS0CRta">[<a href="#ref-182UhQqzp" role="doc-biblioref">229</a>,<a href="#ref-Dwi2eAvT" role="doc-biblioref">230</a>,<a href="#ref-6Nw5JrLI" role="doc-biblioref">231</a>,<a href="#ref-1GOS0CRta" role="doc-biblioref">232</a>]</span>. Specialized layers and regularizers have also been proposed to reduce parameters and learn more robust models by taking advantage of specific properties of DNA sequences such as their reverse complement equivalence <span class="citation" data-cites="iEmvzeT8 Zo0D80FN">[<a href="#ref-iEmvzeT8" role="doc-biblioref">233</a>,<a href="#ref-Zo0D80FN" role="doc-biblioref">234</a>]</span>.</p>
<p>While most of these methods learn independent models for different TFs, <em>in vivo</em> multiple TFs compete or cooperate to occupy DNA binding sites, resulting in complex combinatorial co-binding landscapes. To take advantage of this shared structure in <em>in vivo</em> TF binding data, multi-task neural network architectures have been developed that explicitly share parameters across models for multiple TFs <span class="citation" data-cites="2UI1BZuD iBdkksok 1GOS0CRta">[<a href="#ref-1GOS0CRta" role="doc-biblioref">232</a>,<a href="#ref-2UI1BZuD" role="doc-biblioref">235</a>,<a href="#ref-iBdkksok" role="doc-biblioref">236</a>]</span>. Some of these multi-task models train and evaluate classification performance relative to an unbound background set of regulatory DNA sequences sampled from the genome rather than using synthetic background sequences with matched dinucleotide composition.</p>
<p>The above-mentioned TF binding prediction models that use only DNA sequences as inputs have a fundamental limitation. Because the DNA sequence of a genome is the same across different cell types and states, a sequence-only model of TF binding cannot predict different <em>in vivo</em> TF binding landscapes in new cell types not used during training. One approach for generalizing TF binding predictions to new cell types is to learn models that integrate DNA sequence inputs with other cell-type-specific data modalities that modulate <em>in vivo</em> TF binding such as surrogate measures of TF concentration (e.g. TF gene expression) and chromatin state. Arvey et al. showed that combining the predictions of SVMs trained on DNA sequence inputs and cell-type specific DNase-seq data, which measures genome-wide chromatin accessibility, improved <em>in vivo</em> TF binding prediction within and across cell types <span class="citation" data-cites="gsSLr9vf">[<a href="#ref-gsSLr9vf" role="doc-biblioref">237</a>]</span>. Several “footprinting” based methods have also been developed that learn to discriminate bound from unbound instances of known canonical motifs of a target TF based on high-resolution footprint patterns of chromatin accessibility that are specific to the target TF <span class="citation" data-cites="rOo5pTPS">[<a href="#ref-rOo5pTPS" role="doc-biblioref">238</a>]</span>. However, the genome-wide predictive performance of these methods in new cell types and states has not been evaluated.</p>
<p>Recently, a community challenge known as the “ENCODE-DREAM <em>in vivo</em> TF Binding Site Prediction Challenge” was introduced to systematically evaluate genome-wide performance of methods that can predict TF binding across cell states by integrating DNA sequence and <em>in vitro</em> DNA shape with cell-type-specific chromatin accessibility and gene expression <span class="citation" data-cites="wW6QbBXz">[<a href="#ref-wW6QbBXz" role="doc-biblioref">239</a>]</span>. A deep learning model called FactorNet was amongst the top three performing methods in the challenge <span class="citation" data-cites="BguHMHkG">[<a href="#ref-BguHMHkG" role="doc-biblioref">240</a>]</span>. FactorNet uses a multi-modal hybrid convolutional and recurrent architecture that integrates DNA sequence with chromatin accessibility profiles, gene expression, and evolutionary conservation of sequence. It is worth noting that FactorNet was slightly outperformed by an approach that does not use neural networks <span class="citation" data-cites="pZqk9gDB">[<a href="#ref-pZqk9gDB" role="doc-biblioref">241</a>]</span>. This top ranking approach uses an extensive set of curated features in a weighted variant of a discriminative maximum conditional likelihood model in combination with a novel iterative training strategy and model stacking. There appears to be significant room for improvement because none of the current approaches for cross cell type prediction explicitly account for the fact that TFs can co-bind with distinct co-factors in different cell states. In such cases, sequence features that are predictive of TF binding in one cell state may be detrimental to predicting binding in another.</p>
<p>Singh et al. developed transfer string kernels for SVMs for cross-context TF binding <span class="citation" data-cites="Nc8y8wkO">[<a href="#ref-Nc8y8wkO" role="doc-biblioref">242</a>]</span>. Domain adaptation methods that allow training neural networks which are transferable between differing training and test set distributions of sequence features could be a promising avenue going forward <span class="citation" data-cites="iWxvn0xF nu0eLZr0">[<a href="#ref-iWxvn0xF" role="doc-biblioref">243</a>,<a href="#ref-nu0eLZr0" role="doc-biblioref">244</a>]</span>. These approaches may also be useful for transferring TF binding models across species.</p>
<p>Another class of imputation-based cross cell type <em>in vivo</em> TF binding prediction methods leverage the strong correlation between combinatorial binding landscapes of multiple TFs. Given a partially complete panel of binding profiles of multiple TFs in multiple cell types, a deep learning method called TFImpute learns to predict the missing binding profile of a target TF in some target cell type in the panel based on the binding profiles of other TFs in the target cell type and the binding profile of the target TF in other cell types in the panel <span class="citation" data-cites="Qbtqlmhf">[<a href="#ref-Qbtqlmhf" role="doc-biblioref">217</a>]</span>. However, TFImpute cannot generalize predictions beyond the training panel of cell types and requires TF binding profiles of related TFs.</p>
<p>It is worth noting that TF binding prediction methods in the literature based on neural networks and other machine learning approaches choose to sample the set of bound and unbound sequences in a variety of different ways. These choices and the choice of performance evaluation measures significantly confound systematic comparison of model performance (see Discussion).</p>
<p>Several methods have also been developed to interpret neural network models of TF binding. Alipanahi et al. visualize convolutional filters to obtain insights into the sequence preferences of TFs <span class="citation" data-cites="jJHZHWrl">[<a href="#ref-jJHZHWrl" role="doc-biblioref">227</a>]</span>. They also introduced <em>in silico</em> mutation maps for identifying important predictive nucleotides in input DNA sequences by exhaustively forward propagating perturbations to individual nucleotides to record the corresponding change in output prediction. Shrikumar et al. <span class="citation" data-cites="zhmq9ktJ">[<a href="#ref-zhmq9ktJ" role="doc-biblioref">245</a>]</span> proposed efficient backpropagation based approaches to simultaneously score the contribution of all nucleotides in an input DNA sequence to an output prediction. Lanchantin et al. <span class="citation" data-cites="Dwi2eAvT">[<a href="#ref-Dwi2eAvT" role="doc-biblioref">230</a>]</span> developed tools to visualize TF motifs learned from TF binding site classification tasks. These and other general interpretation techniques (see Discussion) will be critical to improve our understanding of the biologically meaningful patterns learned by deep learning models of TF binding.</p>
<h3 id="promoters-and-enhancers">Promoters and enhancers</h3>
<h4 id="from-tf-binding-to-promoters-and-enhancers">From TF binding to promoters and enhancers</h4>
<p>Multiple TFs act in concert to coordinate changes in gene regulation at the genomic regions known as promoters and enhancers. Each gene has an upstream promoter, essential for initiating that gene’s transcription. The gene may also interact with multiple enhancers, which can amplify transcription in particular cellular contexts. These contexts include different cell types in development or environmental stresses.</p>
<p>Promoters and enhancers provide a nexus where clusters of TFs and binding sites mediate downstream gene regulation, starting with transcription. The gold standard to identify an active promoter or enhancer requires demonstrating its ability to affect transcription or other downstream gene products. Even extensive biochemical TF binding data has thus far proven insufficient on its own to accurately and comprehensively locate promoters and enhancers. We lack sufficient understanding of these elements to derive a mechanistic “promoter code” or “enhancer code”. But extensive labeled data on promoters and enhancers lends itself to probabilistic classification. The complex interplay of TFs and chromatin leading to the emergent properties of promoter and enhancer activity seems particularly apt for representation by deep neural networks.</p>
<h4 id="promoters">Promoters</h4>
<p>Despite decades of work, computational identification of promoters remains a stubborn problem <span class="citation" data-cites="19jjiGHWc">[<a href="#ref-19jjiGHWc" role="doc-biblioref">246</a>]</span>. Researchers have used neural networks for promoter recognition as early as 1996 <span class="citation" data-cites="3Ew5V1iC">[<a href="#ref-3Ew5V1iC" role="doc-biblioref">247</a>]</span>. Recently, a CNN recognized promoter sequences with sensitivity and specificity exceeding 90% <span class="citation" data-cites="as2HfoSh">[<a href="#ref-as2HfoSh" role="doc-biblioref">248</a>]</span>. Most activity in computational prediction of regulatory regions, however, has moved to enhancer identification. Because one can identify promoters with straightforward biochemical assays <span class="citation" data-cites="9XBPQ8b G0J9P3Ln">[<a href="#ref-9XBPQ8b" role="doc-biblioref">249</a>,<a href="#ref-G0J9P3Ln" role="doc-biblioref">250</a>]</span>, the direct rewards of promoter prediction alone have decreased. But the reliable ground truth provided by these assays makes promoter identification an appealing test bed for deep learning approaches that can also identify enhancers.</p>
<h4 id="enhancers">Enhancers</h4>
<p>Recognizing enhancers presents additional challenges. Enhancers may be up to 1,000,000 bp away from the affected promoter, and even within introns of other genes <span class="citation" data-cites="8yA3foA6">[<a href="#ref-8yA3foA6" role="doc-biblioref">251</a>]</span>. Enhancers do not necessarily operate on the nearest gene and may affect multiple genes. Their activity is frequently tissue- or context-specific. No biochemical assay can reliably identify all enhancers. Distinguishing them from other regulatory elements remains difficult, and some believe the distinction somewhat artificial <span class="citation" data-cites="J0PJHcHK">[<a href="#ref-J0PJHcHK" role="doc-biblioref">252</a>]</span>. While these factors make the enhancer identification problem more difficult, they also make a solution more valuable.</p>
<p>Several neural network approaches yielded promising results in enhancer prediction. Both Basset <span class="citation" data-cites="2CbHXoFn">[<a href="#ref-2CbHXoFn" role="doc-biblioref">253</a>]</span> and DeepEnhancer <span class="citation" data-cites="jV2YerUS">[<a href="#ref-jV2YerUS" role="doc-biblioref">254</a>]</span> used CNNs to predict enhancers. DECRES used a feed-forward neural network <span class="citation" data-cites="1HbQQcY2q">[<a href="#ref-1HbQQcY2q" role="doc-biblioref">255</a>]</span> to distinguish between different kinds of regulatory elements, such as active enhancers, and promoters. DECRES had difficulty distinguishing between inactive enhancers and promoters. They also investigated the power of sequence features to drive classification, finding that beyond CpG islands, few were useful.</p>
<p>Comparing the performance of enhancer prediction methods illustrates the problems in using metrics created with different benchmarking procedures. Both the Basset and DeepEnhancer studies include comparisons to a baseline SVM approach, gkm-SVM <span class="citation" data-cites="JxuQvvyk">[<a href="#ref-JxuQvvyk" role="doc-biblioref">226</a>]</span>. The Basset study reports gkm-SVM attains a mean area under the precision-recall curve (AUPR) of 0.322 over 164 cell types <span class="citation" data-cites="2CbHXoFn">[<a href="#ref-2CbHXoFn" role="doc-biblioref">253</a>]</span>. The DeepEnhancer study reports for gkm-SVM a dramatically different AUPR of 0.899 on nine cell types <span class="citation" data-cites="jV2YerUS">[<a href="#ref-jV2YerUS" role="doc-biblioref">254</a>]</span>. This large difference means it’s impossible to directly compare the performance of Basset and DeepEnhancer based solely on their reported metrics. DECRES used a different set of metrics altogether. To drive further progress in enhancer identification, we must develop a common and comparable benchmarking procedure (see Discussion).</p>
<h4 id="promoter-enhancer-interactions">Promoter-enhancer interactions</h4>
<p>In addition to the location of enhancers, identifying enhancer-promoter interactions in three-dimensional space will provide critical knowledge for understanding transcriptional regulation. SPEID used a CNN to predict these interactions with only sequence and the location of putative enhancers and promoters along a one-dimensional chromosome <span class="citation" data-cites="14TqLB9iZ">[<a href="#ref-14TqLB9iZ" role="doc-biblioref">256</a>]</span>. It compared well to other methods using a full complement of biochemical data from ChIP-seq and other epigenomic methods. Of course, the putative enhancers and promoters used were themselves derived from epigenomic methods. But one could easily replace them with the output of one of the enhancer or promoter prediction methods above.</p>
<h3 id="micro-rna-binding">Micro-RNA binding</h3>
<p>Prediction of microRNAs (miRNAs) and miRNA targets is of great interest, as they are critical components of gene regulatory networks and are often conserved across great evolutionary distance <span class="citation" data-cites="yVKIhIAf 8lpCCppx">[<a href="#ref-yVKIhIAf" role="doc-biblioref">257</a>,<a href="#ref-8lpCCppx" role="doc-biblioref">258</a>]</span>. While many machine learning algorithms have been applied to these tasks, they currently require extensive feature selection and optimization. For instance, one of the most widely adopted tools for miRNA target prediction, TargetScan, trained multiple linear regression models on 14 hand-curated features including structural accessibility of the target site on the mRNA, the degree of site conservation, and predicted thermodynamic stability of the miRNA-mRNA complex <span class="citation" data-cites="12vPQi3gp">[<a href="#ref-12vPQi3gp" role="doc-biblioref">259</a>]</span>. Some of these features, including structural accessibility, are imperfect or empirically derived. In addition, current algorithms suffer from low specificity <span class="citation" data-cites="1GwC1ll6h">[<a href="#ref-1GwC1ll6h" role="doc-biblioref">260</a>]</span>.</p>
<p>As in other applications, deep learning promises to achieve equal or better performance in predictive tasks by automatically engineering complex features to minimize an objective function. Two recently published tools use different recurrent neural network-based architectures to perform miRNA and target prediction with solely sequence data as input <span class="citation" data-cites="1TeyWffV 1GwC1ll6h">[<a href="#ref-1GwC1ll6h" role="doc-biblioref">260</a>,<a href="#ref-1TeyWffV" role="doc-biblioref">261</a>]</span>. Though the results are preliminary and still based on a validation set rather than a completely independent test set, they were able to predict microRNA target sites with higher specificity and sensitivity than TargetScan. Excitingly, these tools seem to show that RNNs can accurately align sequences and predict bulges, mismatches, and wobble base pairing without requiring the user to input secondary structure predictions or thermodynamic calculations. Further incremental advances in deep learning for miRNA and target prediction will likely be sufficient to meet the current needs of systems biologists and other researchers who use prediction tools mainly to nominate candidates that are then tested experimentally.</p>
<h3 id="protein-secondary-and-tertiary-structure">Protein secondary and tertiary structure</h3>
<p>Proteins play fundamental roles in almost all biological processes, and understanding their structure is critical for basic biology and drug development. UniProt currently has about 94 million protein sequences, yet fewer than 100,000 proteins across all species have experimentally-solved structures in Protein Data Bank (PDB). As a result, computational structure prediction is essential for a majority of proteins. However, this is very challenging, especially when similar solved structures, called templates, are not available in PDB. Over the past several decades, many computational methods have been developed to predict aspects of protein structure such as secondary structure, torsion angles, solvent accessibility, inter-residue contact maps, disorder regions, and side-chain packing. In recent years, multiple deep learning architectures have been applied, including deep belief networks, LSTMs, CNNs, and deep convolutional neural fields (DeepCNFs) <span class="citation" data-cites="pNoAbBEu UO8L6nd">[<a href="#ref-pNoAbBEu" role="doc-biblioref">262</a>,<a href="#ref-UO8L6nd" role="doc-biblioref">31</a>]</span>.</p>
<p>Here we focus on deep learning methods for two representative sub-problems: secondary structure prediction and contact map prediction. Secondary structure refers to local conformation of a sequence segment, while a contact map contains information on all residue-residue contacts. Secondary structure prediction is a basic problem and an almost essential module of any protein structure prediction package. Contact prediction is much more challenging than secondary structure prediction, but it has a much larger impact on tertiary structure prediction. In recent years, the accuracy of contact prediction has greatly improved <span class="citation" data-cites="BhfjKSY3 7atXz0r kboAopkh 10dNuD89l">[<a href="#ref-7atXz0r" role="doc-biblioref">263</a>,<a href="#ref-kboAopkh" role="doc-biblioref">264</a>,<a href="#ref-10dNuD89l" role="doc-biblioref">265</a>,<a href="#ref-BhfjKSY3" role="doc-biblioref">29</a>]</span>.</p>
<p>One can represent protein secondary structure with three different states (alpha helix, beta strand, and loop regions) or eight finer-grained states. Accuracy of a three-state prediction is called Q3, and accuracy of an 8-state prediction is called Q8. Several groups <span class="citation" data-cites="1AlhRKQbe ZzaRyGuJ UpFrhdJf">[<a href="#ref-1AlhRKQbe" role="doc-biblioref">266</a>,<a href="#ref-UpFrhdJf" role="doc-biblioref">267</a>,<a href="#ref-ZzaRyGuJ" role="doc-biblioref">30</a>]</span> applied deep learning to protein secondary structure prediction but were unable to achieve significant improvement over the <em>de facto</em> standard method PSIPRED <span class="citation" data-cites="Aic7UyXM">[<a href="#ref-Aic7UyXM" role="doc-biblioref">268</a>]</span>, which uses two shallow feedforward neural networks. In 2014, Zhou and Troyanskaya demonstrated that they could improve Q8 accuracy by using a deep supervised and convolutional generative stochastic network <span class="citation" data-cites="8t43CQ9m">[<a href="#ref-8t43CQ9m" role="doc-biblioref">269</a>]</span>. In 2016 Wang et al. developed a DeepCNF model that improved Q3 and Q8 accuracy as well as prediction of solvent accessibility and disorder regions <span class="citation" data-cites="UO8L6nd pNoAbBEu">[<a href="#ref-pNoAbBEu" role="doc-biblioref">262</a>,<a href="#ref-UO8L6nd" role="doc-biblioref">31</a>]</span>. DeepCNF achieved a higher Q3 accuracy than the standard maintained by PSIPRED for more than 10 years. This improvement may be mainly due to the ability of convolutional neural fields to capture long-range sequential information, which is important for beta strand prediction. Nevertheless, the improvements in secondary structure prediction from DeepCNF are unlikely to result in a commensurate improvement in tertiary structure prediction since secondary structure mainly reflects coarse-grained local conformation of a protein structure.</p>
<p>Protein contact prediction and contact-assisted folding (i.e. folding proteins using predicted contacts as restraints) represents a promising new direction for <em>ab initio</em> folding of proteins without good templates in PDB. Co-evolution analysis is effective for proteins with a very large number (&gt;1000) of sequence homologs <span class="citation" data-cites="10dNuD89l">[<a href="#ref-10dNuD89l" role="doc-biblioref">265</a>]</span>, but fares poorly for proteins without many sequence homologs. By combining co-evolution information with a few other protein features, shallow neural network methods such as MetaPSICOV <span class="citation" data-cites="7atXz0r">[<a href="#ref-7atXz0r" role="doc-biblioref">263</a>]</span> and CoinDCA-NN <span class="citation" data-cites="kqjqFesT">[<a href="#ref-kqjqFesT" role="doc-biblioref">270</a>]</span> have shown some advantage over pure co-evolution analysis for proteins with few sequence homologs, but their accuracy is still far from satisfactory. In recent years, deeper architectures have been explored for contact prediction, such as CMAPpro <span class="citation" data-cites="xdoT1yUx">[<a href="#ref-xdoT1yUx" role="doc-biblioref">271</a>]</span>, DNCON <span class="citation" data-cites="18bNbDNlc">[<a href="#ref-18bNbDNlc" role="doc-biblioref">272</a>]</span> and PConsC <span class="citation" data-cites="F13xtRbV">[<a href="#ref-F13xtRbV" role="doc-biblioref">273</a>]</span>. However, blindly tested in the well-known CASP competitions, these methods did not show any advantage over MetaPSICOV <span class="citation" data-cites="7atXz0r">[<a href="#ref-7atXz0r" role="doc-biblioref">263</a>]</span>.</p>
<p>Recently, Wang et al. proposed the deep learning method RaptorX-Contact <span class="citation" data-cites="BhfjKSY3">[<a href="#ref-BhfjKSY3" role="doc-biblioref">29</a>]</span>, which significantly improves contact prediction over MetaPSICOV and pure co-evolution methods, especially for proteins without many sequence homologs. It employs a network architecture formed by one 1D residual neural network and one 2D residual neural network. Blindly tested in the latest CASP competition (i.e. CASP12 <span class="citation" data-cites="zScWGveU">[<a href="#ref-zScWGveU" role="doc-biblioref">274</a>]</span>), RaptorX-Contact ranked first in F₁ score on free-modeling targets as well as the whole set of targets. In CAMEO (which can be interpreted as a fully-automated CASP) <span class="citation" data-cites="u9uApoaB">[<a href="#ref-u9uApoaB" role="doc-biblioref">275</a>]</span>, its predicted contacts were also able to fold proteins with a novel fold and only 65–330 sequence homologs. This technique also worked well on membrane proteins even when trained on non-membrane proteins <span class="citation" data-cites="39RPiE10">[<a href="#ref-39RPiE10" role="doc-biblioref">276</a>]</span>. RaptorX-Contact performed better mainly due to introduction of residual neural networks and exploitation of contact occurrence patterns by simultaneously predicting all the contacts in a single protein.</p>
<p>Taken together, <em>ab initio</em> folding is becoming much easier with the advent of direct evolutionary coupling analysis and deep learning techniques. We expect further improvements in contact prediction for proteins with fewer than 1000 homologs by studying new deep network architectures. The deep learning methods summarized above also apply to interfacial contact prediction for protein complexes but may be less effective since on average protein complexes have fewer sequence homologs. Beyond secondary structure and contact maps, we anticipate increased attention to predicting 3D protein structure directly from amino acid sequence and single residue evolutionary information <span class="citation" data-cites="16s8GKCdZ">[<a href="#ref-16s8GKCdZ" role="doc-biblioref">277</a>]</span>.</p>
<h3 id="structure-determination-and-cryo-electron-microscopy">Structure determination and cryo-electron microscopy</h3>
<p>Complementing computational prediction approaches, cryo-electron microscopy (cryo-EM) allows near-atomic resolution determination of protein models by comparing individual electron micrographs <span class="citation" data-cites="bSYQrdJA">[<a href="#ref-bSYQrdJA" role="doc-biblioref">278</a>]</span>. Detailed structures require tens of thousands of protein images <span class="citation" data-cites="Ud5iHvkw">[<a href="#ref-Ud5iHvkw" role="doc-biblioref">279</a>]</span>. Technological development has increased the throughput of image capture. New hardware, such as direct electron detectors, has made large-scale image production practical, while new software has focused on rapid, automated image processing.</p>
<p>Some components of cryo-EM image processing remain difficult to automate. For instance, in particle picking, micrographs are scanned to identify individual molecular images that will be used in structure refinement. In typical applications, hundreds of thousands of particles are necessary to determine a structure to near atomic resolution, making manual selection impractical <span class="citation" data-cites="Ud5iHvkw">[<a href="#ref-Ud5iHvkw" role="doc-biblioref">279</a>]</span>. Typical selection approaches are semi-supervised; a user will select several particles manually, and these selections will be used to train a classifier <span class="citation" data-cites="6W1hknHI HpxWaOv3">[<a href="#ref-6W1hknHI" role="doc-biblioref">280</a>,<a href="#ref-HpxWaOv3" role="doc-biblioref">281</a>]</span>. Now CNNs are being used to select particles in tools like DeepPicker <span class="citation" data-cites="ku0xqQxt">[<a href="#ref-ku0xqQxt" role="doc-biblioref">282</a>]</span> and DeepEM <span class="citation" data-cites="18QrMkpC5">[<a href="#ref-18QrMkpC5" role="doc-biblioref">283</a>]</span>. In addition to addressing shortcomings from manual selection, such as selection bias and poor discrimination of low-contrast images, these approaches also provide a means of full automation. DeepPicker can be trained by reference particles from other experiments with structurally unrelated macromolecules, allowing for fully automated application to new samples.</p>
<p>Downstream of particle picking, deep learning is being applied to other aspects of cryo-EM image processing. Statistical manifold learning has been implemented in the software package ROME to classify selected particles and elucidate the different conformations of the subject molecule necessary for accurate 3D structures <span class="citation" data-cites="RRR3YEJV">[<a href="#ref-RRR3YEJV" role="doc-biblioref">284</a>]</span>. These recent tools highlight the general applicability of deep learning approaches for image processing to increase the throughput of high-resolution cryo-EM.</p>
<h3 id="protein-protein-interactions">Protein-protein interactions</h3>
<p>Protein-protein interactions (PPIs) are highly specific and non-accidental physical contacts between proteins, which occur for purposes other than generic protein production or degradation <span class="citation" data-cites="imRWjslx">[<a href="#ref-imRWjslx" role="doc-biblioref">285</a>]</span>. Abundant interaction data have been generated in-part thanks to advances in high-throughput screening methods, such as yeast two-hybrid and affinity-purification with mass spectrometry. However, because many PPIs are transient or dependent on biological context, high-throughput methods can fail to capture a number of interactions. The imperfections and costs associated with many experimental PPI screening methods have motivated an interest in high-throughput computational prediction.</p>
<p>Many machine learning approaches to PPI have focused on text mining the literature <span class="citation" data-cites="xEQI6dXW TNHJioqT">[<a href="#ref-xEQI6dXW" role="doc-biblioref">286</a>,<a href="#ref-TNHJioqT" role="doc-biblioref">287</a>]</span>, but these approaches can fail to capture context-specific interactions, motivating <em>de novo</em> PPI prediction. Early <em>de novo</em> prediction approaches used a variety of statistical and machine learning tools on structural and sequential data, sometimes with reference to the existing body of protein structure knowledge. In the context of PPIs—as in other domains—deep learning shows promise both for exceeding current predictive performance and for circumventing limitations from which other approaches suffer.</p>
<p>One of the key difficulties in applying deep learning techniques to binding prediction is the task of representing peptide and protein sequences in a meaningful way. DeepPPI <span class="citation" data-cites="rVgq22nD">[<a href="#ref-rVgq22nD" role="doc-biblioref">288</a>]</span> made PPI predictions from a set of sequence and composition protein descriptors using a two-stage deep neural network that trained two subnetworks for each protein and combined them into a single network. Sun et al. <span class="citation" data-cites="3nZqSy6z">[<a href="#ref-3nZqSy6z" role="doc-biblioref">289</a>]</span> applied autocovariances, a coding scheme that returns uniform-size vectors describing the covariance between physicochemical properties of the protein sequence at various positions. Wang et al. <span class="citation" data-cites="T2lbgFlY">[<a href="#ref-T2lbgFlY" role="doc-biblioref">290</a>]</span> used deep learning as an intermediate step in PPI prediction. They examined 70 amino acid protein sequences from each of which they extracted 1260 features. A stacked sparse autoencoder with two hidden layers was then used to reduce feature dimensions and noisiness before a novel type of classification vector machine made PPI predictions.</p>
<p>Beyond predicting whether or not two proteins interact, Du et al. <span class="citation" data-cites="2Ftmbvt4">[<a href="#ref-2Ftmbvt4" role="doc-biblioref">291</a>]</span> employed a deep learning approach to predict the residue contacts between two interacting proteins. Using features that describe how similar a protein’s residue is relative to similar proteins at the same position, the authors extracted uniform-length features for each residue in the protein sequence. A stacked autoencoder took two such vectors as input for the prediction of contact between two residues. The authors evaluated the performance of this method with several classifiers and showed that a deep neural network classifier paired with the stacked autoencoder significantly exceeded classical machine learning accuracy.</p>
<p>Because many studies used predefined higher-level features, one of the benefits of deep learning—automatic feature extraction—is not fully leveraged. More work is needed to determine the best ways to represent raw protein sequence information so that the full benefits of deep learning as an automatic feature extractor can be realized.</p>
<h3 id="mhc-peptide-binding">MHC-peptide binding</h3>
<p>An important type of PPI involves the immune system’s ability to recognize the body’s own cells. The major histocompatibility complex (MHC) plays a key role in regulating this process by binding antigens and displaying them on the cell surface to be recognized by T cells. Due to its importance in immunity and immune response, peptide-MHC binding prediction is a useful problem in computational biology, and one that must account for the allelic diversity in MHC-encoding gene region.</p>
<p>Shallow, feed-forward neural networks are competitive methods and have made progress toward pan-allele and pan-length peptide representations. Sequence alignment techniques are useful for representing variable-length peptides as uniform-length features <span class="citation" data-cites="ul5VuLBZ EKi3Bq3D">[<a href="#ref-ul5VuLBZ" role="doc-biblioref">292</a>,<a href="#ref-EKi3Bq3D" role="doc-biblioref">293</a>]</span>. For pan-allelic prediction, NetMHCpan <span class="citation" data-cites="QxZTL7xX KJjibUtO">[<a href="#ref-QxZTL7xX" role="doc-biblioref">294</a>,<a href="#ref-KJjibUtO" role="doc-biblioref">295</a>]</span> used a pseudo-sequence representation of the MHC class I molecule, which included only polymorphic peptide contact residues. The sequences of the peptide and MHC were then represented using both sparse vector encoding and Blosum encoding, in which amino acids are encoded by matrix score vectors. A comparable method to the NetMHC tools is MHCflurry <span class="citation" data-cites="9ZNxZTxD">[<a href="#ref-9ZNxZTxD" role="doc-biblioref">296</a>]</span>, a method which shows superior performance on peptides of lengths other than nine. MHCflurry adds placeholder amino acids to transform variable-length peptides to length 15 peptides. In training the MHCflurry feed-forward neural network <span class="citation" data-cites="1Hk3NTSn2">[<a href="#ref-1Hk3NTSn2" role="doc-biblioref">297</a>]</span>, the authors imputed missing MHC-peptide binding affinities using a Gibbs sampling method, showing that imputation improves performance for data-sets with roughly 100 or fewer training examples. MHCflurry’s imputation method increases its performance on poorly characterized alleles, making it competitive with NetMHCpan for this task. Kuksa et al. <span class="citation" data-cites="FRl0MTLd">[<a href="#ref-FRl0MTLd" role="doc-biblioref">298</a>]</span> developed a shallow, higher-order neural network (HONN) comprised of both mean and covariance hidden units to capture some of the higher-order dependencies between amino acid locations. Pretraining this HONN with a semi-restricted Boltzmann machine, the authors found that the performance of the HONN exceeded that of a simple deep neural network, as well as that of NetMHC.</p>
<p>Deep learning’s unique flexibility was recently leveraged by Bhattacharya et al. <span class="citation" data-cites="1aswoG70">[<a href="#ref-1aswoG70" role="doc-biblioref">299</a>]</span>, who used a gated RNN method called MHCnuggets to overcome the difficulty of multiple peptide lengths. Under this framework, they used smoothed sparse encoding to represent amino acids individually. Because MHCnuggets had to be trained for every MHC allele, performance was far better for alleles with abundant, balanced training data. Vang et al. <span class="citation" data-cites="12i8Apfdc">[<a href="#ref-12i8Apfdc" role="doc-biblioref">300</a>]</span> developed HLA-CNN, a method which maps amino acids onto a 15-dimensional vector space based on their context relation to other amino acids before making predictions with a CNN. In a comparison of several current methods, Bhattacharya et al. found that the top methods—NetMHC, NetMHCpan, MHCflurry, and MHCnuggets—showed comparable performance, but large differences in speed. Convolutional neural networks (in this case, HLA-CNN) showed comparatively poor performance, while shallow and recurrent neural networks performed the best. They found that MHCnuggets—the recurrent neural network—was by far the fastest-training among the top performing methods.</p>
<h3 id="ppi-networks-and-graph-analysis">PPI networks and graph analysis</h3>
<p>Because interacting proteins are more likely to share a similar function, the connectivity of a PPI network itself can be a valuable information source for the prediction of protein function <span class="citation" data-cites="8CiDACi3">[<a href="#ref-8CiDACi3" role="doc-biblioref">301</a>]</span>. To incorporate higher-order network information, it is necessary to find a lower-level embedding of network structure that preserves this higher-order structure. Rather than use hand-crafted network features, deep learning shows promise for the automatic discovery of predictive features within networks. For example, Navlakha <span class="citation" data-cites="WMwUw1o4">[<a href="#ref-WMwUw1o4" role="doc-biblioref">302</a>]</span> showed that a deep autoencoder was able to compress a graph to 40% of its original size, while being able to reconstruct 93% of the original graph’s edges, improving upon standard dimension reduction methods. To achieve this, each graph was represented as an adjacency matrix with rows sorted in descending node degree order, then flattened into a vector and given as input to the autoencoder. While the activity of some hidden layers correlated with several popular hand-crafted network features such as k-core size and graph density, this work showed that deep learning can effectively reduce graph dimensionality while retaining much of its structural information.</p>
<p>An important challenge in PPI network prediction is the task of combining different networks and types of networks. Gligorijevic et al. <span class="citation" data-cites="dkPu3iv1">[<a href="#ref-dkPu3iv1" role="doc-biblioref">303</a>]</span> developed a multimodal deep autoencoder, deepNF, to find a feature representation common among several different PPI networks. This common lower-level representation allows for the combination of various PPI data sources towards a single predictive task. An SVM classifier trained on the compressed features from the middle layer of the autoencoder outperformed previous methods in predicting protein function.</p>
<p>Hamilton et al. addressed the issue of large, heterogeneous, and changing networks with an inductive approach called GraphSAGE <span class="citation" data-cites="MwUPw4CD">[<a href="#ref-MwUPw4CD" role="doc-biblioref">304</a>]</span>. By finding node embeddings through learned aggregator functions that describe the node and its neighbors in the network, the GraphSAGE approach allows for the generalization of the model to new graphs. In a classification task for the prediction of protein function, Chen and Zhu <span class="citation" data-cites="FMsqNIQ4">[<a href="#ref-FMsqNIQ4" role="doc-biblioref">305</a>]</span> optimized this approach and enhanced the graph convolutional network with a preprocessing step that uses an approximation to the dropout operation. This preprocessing effectively reduces the number of graph convolutional layers and it significantly improves both training time and prediction accuracy.</p>
<h3 id="morphological-phenotypes">Morphological phenotypes</h3>
<p>A field poised for dramatic revolution by deep learning is bioimage analysis. Thus far, the primary use of deep learning for biological images has been for segmentation—that is, for the identification of biologically relevant structures in images such as nuclei, infected cells, or vasculature—in fluorescence or even brightfield channels <span class="citation" data-cites="40EG4ZEU">[<a href="#ref-40EG4ZEU" role="doc-biblioref">306</a>]</span>. Once so-called regions of interest have been identified, it is often straightforward to measure biological properties of interest, such as fluorescence intensities, textures, and sizes. Given the dramatic successes of deep learning in biological imaging, we simply refer to articles that review recent advancements <span class="citation" data-cites="MmRGFVUu 40EG4ZEU TutLhFSz">[<a href="#ref-MmRGFVUu" role="doc-biblioref">17</a>,<a href="#ref-40EG4ZEU" role="doc-biblioref">306</a>,<a href="#ref-TutLhFSz" role="doc-biblioref">307</a>]</span>. However, user-friendly tools must be developed for deep learning to become commonplace for biological image segmentation.</p>
<p>We anticipate an additional paradigm shift in bioimaging that will be brought about by deep learning: what if images of biological samples, from simple cell cultures to three-dimensional organoids and tissue samples, could be mined for much more extensive biologically meaningful information than is currently standard? For example, a recent study demonstrated the ability to predict lineage fate in hematopoietic cells up to three generations in advance of differentiation <span class="citation" data-cites="On4vW5aU">[<a href="#ref-On4vW5aU" role="doc-biblioref">308</a>]</span>. In biomedical research, most often biologists decide in advance what feature to measure in images from their assay system. Although classical methods of segmentation and feature extraction can produce hundreds of metrics per cell in an image, deep learning is unconstrained by human intuition and can in theory extract more subtle features through its hidden nodes. Already, there is evidence deep learning can surpass the efficacy of classical methods <span class="citation" data-cites="gllSeTW">[<a href="#ref-gllSeTW" role="doc-biblioref">309</a>]</span>, even using generic deep convolutional networks trained on natural images <span class="citation" data-cites="BMg062hc">[<a href="#ref-BMg062hc" role="doc-biblioref">310</a>]</span>, known as transfer learning. Recent work by Johnson et al. <span class="citation" data-cites="71c6rs2z">[<a href="#ref-71c6rs2z" role="doc-biblioref">311</a>]</span> demonstrated how the use of a conditional adversarial autoencoder allows for a probabilistic interpretation of cell and nuclear morphology and structure localization from fluorescence images. The proposed model is able to generalize well to a wide range of subcellular localizations. The generative nature of the model allows it to produce high-quality synthetic images predicting localization of subcellular structures by directly modeling the localization of fluorescent labels. Notably, this approach reduces the modeling time by omitting the subcellular structure segmentation step.</p>
<p>The impact of further improvements on biomedicine could be enormous. Comparing cell population morphologies using conventional methods of segmentation and feature extraction has already proven useful for functionally annotating genes and alleles, identifying the cellular target of small molecules, and identifying disease-specific phenotypes suitable for drug screening <span class="citation" data-cites="hkKO4QYl m3Ij21U8 McjXFLLq">[<a href="#ref-hkKO4QYl" role="doc-biblioref">312</a>,<a href="#ref-m3Ij21U8" role="doc-biblioref">313</a>,<a href="#ref-McjXFLLq" role="doc-biblioref">314</a>]</span>. Deep learning would bring to these new kinds of experiments—known as image-based profiling or morphological profiling—a higher degree of accuracy, stemming from the freedom from human-tuned feature extraction strategies.</p>
<h3 id="single-cell-data">Single-cell data</h3>
<p>Single-cell methods are generating excitement as biologists characterize the vast heterogeneity within unicellular species and between cells of the same tissue type in the same organism <span class="citation" data-cites="1AWC7HsO0">[<a href="#ref-1AWC7HsO0" role="doc-biblioref">315</a>]</span>. For instance, tumor cells and neurons can both harbor extensive somatic variation <span class="citation" data-cites="1GvfSy48x">[<a href="#ref-1GvfSy48x" role="doc-biblioref">316</a>]</span>. Understanding single-cell diversity in all its dimensions—genetic, epigenomic, transcriptomic, proteomic, morphologic, and metabolic—is key if treatments are to be targeted not only to a specific individual, but also to specific pathological subsets of cells. Single-cell methods also promise to uncover a wealth of new biological knowledge. A sufficiently large population of single cells will have enough representative “snapshots” to recreate timelines of dynamic biological processes. If tracking processes over time is not the limiting factor, single-cell techniques can provide maximal resolution compared to averaging across all cells in bulk tissue, enabling the study of transcriptional bursting with single-cell fluorescence <em>in situ</em> hybridization or the heterogeneity of epigenomic patterns with single-cell Hi-C or ATAC-seq <span class="citation" data-cites="QafUwNKn v97iPXDw">[<a href="#ref-QafUwNKn" role="doc-biblioref">317</a>,<a href="#ref-v97iPXDw" role="doc-biblioref">318</a>]</span>. Joint profiling of single-cell epigenomic and transcriptional states provides unprecedented views of regulatory processes <span class="citation" data-cites="1CAw3FaPI">[<a href="#ref-1CAw3FaPI" role="doc-biblioref">319</a>]</span>.</p>
<p>However, large challenges exist in studying single cells. Relatively few cells can be assayed at once using current droplet, imaging, or microwell technologies, and low-abundance molecules or modifications may not be detected by chance due to a phenomenon known as dropout, not to be confused with the dropout layer of deep learning. To solve this problem, Angermueller et al. <span class="citation" data-cites="19EJTHByG">[<a href="#ref-19EJTHByG" role="doc-biblioref">189</a>]</span> trained a neural network to predict the presence or absence of methylation of a specific CpG site in single cells based on surrounding methylation signal and underlying DNA sequence, achieving several percentage points of improvement compared to random forests or deep networks trained only on CpG or sequence information. Similar deep learning methods have been applied to impute low-resolution ChIP-seq signal from bulk tissue with great success, and they could easily be adapted to single-cell data <span class="citation" data-cites="Qbtqlmhf XimuXZlz">[<a href="#ref-Qbtqlmhf" role="doc-biblioref">217</a>,<a href="#ref-XimuXZlz" role="doc-biblioref">320</a>]</span>. Deep learning has also been useful for dealing with batch effects <span class="citation" data-cites="1ERrBiqG7">[<a href="#ref-1ERrBiqG7" role="doc-biblioref">321</a>]</span>.</p>
<p>Examining populations of single cells can reveal biologically meaningful subsets of cells as well as their underlying gene regulatory networks <span class="citation" data-cites="1HPu3R2B4">[<a href="#ref-1HPu3R2B4" role="doc-biblioref">322</a>]</span>. Unfortunately, machine learning methods generally struggle with imbalanced data—when there are many more examples of class 1 than class 2—because prediction accuracy is usually evaluated over the entire dataset. To tackle this challenge, Arvaniti et al. <span class="citation" data-cites="r3Gbjksq">[<a href="#ref-r3Gbjksq" role="doc-biblioref">323</a>]</span> classified healthy and cancer cells expressing 25 markers by using the most discriminative filters from a CNN trained on the data as a linear classifier. They achieved impressive performance, even for cell types where the subset percentage ranged from 0.1 to 1%, significantly outperforming logistic regression and distance-based outlier detection methods. However, they did not benchmark against random forests, which tend to work better for imbalanced data, and their data was relatively low dimensional.</p>
<p>Neural networks can also learn low-dimensional representations of single-cell gene expression data for visualization, clustering, and other tasks. Both scvis <span class="citation" data-cites="yJxCo4h1">[<a href="#ref-yJxCo4h1" role="doc-biblioref">324</a>]</span> and scVI <span class="citation" data-cites="RHqbJgpe">[<a href="#ref-RHqbJgpe" role="doc-biblioref">325</a>]</span> are unsupervised approaches based on variational autoencoders (VAEs). Whereas scvis primarily focuses on single-cell visualization as a replacement for t-Distributed Stochastic Neighbor Embedding <span class="citation" data-cites="TGyu2Woj">[<a href="#ref-TGyu2Woj" role="doc-biblioref">326</a>]</span>, the scVI model accounts for zero-inflated expression distributions and can impute zero values that are due to technical effects. Beyond VAEs, Lin et al. developed a supervised model to predict cell type <span class="citation" data-cites="owp8L957">[<a href="#ref-owp8L957" role="doc-biblioref">327</a>]</span>. Similar to transfer learning approaches for microscopy images <span class="citation" data-cites="BMg062hc">[<a href="#ref-BMg062hc" role="doc-biblioref">310</a>]</span>, they demonstrated that the hidden layer representations were informative in general and could be used to identify cellular subpopulations or match new cells to known cell types. The supervised neural network’s representation was better overall at retrieving cell types than alternatives, but all methods struggled to recover certain cell types such as hematopoietic stem cells and inner cell mass cells. As the Human Cell Atlas <span class="citation" data-cites="vk9ZInF3">[<a href="#ref-vk9ZInF3" role="doc-biblioref">328</a>]</span> and related efforts generate more single-cell expression data, there will be opportunities to assess how well these low-dimensional representations generalize to new cell types as well as abundant training data to learn broadly-applicable representations.</p>
<p>The sheer quantity of omic information that can be obtained from each cell, as well as the number of cells in each dataset, uniquely position single-cell data to benefit from deep learning. In the future, lineage tracing could be revolutionized by using autoencoders to reduce the feature space of transcriptomic or variant data followed by algorithms to learn optimal cell differentiation trajectories <span class="citation" data-cites="Oljj2W96">[<a href="#ref-Oljj2W96" role="doc-biblioref">329</a>]</span> or by feeding cell morphology and movement into neural networks <span class="citation" data-cites="On4vW5aU">[<a href="#ref-On4vW5aU" role="doc-biblioref">308</a>]</span>. Reinforcement learning algorithms <span class="citation" data-cites="2gn6PKkv">[<a href="#ref-2gn6PKkv" role="doc-biblioref">330</a>]</span> could be trained on the evolutionary dynamics of cancer cells or bacterial cells undergoing selection pressure and reveal whether patterns of adaptation are random or deterministic, allowing us to develop therapeutic strategies that forestall resistance. We are excited to see the creative applications of deep learning to single-cell biology that emerge over the next few years.</p>
<h3 id="metagenomics">Metagenomics</h3>
<p>Metagenomics, which refers to the study of genetic material—16S rRNA or whole-genome shotgun DNA—from microbial communities, has revolutionized the study of micro-scale ecosystems within and around us. In recent years, machine learning has proved to be a powerful tool for metagenomic analysis. 16S rRNA has long been used to deconvolve mixtures of microbial genomes, yet this ignores more than 99% of the genomic content. Subsequent tools aimed to classify 300–3000 bp reads from complex mixtures of microbial genomes based on tetranucleotide frequencies, which differ across organisms <span class="citation" data-cites="N9NzkOjA">[<a href="#ref-N9NzkOjA" role="doc-biblioref">331</a>]</span>, using supervised <span class="citation" data-cites="QV551Nlx 1HtJuEkb2">[<a href="#ref-QV551Nlx" role="doc-biblioref">332</a>,<a href="#ref-1HtJuEkb2" role="doc-biblioref">333</a>]</span> or unsupervised methods <span class="citation" data-cites="1HhqhBwrM">[<a href="#ref-1HhqhBwrM" role="doc-biblioref">334</a>]</span>. Then, researchers began to use techniques that could estimate relative abundances from an entire sample faster than classifying individual reads <span class="citation" data-cites="56wEWVIl RqhGD9c7 189TQrQA9 8DLzxOEt">[<a href="#ref-56wEWVIl" role="doc-biblioref">335</a>,<a href="#ref-RqhGD9c7" role="doc-biblioref">336</a>,<a href="#ref-189TQrQA9" role="doc-biblioref">337</a>,<a href="#ref-8DLzxOEt" role="doc-biblioref">338</a>]</span>. There is also great interest in identifying and annotating sequence reads <span class="citation" data-cites="qUGH5CX8 yFOAeemA">[<a href="#ref-qUGH5CX8" role="doc-biblioref">339</a>,<a href="#ref-yFOAeemA" role="doc-biblioref">340</a>]</span>. However, the focus on taxonomic and functional annotation is just the first step. Several groups have proposed methods to determine host or environment phenotypes from the organisms that are identified <span class="citation" data-cites="W0cYSf89 aI9g2UOc c5P9jHCg y9s5irW">[<a href="#ref-W0cYSf89" role="doc-biblioref">341</a>,<a href="#ref-aI9g2UOc" role="doc-biblioref">342</a>,<a href="#ref-c5P9jHCg" role="doc-biblioref">343</a>,<a href="#ref-y9s5irW" role="doc-biblioref">344</a>]</span> or overall sequence composition <span class="citation" data-cites="5W4KMSdT">[<a href="#ref-5W4KMSdT" role="doc-biblioref">345</a>]</span>. Also, researchers have looked into how feature selection can improve classification <span class="citation" data-cites="Vb3Fwx7d y9s5irW">[<a href="#ref-y9s5irW" role="doc-biblioref">344</a>,<a href="#ref-Vb3Fwx7d" role="doc-biblioref">346</a>]</span>, and techniques have been proposed that are classifier-independent <span class="citation" data-cites="1AN5UPfb1 O9D66oYa">[<a href="#ref-1AN5UPfb1" role="doc-biblioref">347</a>,<a href="#ref-O9D66oYa" role="doc-biblioref">348</a>]</span>.</p>
<p>Most neural networks are used for phylogenetic classification or functional annotation from sequence data where there is ample data for training. Neural networks have been applied successfully to gene annotation (e.g. Orphelia <span class="citation" data-cites="q1A2AEtO">[<a href="#ref-q1A2AEtO" role="doc-biblioref">349</a>]</span> and FragGeneScan <span class="citation" data-cites="QlbXLqH">[<a href="#ref-QlbXLqH" role="doc-biblioref">350</a>]</span>). Representations (similar to Word2Vec <span class="citation" data-cites="1GhHIDxuW">[<a href="#ref-1GhHIDxuW" role="doc-biblioref">105</a>]</span> in natural language processing) for protein family classification have been introduced and classified with a skip-gram neural network <span class="citation" data-cites="1E1PWjqTm">[<a href="#ref-1E1PWjqTm" role="doc-biblioref">351</a>]</span>. Recurrent neural networks show good performance for homology and protein family identification <span class="citation" data-cites="G8RKF6sz zYUI7tc1">[<a href="#ref-G8RKF6sz" role="doc-biblioref">352</a>,<a href="#ref-zYUI7tc1" role="doc-biblioref">353</a>]</span>.</p>
<p>One of the first techniques of <em>de novo</em> genome binning used self-organizing maps, a type of neural network <span class="citation" data-cites="1HhqhBwrM">[<a href="#ref-1HhqhBwrM" role="doc-biblioref">334</a>]</span>. Essinger et al. <span class="citation" data-cites="11wVLI2Hn">[<a href="#ref-11wVLI2Hn" role="doc-biblioref">354</a>]</span> used Adaptive Resonance Theory to cluster similar genomic fragments and showed that it had better performance than k-means. However, other methods based on interpolated Markov models <span class="citation" data-cites="c4rnN1wo">[<a href="#ref-c4rnN1wo" role="doc-biblioref">355</a>]</span> have performed better than these early genome binners. Neural networks can be slow and therefore have had limited use for reference-based taxonomic classification, with TAC-ELM <span class="citation" data-cites="Wz7VUS03">[<a href="#ref-Wz7VUS03" role="doc-biblioref">356</a>]</span> being the only neural network-based algorithm to taxonomically classify massive amounts of metagenomic data. An initial study successfully applied neural networks to taxonomic classification of 16S rRNA genes, with convolutional networks providing about 10% accuracy genus-level improvement over RNNs and random forests <span class="citation" data-cites="iPIJrVVs">[<a href="#ref-iPIJrVVs" role="doc-biblioref">357</a>]</span>. However, this study evaluated only 3000 sequences.</p>
<p>Neural network uses for classifying phenotype from microbial composition are just beginning. A simple multi-layer perceptron (MLP) was able to classify wound severity from microbial species present in the wound <span class="citation" data-cites="oas5tbC7">[<a href="#ref-oas5tbC7" role="doc-biblioref">358</a>]</span>. Recently, Ditzler et al. associated soil samples with pH level using MLPs, DBNs, and RNNs <span class="citation" data-cites="i38A0beL">[<a href="#ref-i38A0beL" role="doc-biblioref">359</a>]</span>. Besides classifying samples appropriately, internal phylogenetic tree nodes inferred by the networks represented features for low and high pH. Thus, hidden nodes might provide biological insight as well as new features for future metagenomic sample comparison. Also, an initial study has shown promise of these networks for diagnosing disease <span class="citation" data-cites="NQ5jiN7B">[<a href="#ref-NQ5jiN7B" role="doc-biblioref">360</a>]</span>.</p>
<p>Challenges remain in applying deep neural networks to metagenomics problems. They are not yet ideal for phenotype classification because most studies contain tens of samples and hundreds or thousands of features (species). Such underdetermined, or ill-conditioned, problems are still a challenge for deep neural networks that require many training examples. Also, due to convergence issues <span class="citation" data-cites="g2vvbB91">[<a href="#ref-g2vvbB91" role="doc-biblioref">361</a>]</span>, taxonomic classification of reads from whole genome sequencing seems out of reach at the moment for deep neural networks. There are only thousands of full-sequenced genomes as compared to hundreds of thousands of 16S rRNA sequences available for training.</p>
<p>However, because RNNs have been applied to base calls for the Oxford Nanopore long-read sequencer with some success <span class="citation" data-cites="Jw2asgH1">[<a href="#ref-Jw2asgH1" role="doc-biblioref">362</a>]</span> (discussed below), one day the entire pipeline, from denoising to functional classification, may be combined into one step using powerful LSTMs <span class="citation" data-cites="2cMhMv5A">[<a href="#ref-2cMhMv5A" role="doc-biblioref">363</a>]</span>. For example, metagenomic assembly usually requires binning then assembly, but could deep neural nets accomplish both tasks in one network? We believe the greatest potential in deep learning is to learn the complete characteristics of a metagenomic sample in one complex network.</p>
<h3 id="sequencing-and-variant-calling">Sequencing and variant calling</h3>
<p>While we have so far primarily discussed the role of deep learning in analyzing genomic data, deep learning can also substantially improve our ability to obtain the genomic data itself. We discuss two specific challenges: calling SNPs and indels (insertions and deletions) with high specificity and sensitivity and improving the accuracy of new types of data such as nanopore sequencing. These two tasks are critical for studying rare variation, allele-specific transcription and translation, and splice site mutations. In the clinical realm, sequencing of rare tumor clones and other genetic diseases will require accurate calling of SNPs and indels.</p>
<p>Current methods achieve relatively high (&gt;99%) precision at 90% recall for SNPs and indel calls from Illumina short-read data <span class="citation" data-cites="FVfZESYP">[<a href="#ref-FVfZESYP" role="doc-biblioref">364</a>]</span>, yet this leaves a large number of potentially clinically-important remaining false positives and false negatives. These methods have so far relied on experts to build probabilistic models that reliably separate signal from noise. However, this process is time consuming and fundamentally limited by how well we understand and can model the factors that contribute to noise. Recently, two groups have applied deep learning to construct data-driven unbiased noise models. One of these models, DeepVariant, leverages Inception, a neural network trained for image classification by Google Brain, by encoding reads around a candidate SNP as a 221x100 bitmap image, where each column is a nucleotide and each row is a read from the sample library <span class="citation" data-cites="FVfZESYP">[<a href="#ref-FVfZESYP" role="doc-biblioref">364</a>]</span>. The top 5 rows represent the reference, and the bottom 95 rows represent randomly sampled reads that overlap the candidate variant. Each RGBA (red/green/blue/alpha) image pixel encodes the base (A, C, G, T) as a different red value, quality score as a green value, strand as a blue value, and variation from the reference as the alpha value. The neural network outputs genotype probabilities for each candidate variant. They were able to achieve better performance than GATK <span class="citation" data-cites="NCr4QkOg">[<a href="#ref-NCr4QkOg" role="doc-biblioref">365</a>]</span>, a leading genotype caller, even when GATK was given information about population variation for each candidate variant. Another method, still in its infancy, hand-developed 62 features for each candidate variant and fed these vectors into a fully connected deep neural network <span class="citation" data-cites="GSLRw2L5">[<a href="#ref-GSLRw2L5" role="doc-biblioref">366</a>]</span>. Unfortunately, this feature set required at least 15 iterations of software development to fine-tune, which suggests that these models may not generalize.</p>
<p>Variant calling will benefit more from optimizing neural network architectures than from developing features by hand. An interesting and informative next step would be to rigorously test if encoding raw sequence and quality data as an image, tensor, or some other mixed format produces the best variant calls. Because many of the latest neural network architectures (ResNet, Inception, Xception, and others) are already optimized for and pre-trained on generic, large-scale image datasets <span class="citation" data-cites="VMkPJjVk">[<a href="#ref-VMkPJjVk" role="doc-biblioref">367</a>]</span>, encoding genomic data as images could prove to be a generally effective and efficient strategy.</p>
<p>In limited experiments, DeepVariant was robust to sequencing depth, read length, and even species <span class="citation" data-cites="FVfZESYP">[<a href="#ref-FVfZESYP" role="doc-biblioref">364</a>]</span>. However, a model built on Illumina data, for instance, may not be optimal for Pacific Biosciences long-read data or MinION nanopore data, which have vastly different specificity and sensitivity profiles and signal-to-noise characteristics. Recently, Boza et al. used bidirectional recurrent neural networks to infer the <em>E. coli</em> sequence from MinION nanopore electric current data with higher per-base accuracy than the proprietary hidden Markov model-based algorithm Metrichor <span class="citation" data-cites="Jw2asgH1">[<a href="#ref-Jw2asgH1" role="doc-biblioref">362</a>]</span>. Unfortunately, training any neural network requires a large amount of data, which is often not available for new sequencing technologies. To circumvent this, one very preliminary study simulated mutations and spiked them into somatic and germline RNA-seq data, then trained and tested a neural network on simulated paired RNA-seq and exome sequencing data <span class="citation" data-cites="ECTm1SuA">[<a href="#ref-ECTm1SuA" role="doc-biblioref">368</a>]</span>. However, because this model was not subsequently tested on ground-truth datasets, it is unclear whether simulation can produce sufficiently realistic data to produce reliable models.</p>
<p>Method development for interpreting new types of sequencing data has historically taken two steps: first, easily implemented hard cutoffs that prioritize specificity over sensitivity, then expert development of probabilistic models with hand-developed inputs <span class="citation" data-cites="ECTm1SuA">[<a href="#ref-ECTm1SuA" role="doc-biblioref">368</a>]</span>. We anticipate that these steps will be replaced by deep learning, which will infer features simply by its ability to optimize a complex model against data.</p>
<h3 id="neuroscience">Neuroscience</h3>
<p>Artificial neural networks were originally conceived as a model for computation in the brain <span class="citation" data-cites="1HVDhhwpK">[<a href="#ref-1HVDhhwpK" role="doc-biblioref">7</a>]</span>. Although deep neural networks have evolved to become a workhorse across many fields, there is still a strong connection between deep networks and the study of the brain. The rich parallel history of artificial neural networks in computer science and neuroscience is reviewed in <span class="citation" data-cites="WNE8N7Cp Exe9wdYF Oc2wJ1JO">[<a href="#ref-WNE8N7Cp" role="doc-biblioref">369</a>,<a href="#ref-Exe9wdYF" role="doc-biblioref">370</a>,<a href="#ref-Oc2wJ1JO" role="doc-biblioref">371</a>]</span>.</p>
<p>Convolutional neural networks were originally conceived as faithful models of visual information processing in the primate visual system, and are still considered so <span class="citation" data-cites="EGwetrwp">[<a href="#ref-EGwetrwp" role="doc-biblioref">372</a>]</span>. The activations of hidden units in consecutive layers of deep convolutional networks have been found to parallel the activity of neurons in consecutive brain regions involved in processing visual scenes. Such models of neural computation are called “encoding” models, as they predict how the nervous system might encode sensory information in the world.</p>
<p>Even when they are not directly modeling biological neurons, deep networks have been a useful computational tool in neuroscience. They have been developed as statistical time series models of neural activity in the brain. And in contrast to the encoding models described earlier, these models are used for decoding neural activity, for instance in brain machine interfaces <span class="citation" data-cites="1E5vZzUF2">[<a href="#ref-1E5vZzUF2" role="doc-biblioref">373</a>]</span>. They have been crucial to the field of connectomics, which is concerned with mapping the connectivity of biological neural networks in the brain. In connectomics, deep networks are used to segment the shapes of individual neurons and to infer their connectivity from 3D electron microscopic images <span class="citation" data-cites="1bJ0G7FE">[<a href="#ref-1bJ0G7FE" role="doc-biblioref">374</a>]</span>, and they have been also been used to infer causal connectivity from optical measurement and perturbation of neural activity <span class="citation" data-cites="JaDImylU">[<a href="#ref-JaDImylU" role="doc-biblioref">375</a>]</span>.</p>
<p>It is an exciting time for neuroscience. Recent rapid progress in deep networks continues to inspire new machine learning based models of brain computation <span class="citation" data-cites="WNE8N7Cp">[<a href="#ref-WNE8N7Cp" role="doc-biblioref">369</a>]</span>. And neuroscience continues to inspire new models of artificial intelligence <span class="citation" data-cites="Oc2wJ1JO">[<a href="#ref-Oc2wJ1JO" role="doc-biblioref">371</a>]</span>.</p>
<h2 id="the-impact-of-deep-learning-in-treating-disease-and-developing-new-treatments">The impact of deep learning in treating disease and developing new treatments</h2>
<p>Given the need to make better, faster interventions at the point of care—incorporating the complex calculus of a patient’s symptoms, diagnostics, and life history—there have been many attempts to apply deep learning to patient treatment. Success in this area could help to enable personalized healthcare or precision medicine <span class="citation" data-cites="VOQtVhWs 3JyJ3DTh">[<a href="#ref-VOQtVhWs" role="doc-biblioref">376</a>,<a href="#ref-3JyJ3DTh" role="doc-biblioref">377</a>]</span>. Earlier, we reviewed approaches for patient categorization. Here, we examine the potential for better treatment, which broadly, may be divided into methods for improved choices of interventions for patients and those for development of new interventions.</p>
<h3 id="clinical-decision-making">Clinical decision making</h3>
<p>In 1996, Tu <span class="citation" data-cites="kL0B4m9d">[<a href="#ref-kL0B4m9d" role="doc-biblioref">378</a>]</span> compared the effectiveness of artificial neural networks and logistic regression, questioning whether these techniques would replace traditional statistical methods for predicting medical outcomes such as myocardial infarction <span class="citation" data-cites="jdg2u7bX">[<a href="#ref-jdg2u7bX" role="doc-biblioref">379</a>]</span> or mortality <span class="citation" data-cites="xX68eyvs">[<a href="#ref-xX68eyvs" role="doc-biblioref">380</a>]</span>. He posited that while neural networks have several advantages in representational power, the difficulties in interpretation may limit clinical applications, a limitation that still remains today. In addition, the challenges faced by physicians parallel those encountered by deep learning. For a given patient, the number of possible diseases is very large, with a long tail of rare diseases and patients are highly heterogeneous and may present with very different signs and symptoms for the same disease. Still, in 2006 Lisboa and Taktak <span class="citation" data-cites="qxxwkSAT">[<a href="#ref-qxxwkSAT" role="doc-biblioref">381</a>]</span> examined the use of artificial neural networks in medical journals, concluding that they improved healthcare relative to traditional screening methods in 21 of 27 studies. Recent applications of deep learning in pharmacogenomics and pharmacoepigenomics show the potential for improving patient treatment response and outcome prediction using patient-specific data, pharmacogenomic targets, and pharmacological knowledge bases <span class="citation" data-cites="brPjEjYw">[<a href="#ref-brPjEjYw" role="doc-biblioref">20</a>]</span>.</p>
<p>While further progress has been made in using deep learning for clinical decision making, it is hindered by a challenge common to many deep learning applications: it is much easier to predict an outcome than to suggest an action to change the outcome. Several attempts <span class="citation" data-cites="1FE0F2pQ qXdO2aMm">[<a href="#ref-qXdO2aMm" role="doc-biblioref">121</a>,<a href="#ref-1FE0F2pQ" role="doc-biblioref">123</a>]</span> at recasting the clinical decision-making problem into a prediction problem (i.e. prediction of which treatment will most improve the patient’s health) have accurately predicted survival patterns, but technical and medical challenges remain for clinical adoption (similar to those for categorization). In particular, remaining barriers include actionable interpretability of deep learning models, fitting deep models to limited and heterogeneous data, and integrating complex predictive models into a dynamic clinical environment.</p>
<p>A critical challenge in providing treatment recommendations is identifying a causal relationship for each recommendation. Causal inference is often framed in terms of counterfactual question <span class="citation" data-cites="cpNVdlL7">[<a href="#ref-cpNVdlL7" role="doc-biblioref">382</a>]</span>. Johansson et al. <span class="citation" data-cites="173ftiSzF">[<a href="#ref-173ftiSzF" role="doc-biblioref">383</a>]</span> use deep neural networks to create representation models for covariates that capture nonlinear effects and show significant performance improvements over existing models. In a less formal approach, Kale et al. <span class="citation" data-cites="1GRT18Tt2">[<a href="#ref-1GRT18Tt2" role="doc-biblioref">384</a>]</span> first create a deep neural network to model clinical time series and then analyze the relationship of the hidden features to the output using a causal approach.</p>
<p>A common challenge for deep learning is the interpretability of the models and their predictions. The task of clinical decision making is necessarily risk-averse, so model interpretability is key. Without clear reasoning, it is difficult to establish trust in a model. As described above, there has been some work to directly assign treatment plans without interpretability; however, the removal of human experts from the decision-making loop make the models difficult to integrate with clinical practice. To alleviate this challenge, several studies have attempted to create more interpretable deep models, either specifically for healthcare or as a general procedure for deep learning (see Discussion).</p>
<h4 id="predicting-patient-trajectories">Predicting patient trajectories</h4>
<p>A common application for deep learning in this domain is the temporal structure of healthcare records. Many studies <span class="citation" data-cites="4zpZxjHR O7Vbecm2 fOaBA9Vc glyI7H6F">[<a href="#ref-4zpZxjHR" role="doc-biblioref">385</a>,<a href="#ref-O7Vbecm2" role="doc-biblioref">386</a>,<a href="#ref-fOaBA9Vc" role="doc-biblioref">387</a>,<a href="#ref-glyI7H6F" role="doc-biblioref">388</a>]</span> have used RNNs to categorize patients, but most stop short of suggesting clinical decisions. Nemati et al. <span class="citation" data-cites="16OQvsRqJ">[<a href="#ref-16OQvsRqJ" role="doc-biblioref">389</a>]</span> used deep reinforcement learning to optimize a heparin dosing policy for intensive care patients. However, because the ideal dosing policy is unknown, the model’s predictions must be evaluated on counter-factual data. This represents a common challenge when bridging the gap between research and clinical practice. Because the ground-truth is unknown, researchers struggle to evaluate model predictions in the absence of interventional data, but clinical application is unlikely until the model has been shown to be effective. The impressive applications of deep reinforcement learning to other domains <span class="citation" data-cites="2gn6PKkv">[<a href="#ref-2gn6PKkv" role="doc-biblioref">330</a>]</span> have relied on knowledge of the underlying processes (e.g. the rules of the game). Some models have been developed for targeted medical problems <span class="citation" data-cites="eCrLGgiX">[<a href="#ref-eCrLGgiX" role="doc-biblioref">390</a>]</span>, but a generalized engine is beyond current capabilities.</p>
<h4 id="clinical-trial-efficiency">Clinical trial efficiency</h4>
<p>A clinical deep learning task that has been more successful is the assignment of patients to clinical trials. Ithapu et al. <span class="citation" data-cites="eehGXQlY">[<a href="#ref-eehGXQlY" role="doc-biblioref">391</a>]</span> used a randomized denoising autoencoder to learn a multimodal imaging marker that predicts future cognitive and neural decline from positron emission tomography (PET), amyloid florbetapir PET, and structural magnetic resonance imaging. By accurately predicting which cases will progress to dementia, they were able to efficiently assign patients to a clinical trial and reduced the required sample sizes by a factor of five. Similarly, Artemov et al. <span class="citation" data-cites="mo3GQwJj">[<a href="#ref-mo3GQwJj" role="doc-biblioref">392</a>]</span> applied deep learning to predict which clinical trials were likely to fail and which were likely to succeed. By predicting the side effects and pathway activations of each drug and translating these activations to a success probability, their deep learning-based approach was able to significantly outperform a random forest classifier trained on gene expression changes. These approaches suggest promising directions to improve the efficiency of clinical trials and accelerate drug development.</p>
<h3 id="drug-repositioning">Drug repositioning</h3>
<p>Drug repositioning (or repurposing) is an attractive option for delivering new drugs to the market because of the high costs and failure rates associated with more traditional drug discovery approaches <span class="citation" data-cites="13c9OPizf 79Ktl2">[<a href="#ref-13c9OPizf" role="doc-biblioref">393</a>,<a href="#ref-79Ktl2" role="doc-biblioref">394</a>]</span>. A decade ago, the Connectivity Map <span class="citation" data-cites="Ot5bUkmI">[<a href="#ref-Ot5bUkmI" role="doc-biblioref">395</a>]</span> had a sizeable impact. Reverse matching disease gene expression signatures with a large set of reference compound profiles allowed researchers to formulate repurposing hypotheses at scale using a simple non-parametric test. Since then, several advanced computational methods have been applied to formulate and validate drug repositioning hypotheses <span class="citation" data-cites="gTwjIQqB 1BkEtNVsj ir7ElHha">[<a href="#ref-gTwjIQqB" role="doc-biblioref">396</a>,<a href="#ref-1BkEtNVsj" role="doc-biblioref">397</a>,<a href="#ref-ir7ElHha" role="doc-biblioref">398</a>]</span>. Using supervised learning and collaborative filtering to tackle this type of problem is proving successful, especially when coupling disease or compound omic data with topological information from protein-protein or protein-compound interaction networks <span class="citation" data-cites="M1EW8Rfl 16FEYidu2 18lqFDKRR">[<a href="#ref-M1EW8Rfl" role="doc-biblioref">399</a>,<a href="#ref-16FEYidu2" role="doc-biblioref">400</a>,<a href="#ref-18lqFDKRR" role="doc-biblioref">401</a>]</span>.</p>
<p>For example, Menden et al. <span class="citation" data-cites="QcwZC8wG">[<a href="#ref-QcwZC8wG" role="doc-biblioref">402</a>]</span> used a shallow neural network to predict sensitivity of cancer cell lines to drug treatment using both cell line and drug features, opening the door to precision medicine and drug repositioning opportunities in cancer. More recently, Aliper et al. <span class="citation" data-cites="EMDwvRGb">[<a href="#ref-EMDwvRGb" role="doc-biblioref">37</a>]</span> used gene- and pathway-level drug perturbation transcriptional profiles from the Library of Network-Based Cellular Signatures <span class="citation" data-cites="ppGS5h4v">[<a href="#ref-ppGS5h4v" role="doc-biblioref">403</a>]</span> to train a fully connected deep neural network to predict drug therapeutic uses and indications. By using confusion matrices and leveraging misclassification, the authors formulated a number of interesting hypotheses, including repurposing cardiovascular drugs such as otenzepad and pinacidil for neurological disorders.</p>
<p>Drug repositioning can also be approached by attempting to predict novel drug-target interactions and then repurposing the drug for the associated indication <span class="citation" data-cites="tOpadZQw 1SIuofeg">[<a href="#ref-tOpadZQw" role="doc-biblioref">404</a>,<a href="#ref-1SIuofeg" role="doc-biblioref">405</a>]</span>. Wang et al. <span class="citation" data-cites="TeIxEjqm">[<a href="#ref-TeIxEjqm" role="doc-biblioref">406</a>]</span> devised a pairwise input neural network with two hidden layers that takes two inputs, a drug and a target binding site, and predicts whether they interact. Wang et al. <span class="citation" data-cites="1AU7wzPqa">[<a href="#ref-1AU7wzPqa" role="doc-biblioref">38</a>]</span> trained individual RBMs for each target in a drug-target interaction network and used these models to predict novel interactions pointing to new indications for existing drugs. Wen et al. <span class="citation" data-cites="oTF8O79C">[<a href="#ref-oTF8O79C" role="doc-biblioref">39</a>]</span> extended this concept to deep learning by creating a DBN called DeepDTIs, which predicts interactions using chemical structure and protein sequence features.</p>
<p>Drug repositioning appears an obvious candidate for deep learning both because of the large amount of high-dimensional data available and the complexity of the question being asked. However, perhaps the most promising piece of work in this space <span class="citation" data-cites="EMDwvRGb">[<a href="#ref-EMDwvRGb" role="doc-biblioref">37</a>]</span> is more of a proof of concept than a real-world hypothesis-generation tool; notably, deep learning was used to predict drug indications but not for the actual repositioning. At present, some of the most popular state-of-the-art methods for signature-based drug repurposing <span class="citation" data-cites="cQAldRdg">[<a href="#ref-cQAldRdg" role="doc-biblioref">407</a>]</span> do not use predictive modeling. A mature and production-ready framework for drug repositioning via deep learning is currently missing.</p>
<h3 id="drug-development">Drug development</h3>
<h4 id="ligand-based-prediction-of-bioactivity">Ligand-based prediction of bioactivity</h4>
<p>High-throughput chemical screening in biomedical research aims to improve therapeutic options over a long term horizon <span class="citation" data-cites="1DTUK3YyI">[<a href="#ref-1DTUK3YyI" role="doc-biblioref">22</a>]</span>. The objective is to discover which small molecules (also referred to as chemical compounds or ligands) specifically affect the activity of a target, such as a kinase, protein-protein interaction, or broader cellular phenotype. This screening is often one of the first steps in a long drug discovery pipeline, where novel molecules are pursued for their ability to inhibit or enhance disease-relevant biological mechanisms <span class="citation" data-cites="RAadmvJN">[<a href="#ref-RAadmvJN" role="doc-biblioref">408</a>]</span>. Initial hits are confirmed to eliminate false positives and proceed to the lead generation stage <span class="citation" data-cites="1D6emOV6q">[<a href="#ref-1D6emOV6q" role="doc-biblioref">409</a>]</span>, where they are evaluated for absorption, distribution, metabolism, excretion, and toxicity (ADMET) and other properties. It is desirable to advance multiple lead series, clusters of structurally-similar active chemicals, for further optimization by medicinal chemists to protect against unexpected failures in the later stages of drug discovery <span class="citation" data-cites="RAadmvJN">[<a href="#ref-RAadmvJN" role="doc-biblioref">408</a>]</span>.</p>
<p>Computational work in this domain aims to identify sufficient candidate active compounds without exhaustively screening libraries of hundreds of thousands or millions of chemicals. Predicting chemical activity computationally is known as virtual screening. An ideal algorithm will rank a sufficient number of active compounds before the inactives, but the rankings of actives relative to other actives and inactives are less important <span class="citation" data-cites="cjj5vT3H">[<a href="#ref-cjj5vT3H" role="doc-biblioref">410</a>]</span>. Computational modeling also has the potential to predict ADMET traits for lead generation <span class="citation" data-cites="uP7SgBVd">[<a href="#ref-uP7SgBVd" role="doc-biblioref">411</a>]</span> and how drugs are metabolized <span class="citation" data-cites="7QsMcDYy">[<a href="#ref-7QsMcDYy" role="doc-biblioref">412</a>]</span>.</p>
<p>Ligand-based approaches train on chemicals’ features without modeling target features (e.g. protein structure). Neural networks have a long history in this domain <span class="citation" data-cites="xPkT1z7D gJE0ExFr">[<a href="#ref-gJE0ExFr" role="doc-biblioref">21</a>,<a href="#ref-xPkT1z7D" role="doc-biblioref">23</a>]</span>, and the 2012 Merck Molecular Activity Challenge on Kaggle generated substantial excitement about the potential for high-parameter deep learning approaches. The winning submission was an ensemble that included a multi-task multi-layer perceptron network <span class="citation" data-cites="1Dzz0P0qr">[<a href="#ref-1Dzz0P0qr" role="doc-biblioref">413</a>]</span>. The sponsors noted drastic improvements over a random forest baseline, remarking “we have seldom seen any method in the past 10 years that could consistently outperform [random forest] by such a margin” <span class="citation" data-cites="xOaTIeBY">[<a href="#ref-xOaTIeBY" role="doc-biblioref">414</a>]</span>, but not all outside experts were convinced <span class="citation" data-cites="KJCJKadA">[<a href="#ref-KJCJKadA" role="doc-biblioref">415</a>]</span>. Subsequent work (reviewed in more detail by Goh et al. <span class="citation" data-cites="zCt6PUXj">[<a href="#ref-zCt6PUXj" role="doc-biblioref">4</a>]</span>) explored the effects of jointly modeling far more targets than the Merck challenge <span class="citation" data-cites="F8fP2vAg yAoN5gTU">[<a href="#ref-F8fP2vAg" role="doc-biblioref">416</a>,<a href="#ref-yAoN5gTU" role="doc-biblioref">417</a>]</span>, with Ramsundar et al. <span class="citation" data-cites="yAoN5gTU">[<a href="#ref-yAoN5gTU" role="doc-biblioref">417</a>]</span> showing that the benefits of multi-task networks had not yet saturated even with 259 targets. Although DeepTox <span class="citation" data-cites="Y1D0SZrO">[<a href="#ref-Y1D0SZrO" role="doc-biblioref">418</a>]</span>, a deep learning approach, won another competition, the Toxicology in the 21st Century (Tox21) Data Challenge, it did not dominate alternative methods as thoroughly as in other domains. DeepTox was the top performer on 9 of 15 targets and highly competitive with the top performer on the others. However, for many targets there was little separation between the top two or three methods.</p>
<p>The nuanced Tox21 performance may be more reflective of the practical challenges encountered in ligand-based chemical screening than the extreme enthusiasm generated by the Merck competition. A study of 22 ADMET tasks demonstrated that there are limitations to multi-task transfer learning that are in part a consequence of the degree to which tasks are related <span class="citation" data-cites="uP7SgBVd">[<a href="#ref-uP7SgBVd" role="doc-biblioref">411</a>]</span>. Some of the ADMET datasets showed superior performance in multi-task models with only 22 ADMET tasks compared to multi-task models with over 500 less-similar tasks. In addition, the training datasets encountered in practical applications may be tiny relative to what is available in public datasets and organized competitions. A study of BACE-1 inhibitors included only 1547 compounds <span class="citation" data-cites="B4cL1o2P">[<a href="#ref-B4cL1o2P" role="doc-biblioref">419</a>]</span>. Machine learning models were able to train on this limited dataset, but overfitting was a challenge and the differences between random forests and a deep neural network were negligible, especially in the classification setting. Overfitting is still a problem in larger chemical screening datasets with tens or hundreds of thousands of compounds because the number of active compounds can be very small, on the order of 0.1% of all tested chemicals for a typical target <span class="citation" data-cites="WeiyYhfy">[<a href="#ref-WeiyYhfy" role="doc-biblioref">420</a>]</span>. This has motivated low-parameter neural networks that emphasize compound-compound similarity, such as influence-relevance voter <span class="citation" data-cites="cjj5vT3H 1E0x7QgLP">[<a href="#ref-cjj5vT3H" role="doc-biblioref">410</a>,<a href="#ref-1E0x7QgLP" role="doc-biblioref">421</a>]</span>, instead of predicting compound activity directly from chemical features.</p>
<h4 id="chemical-featurization-and-representation-learning">Chemical featurization and representation learning</h4>
<p>Much of the recent excitement in this domain has come from what could be considered a creative experimentation phase, in which deep learning has offered novel possibilities for feature representation and modeling of chemical compounds. A molecular graph, where atoms are labeled nodes and bonds are labeled edges, is a natural way to represent a chemical structure. Chemical features can be represented as a list of molecular descriptors such as molecular weight, atom counts, functional groups, charge representations, summaries of atom-atom relationships in the molecular graph, and more sophisticated derived properties <span class="citation" data-cites="17eGl2pn9">[<a href="#ref-17eGl2pn9" role="doc-biblioref">422</a>]</span>. Traditional machine learning approaches relied on preprocessing the graph into a feature vector of molecular descriptors or a fixed-width bit vector known as a fingerprint <span class="citation" data-cites="QnZ7V9Rd">[<a href="#ref-QnZ7V9Rd" role="doc-biblioref">423</a>]</span>. The same fingerprints have been used by some drug-target interaction methods discussed above <span class="citation" data-cites="oTF8O79C">[<a href="#ref-oTF8O79C" role="doc-biblioref">39</a>]</span>. An overly simplistic but approximately correct view of chemical fingerprints is that each bit represents the presence or absence of a particular chemical substructure in the molecular graph. Instead of using molecular descriptors or fingerprints as input, modern neural networks can represent chemicals as textual strings <span class="citation" data-cites="qpmV0H2p">[<a href="#ref-qpmV0H2p" role="doc-biblioref">424</a>]</span> or images <span class="citation" data-cites="PMuw2Jdj">[<a href="#ref-PMuw2Jdj" role="doc-biblioref">425</a>]</span> or operate directly on the molecular graph, which has enabled strategies for learning novel chemical representations.</p>
<p>Virtual screening and chemical property prediction have emerged as one of the major applications areas for graph-based neural networks. Duvenaud et al. <span class="citation" data-cites="Oe573FaL">[<a href="#ref-Oe573FaL" role="doc-biblioref">426</a>]</span> generalized standard circular fingerprints by substituting discrete operations in the fingerprinting algorithm with operations in a neural network, producing a real-valued feature vector instead of a bit vector. Other approaches offer trainable networks that can learn chemical feature representations that are optimized for a particular prediction task. Lusci et al. <span class="citation" data-cites="17Wih4Hd5">[<a href="#ref-17Wih4Hd5" role="doc-biblioref">427</a>]</span> applied recursive neural networks for directed acyclic graphs to undirected molecular graphs by creating an ensemble of directed graphs in which one atom is selected as the root node. Graph convolutions on undirected molecular graphs have eliminated the need to enumerate artificially directed graphs, learning feature vectors for atoms that are a function of the properties of neighboring atoms and local regions on the molecular graph <span class="citation" data-cites="145os4Y6t P4ixsM8i UKHPpRBn">[<a href="#ref-145os4Y6t" role="doc-biblioref">428</a>,<a href="#ref-P4ixsM8i" role="doc-biblioref">429</a>,<a href="#ref-UKHPpRBn" role="doc-biblioref">430</a>]</span>. More sophisticated graph algorithms <span class="citation" data-cites="dVi4xCKj nMXQBadV">[<a href="#ref-dVi4xCKj" role="doc-biblioref">431</a>,<a href="#ref-nMXQBadV" role="doc-biblioref">432</a>]</span> addressed limitations of standard graph convolutions that primarily operate on each node’s local neighborhood. We anticipate that these graph-based neural networks could also be applicable in other types of biological networks, such as the PPI networks we discussed previously.</p>
<p>Advances in chemical representation learning have also enabled new strategies for learning chemical-chemical similarity functions. Altae-Tran et al. developed a one-shot learning network <span class="citation" data-cites="P4ixsM8i">[<a href="#ref-P4ixsM8i" role="doc-biblioref">429</a>]</span> to address the reality that most practical chemical screening studies are unable to provide the thousands or millions of training compounds that are needed to train larger multi-task networks. Using graph convolutions to featurize chemicals, the network learns an embedding from compounds into a continuous feature space such that compounds with similar activities in a set of training tasks have similar embeddings. The approach is evaluated in an extremely challenging setting. The embedding is learned from a subset of prediction tasks (e.g. activity assays for individual proteins), and only one to ten labeled examples are provided as training data on a new task. On Tox21 targets, even when trained with <em>one</em> task-specific active compound and <em>one</em> inactive compound, the model is able to generalize reasonably well because it has learned an informative embedding function from the related tasks. Random forests, which cannot take advantage of the related training tasks, trained in the same setting are only slightly better than a random classifier. Despite the success on Tox21, performance on MUV datasets, which contains assays designed to be challenging for chemical informatics algorithms, is considerably worse. The authors also demonstrate the limitations of transfer learning as embeddings learned from the Tox21 assays have little utility for a drug adverse reaction dataset.</p>
<p>These novel, learned chemical feature representations may prove to be essential for accurately predicting why some compounds with similar structures yield similar target effects and others produce drastically different results. Currently, these methods are enticing but do not necessarily outperform classic approaches by a large margin. The neural fingerprints <span class="citation" data-cites="Oe573FaL">[<a href="#ref-Oe573FaL" role="doc-biblioref">426</a>]</span> were narrowly beaten by regression using traditional circular fingerprints on a drug efficacy prediction task but were superior for predicting solubility or photovoltaic efficiency. In the original study, graph convolutions <span class="citation" data-cites="145os4Y6t">[<a href="#ref-145os4Y6t" role="doc-biblioref">428</a>]</span> performed comparably to a multi-task network using standard fingerprints and slightly better than the neural fingerprints <span class="citation" data-cites="Oe573FaL">[<a href="#ref-Oe573FaL" role="doc-biblioref">426</a>]</span> on the drug efficacy task but were slightly worse than the influence-relevance voter method on an HIV dataset <span class="citation" data-cites="cjj5vT3H">[<a href="#ref-cjj5vT3H" role="doc-biblioref">410</a>]</span>. Broader recent benchmarking has shown that relative merits of these methods depends on the dataset and cross validation strategy <span class="citation" data-cites="11QhcW8tX">[<a href="#ref-11QhcW8tX" role="doc-biblioref">433</a>]</span>, though evaluation in this domain often uses area under the receiver operating characteristic curve (AUROC) <span class="citation" data-cites="bbJMoSfn">[<a href="#ref-bbJMoSfn" role="doc-biblioref">434</a>]</span>, which has limited utility due to the large class imbalance (see Discussion).</p>
<p>We remain optimistic for the potential of deep learning and specifically representation learning in drug discovery. Rigorous benchmarking on broad and diverse prediction tasks will be as important as novel neural network architectures to advance the state of the art and convincingly demonstrate superiority over traditional cheminformatics techniques. Fortunately, there has recently been much progress in this direction. The DeepChem software <span class="citation" data-cites="P4ixsM8i Ytvk62dX">[<a href="#ref-P4ixsM8i" role="doc-biblioref">429</a>,<a href="#ref-Ytvk62dX" role="doc-biblioref">435</a>]</span> and MoleculeNet benchmarking suite <span class="citation" data-cites="11QhcW8tX">[<a href="#ref-11QhcW8tX" role="doc-biblioref">433</a>]</span> built upon it contain chemical bioactivity and toxicity prediction datasets, multiple compound featurization approaches including graph convolutions, and various machine learning algorithms ranging from standard baselines like logistic regression and random forests to recent neural network architectures. Independent research groups have already contributed additional datasets and prediction algorithms to DeepChem. Adoption of common benchmarking evaluation metrics, datasets, and baseline algorithms has the potential to establish the practical utility of deep learning in chemical bioactivity prediction and lower the barrier to entry for machine learning researchers without biochemistry expertise.</p>
<p>One open question in ligand-based screening pertains to the benefits and limitations of transfer learning. Multi-task neural networks have shown the advantages of jointly modeling many targets <span class="citation" data-cites="F8fP2vAg yAoN5gTU">[<a href="#ref-F8fP2vAg" role="doc-biblioref">416</a>,<a href="#ref-yAoN5gTU" role="doc-biblioref">417</a>]</span>. Other studies have shown the limitations of transfer learning when the prediction tasks are insufficiently related <span class="citation" data-cites="uP7SgBVd P4ixsM8i">[<a href="#ref-uP7SgBVd" role="doc-biblioref">411</a>,<a href="#ref-P4ixsM8i" role="doc-biblioref">429</a>]</span>. This has important implications for representation learning. The typical approach to improve deep learning models by expanding the dataset size may not be applicable if only “related” tasks are beneficial, especially because task-task relatedness is ill-defined. The massive chemical state space will also influence the development of unsupervised representation learning methods <span class="citation" data-cites="qpmV0H2p 5GEmrHBd">[<a href="#ref-qpmV0H2p" role="doc-biblioref">424</a>,<a href="#ref-5GEmrHBd" role="doc-biblioref">436</a>]</span>. Future work will establish whether it is better to train on massive collections of diverse compounds, drug-like small molecules, or specialized subsets.</p>
<h4 id="structure-based-prediction-of-bioactivity">Structure-based prediction of bioactivity</h4>
<p>When protein structure is available, virtual screening has traditionally relied on docking programs to predict how a compound best fits in the target’s binding site and score the predicted ligand-target complex <span class="citation" data-cites="13iyYvEcB">[<a href="#ref-13iyYvEcB" role="doc-biblioref">437</a>]</span>. Recently, deep learning approaches have been developed to model protein structure, which is expected to improve upon the simpler drug-target interaction algorithms described above that represent proteins with feature vectors derived from amino acid sequences <span class="citation" data-cites="TeIxEjqm oTF8O79C">[<a href="#ref-oTF8O79C" role="doc-biblioref">39</a>,<a href="#ref-TeIxEjqm" role="doc-biblioref">406</a>]</span>.</p>
<p>Structure-based deep learning methods differ in whether they use experimentally-derived or predicted ligand-target complexes and how they represent the 3D structure. The Atomic CNN <span class="citation" data-cites="17YaKNLKk">[<a href="#ref-17YaKNLKk" role="doc-biblioref">438</a>]</span> and TopologyNet <span class="citation" data-cites="Oc6JOTS6">[<a href="#ref-Oc6JOTS6" role="doc-biblioref">439</a>]</span> models take 3D structures from PDBBind <span class="citation" data-cites="YO41GAOP">[<a href="#ref-YO41GAOP" role="doc-biblioref">440</a>]</span> as input, ensuring the ligand-target complexes are reliable. AtomNet <span class="citation" data-cites="Z7fd0BYf">[<a href="#ref-Z7fd0BYf" role="doc-biblioref">36</a>]</span> samples multiple ligand poses within the target binding site, and DeepVS <span class="citation" data-cites="Gue0c5Gb">[<a href="#ref-Gue0c5Gb" role="doc-biblioref">441</a>]</span> and Ragoza et al. <span class="citation" data-cites="bNBiIiTt">[<a href="#ref-bNBiIiTt" role="doc-biblioref">442</a>]</span> use a docking program to generate protein-compound complexes. If they are sufficiently accurate, these latter approaches would have wider applicability to a much larger set of compounds and proteins. However, incorrect ligand poses will be misleading during training, and the predictive performance is sensitive to the docking quality <span class="citation" data-cites="Gue0c5Gb">[<a href="#ref-Gue0c5Gb" role="doc-biblioref">441</a>]</span>.</p>
<p>There are two established options for representing a protein-compound complex. One option, a 3D grid, can featurize the input complex <span class="citation" data-cites="Z7fd0BYf bNBiIiTt">[<a href="#ref-Z7fd0BYf" role="doc-biblioref">36</a>,<a href="#ref-bNBiIiTt" role="doc-biblioref">442</a>]</span>. Each entry in the grid tracks the types of protein and ligand atoms in that region of the 3D space or descriptors derived from those atoms. Alternatively, DeepVS <span class="citation" data-cites="Gue0c5Gb">[<a href="#ref-Gue0c5Gb" role="doc-biblioref">441</a>]</span> and atomic convolutions <span class="citation" data-cites="17YaKNLKk">[<a href="#ref-17YaKNLKk" role="doc-biblioref">438</a>]</span> offer greater flexibility in their convolutions by eschewing the 3D grid. Instead, they each implement techniques for executing convolutions over atoms’ neighboring atoms in the 3D space. Gomes et al. demonstrate that currently random forest on a 1D feature vector that describes the 3D ligand-target structure generally outperforms neural networks on the same feature vector as well as atomic convolutions and ligand-based neural networks when predicting the continuous-valued inhibition constant on the PDBBind refined dataset <span class="citation" data-cites="17YaKNLKk">[<a href="#ref-17YaKNLKk" role="doc-biblioref">438</a>]</span>. However, in the long term, atomic convolutions may ultimately overtake grid-based methods, as they provide greater freedom to model atom-atom interactions and the forces that govern binding affinity.</p>
<h4 id="de-novo-drug-design"><em>De novo</em> drug design</h4>
<p><em>De novo</em> drug design attempts to model the typical design-synthesize-test cycle of drug discovery <span class="citation" data-cites="kJ4hy7E omzv9ryI">[<a href="#ref-kJ4hy7E" role="doc-biblioref">443</a>,<a href="#ref-omzv9ryI" role="doc-biblioref">444</a>]</span>. It explores an estimated 10<sup>60</sup> synthesizable organic molecules with drug-like properties without explicit enumeration <span class="citation" data-cites="WeiyYhfy">[<a href="#ref-WeiyYhfy" role="doc-biblioref">420</a>]</span>. To test or score structures, algorithms like those discussed earlier are used. To “design” and “synthesize”, traditional <em>de novo</em> design software relied on classical optimizers such as genetic algorithms. Unfortunately, this often leads to overfit, “weird” molecules, which are difficult to synthesize in the lab. Current programs have settled on rule-based virtual chemical reactions to generate molecular structures <span class="citation" data-cites="omzv9ryI">[<a href="#ref-omzv9ryI" role="doc-biblioref">444</a>]</span>. Deep learning models that generate realistic, synthesizable molecules have been proposed as an alternative. In contrast to the classical, symbolic approaches, generative models learned from data would not depend on laboriously encoded expert knowledge. The challenge of generating molecules has parallels to the generation of syntactically and semantically correct text <span class="citation" data-cites="15y7iq6HF">[<a href="#ref-15y7iq6HF" role="doc-biblioref">445</a>]</span>.</p>
<p>As deep learning models that directly output (molecular) graphs remain under-explored, generative neural networks for drug design typically represent chemicals with the simplified molecular-input line-entry system (SMILES), a standard string-based representation with characters that represent atoms, bonds, and rings <span class="citation" data-cites="8LWFFeYg">[<a href="#ref-8LWFFeYg" role="doc-biblioref">446</a>]</span>. This allows treating molecules as sequences and leveraging recent progress in recurrent neural networks. Gómez-Bombarelli et al. designed a SMILES-to-SMILES autoencoder to learn a continuous latent feature space for chemicals <span class="citation" data-cites="qpmV0H2p">[<a href="#ref-qpmV0H2p" role="doc-biblioref">424</a>]</span>. In this learned continuous space it was possible to interpolate between continuous representations of chemicals in a manner that is not possible with discrete (e.g. bit vector or string) features or in symbolic, molecular graph space. Even more interesting is the prospect of performing gradient-based or Bayesian optimization of molecules within this latent space. The strategy of constructing simple, continuous features before applying supervised learning techniques is reminiscent of autoencoders trained on high-dimensional EHR data <span class="citation" data-cites="5x3uMSKi">[<a href="#ref-5x3uMSKi" role="doc-biblioref">115</a>]</span>. A drawback of the SMILES-to-SMILES autoencoder is that not all SMILES strings produced by the autoencoder’s decoder correspond to valid chemical structures. Recently, the Grammar Variational Autoencoder, which takes the SMILES grammar into account and is guaranteed to produce syntactically valid SMILES, has been proposed to alleviate this issue <span class="citation" data-cites="AQ3N6Ayw">[<a href="#ref-AQ3N6Ayw" role="doc-biblioref">447</a>]</span>.</p>
<p>Another approach to <em>de novo</em> design is to train character-based RNNs on large collections of molecules, for example, ChEMBL <span class="citation" data-cites="x1nE5icc">[<a href="#ref-x1nE5icc" role="doc-biblioref">448</a>]</span>, to first obtain a generic generative model for drug-like compounds <span class="citation" data-cites="8LWFFeYg">[<a href="#ref-8LWFFeYg" role="doc-biblioref">446</a>]</span>. These generative models successfully learn the grammar of compound representations, with 94% <span class="citation" data-cites="1EayJRsI">[<a href="#ref-1EayJRsI" role="doc-biblioref">449</a>]</span> or nearly 98% <span class="citation" data-cites="8LWFFeYg">[<a href="#ref-8LWFFeYg" role="doc-biblioref">446</a>]</span> of generated SMILES corresponding to valid molecular structures. The initial RNN is then fine-tuned to generate molecules that are likely to be active against a specific target by either continuing training on a small set of positive examples <span class="citation" data-cites="8LWFFeYg">[<a href="#ref-8LWFFeYg" role="doc-biblioref">446</a>]</span> or adopting reinforcement learning strategies <span class="citation" data-cites="1EayJRsI lERqKdZJ">[<a href="#ref-1EayJRsI" role="doc-biblioref">449</a>,<a href="#ref-lERqKdZJ" role="doc-biblioref">450</a>]</span>. Both the fine-tuning and reinforcement learning approaches can rediscover known, held-out active molecules. The great flexibility of neural networks, and progress in generative models offers many opportunities for deep architectures in <em>de novo</em> design (e.g. the adaptation of GANs for molecules).</p>
<h2 id="discussion">Discussion</h2>
<p>Despite the disparate types of data and scientific goals in the learning tasks covered above, several challenges are broadly important for deep learning in the biomedical domain. Here we examine these factors that may impede further progress, ask what steps have already been taken to overcome them, and suggest future research directions.</p>
<h3 id="customizing-deep-learning-models-reflects-a-tradeoff-between-bias-and-variance">Customizing deep learning models reflects a tradeoff between bias and variance</h3>
<p>Some of the challenges in applying deep learning are shared with other machine learning methods. In particular, many problem-specific optimizations described in this review reflect a recurring universal tradeoff—controlling the flexibility of a model in order to maximize predictivity. Methods for adjusting the flexibility of deep learning models include dropout, reduced data projections, and transfer learning (described below). One way of understanding such model optimizations is that they incorporate external information to limit model flexibility and thereby improve predictions. This balance is formally described as a tradeoff between “bias and variance” <span class="citation" data-cites="yg8NW0K7">[<a href="#ref-yg8NW0K7" role="doc-biblioref">11</a>]</span>.</p>
<p>Although the bias-variance tradeoff is common to all machine learning applications, recent empirical and theoretical observations suggest that deep learning models may have uniquely advantageous generalization properties <span class="citation" data-cites="D2B03NVK bBG5t78u">[<a href="#ref-D2B03NVK" role="doc-biblioref">451</a>,<a href="#ref-bBG5t78u" role="doc-biblioref">452</a>]</span>. Nevertheless, additional advances will be needed to establish a coherent theoretical foundation that enables practitioners to better reason about their models from first principles.</p>
<h4 id="evaluation-metrics-for-imbalanced-classification">Evaluation metrics for imbalanced classification</h4>
<p>Making predictions in the presence of high class imbalance and differences between training and generalization data is a common feature of many large biomedical datasets, including deep learning models of genomic features, patient classification, disease detection, and virtual screening. Prediction of transcription factor binding sites exemplifies the difficulties with learning from highly imbalanced data. The human genome has 3 billion base pairs, and only a small fraction of them are implicated in specific biochemical activities. Less than 1% of the genome can be confidently labeled as bound for most transcription factors.</p>
<p>Estimating the false discovery rate (FDR) is a standard method of evaluation in genomics that can also be applied to deep learning model predictions of genomic features. Using deep learning predictions for targeted validation experiments of specific biochemical activities necessitates a more stringent FDR (typically 5–25%). However, when predicted biochemical activities are used as features in other models, such as gene expression models, a low FDR may not be necessary.</p>
<p>What is the correspondence between FDR metrics and commonly used classification metrics such as AUPR and AUROC? AUPR evaluates the average precision, or equivalently, the average FDR across all recall thresholds. This metric provides an overall estimate of performance across all possible use cases, which can be misleading for targeted validation experiments. For example, classification of TF binding sites can exhibit a recall of 0% at 10% FDR and AUPR greater than 0.6. In this case, the AUPR may be competitive, but the predictions are ill-suited for targeted validation that can only examine a few of the highest-confidence predictions. Likewise, AUROC evaluates the average recall across all false positive rate (FPR) thresholds, which is often a highly misleading metric in class-imbalanced domains <span class="citation" data-cites="JNnkm5Zt JmHFuXEM">[<a href="#ref-JNnkm5Zt" role="doc-biblioref">453</a>,<a href="#ref-JmHFuXEM" role="doc-biblioref">72</a>]</span>. Consider a classification model with recall of 0% at FDR less than 25% and 100% recall at FDR greater than 25%. In the context of TF binding predictions where only 1% of genomic regions are bound by the TF, this is equivalent to a recall of 100% for FPR greater than 0.33%. In other words, the AUROC would be 0.9967, but the classifier would be useless for targeted validation. It is not unusual to obtain a chromosome-wide AUROC greater than 0.99 for TF binding predictions but a recall of 0% at 10% FDR. Consequently, practitioners must select the metric most tailored to their subsequent use case to use these methods most effectively.</p>
<h4 id="formulation-of-classification-labels">Formulation of classification labels</h4>
<p>Genome-wide continuous signals are commonly formulated into classification labels through signal peak detection. ChIP-seq peaks are used to identify locations of TF binding and histone modifications. Such procedures rely on thresholding criteria to define what constitutes a peak in the signal. This inevitably results in a set of signal peaks that are close to the threshold, not sufficient to constitute a positive label but too similar to positively labeled examples to constitute a negative label. To avoid an arbitrary label for these examples they may be labeled as “ambiguous”. Ambiguously labeled examples can then be ignored during model training and evaluation of recall and FDR. The correlation between model predictions on these examples and their signal values can be used to evaluate if the model correctly ranks these examples between positive and negative examples.</p>
<h4 id="formulation-of-a-performance-upper-bound">Formulation of a performance upper bound</h4>
<p>In assessing the upper bound on the predictive performance of a deep learning model, it is necessary to incorporate inherent between-study variation inherent to biomedical research <span class="citation" data-cites="ZxQ49E8q">[<a href="#ref-ZxQ49E8q" role="doc-biblioref">454</a>]</span>. Study-level variability limits classification performance and can lead to underestimating prediction error if the generalization error is estimated by splitting a single dataset. Analyses can incorporate data from multiple labs and experiments to capture between-study variation within the prediction model mitigating some of these issues.</p>
<h3 id="uncertainty-quantification">Uncertainty quantification</h3>
<p>Deep learning based solutions for biomedical applications could substantially benefit from guarantees on the reliability of predictions and a quantification of uncertainty. Due to biological variability and precision limits of equipment, biomedical data do not consist of precise measurements but of estimates with noise. Hence, it is crucial to obtain uncertainty measures that capture how noise in input values propagates through deep neural networks. Such measures can be used for reliability assessment of automated decisions in clinical and public health applications, and for guarding against model vulnerabilities in the face of rare or adversarial cases <span class="citation" data-cites="P9vKrhYz">[<a href="#ref-P9vKrhYz" role="doc-biblioref">455</a>]</span>. Moreover, in fundamental biological research, measures of uncertainty help researchers distinguish between true regularities in the data and patterns that are false or merely anecdotal. There are two main uncertainties that one can calculate: epistemic and aleatoric <span class="citation" data-cites="1GDnudDWl">[<a href="#ref-1GDnudDWl" role="doc-biblioref">456</a>]</span>. Epistemic uncertainty describes uncertainty about the model, its structure, or its parameters. This uncertainty is caused by insufficient training data or by a difference in the training set and testing set distributions, so it vanishes in the limit of infinite data. On the other hand, aleatoric uncertainty describes uncertainty inherent in the observations. This uncertainty is due to noisy or missing data, so it vanishes with the ability to observe all independent variables with infinite precision. A good way to represent aleatoric uncertainty is to design an appropriate loss function with an uncertainty variable. In the case of data-dependent aleatoric uncertainty, one can train the model to increase its uncertainty when it is incorrect due to noisy or missing data, and in the case of task-dependent aleatoric uncertainty, one can optimize for the best uncertainty parameter for each task <span class="citation" data-cites="WZjlu7tr">[<a href="#ref-WZjlu7tr" role="doc-biblioref">457</a>]</span>. Meanwhile, there are various methods for modeling epistemic uncertainty, outlined below.</p>
<p>In classification tasks, confidence calibration is the problem of using classifier scores to predict class membership probabilities that match the true membership likelihoods. These membership probabilities can be used to assess the uncertainty associated with assigning the example to each of the classes. Guo et al. <span class="citation" data-cites="QJ6hYH8N">[<a href="#ref-QJ6hYH8N" role="doc-biblioref">458</a>]</span> observed that contemporary neural networks are poorly calibrated and provided a simple recommendation for calibration: temperature scaling, a single parameter special case of Platt scaling <span class="citation" data-cites="ZhuDaoNw">[<a href="#ref-ZhuDaoNw" role="doc-biblioref">459</a>]</span>. In addition to confidence calibration, there is early work from Chryssolouris et al. <span class="citation" data-cites="9SnNyc8Y">[<a href="#ref-9SnNyc8Y" role="doc-biblioref">460</a>]</span> that described a method for obtaining confidence intervals with the assumption of normally distributed error for the neural network. More recently, Hendrycks and Gimpel discovered that incorrect or out-of-distribution examples usually have lower maximum softmax probabilities than correctly classified examples, allowing for effective detection of misclassified examples <span class="citation" data-cites="Tkobp7Qj">[<a href="#ref-Tkobp7Qj" role="doc-biblioref">461</a>]</span>. Liang et al. used temperature scaling and small perturbations to further separate the softmax scores of correctly classified examples and the scores of out-of-distribution examples, allowing for more effective detection <span class="citation" data-cites="vJxaHm0b">[<a href="#ref-vJxaHm0b" role="doc-biblioref">462</a>]</span>. This approach outperformed the baseline approaches by a large margin, establishing a new state-of-the-art performance.</p>
<p>An alternative approach for obtaining principled uncertainty estimates from deep learning models is to use Bayesian neural networks. Deep learning models are usually trained to obtain the most likely parameters given the data. However, choosing the single most likely set of parameters ignores the uncertainty about which set of parameters (among the possible models that explain the given dataset) should be used. This sometimes leads to uncertainty in predictions when the chosen likely parameters produce high-confidence but incorrect results. On the other hand, the parameters of Bayesian neural networks are modeled as full probability distributions. This Bayesian approach comes with a whole host of benefits, including better calibrated confidence estimates <span class="citation" data-cites="InL72p4N">[<a href="#ref-InL72p4N" role="doc-biblioref">463</a>]</span> and more robustness to adversarial and out-of-distribution examples <span class="citation" data-cites="b5zIzCEO">[<a href="#ref-b5zIzCEO" role="doc-biblioref">464</a>]</span>. Unfortunately, modeling the full posterior distribution for the model’s parameters given the data is usually computationally intractable. One popular method for circumventing this high computational cost is called test-time dropout <span class="citation" data-cites="1FDihfnM">[<a href="#ref-1FDihfnM" role="doc-biblioref">465</a>]</span>, where an approximate posterior distribution is obtained using variational inference. Gal and Ghahramani showed that a stack of fully connected layers with dropout between the layers is equivalent to approximate inference in a Gaussian process model <span class="citation" data-cites="1FDihfnM">[<a href="#ref-1FDihfnM" role="doc-biblioref">465</a>]</span>. The authors interpret dropout as a variational inference method and apply their method to convolutional neural networks. This is simple to implement and preserves the possibility of obtaining cheap samples from the approximate posterior distribution. Operationally, obtaining model uncertainty for a given case becomes as straightforward as leaving dropout turned on and predicting multiple times. The spread of the different predictions is a reasonable proxy for model uncertainty. This technique has been successfully applied in an automated system for detecting diabetic retinopathy <span class="citation" data-cites="12PvceitW">[<a href="#ref-12PvceitW" role="doc-biblioref">466</a>]</span>, where uncertainty-informed referrals improved diagnostic performance and allowed the model to meet the National Health Service recommended levels of sensitivity and specificity. The authors also found that entropy performs comparably to the spread obtained via test-time dropout for identifying uncertain cases, and therefore it can be used instead for automated referrals.</p>
<p>Several other techniques have been proposed for effectively estimating predictive uncertainty as uncertainty quantification for neural networks continues to be an active research area. Recently, McClure and Kriegeskorte observed that test-time sampling improved calibration of the probabilistic predictions, sampling weights led to more robust uncertainty estimates than sampling units, and spike-and-slab sampling was superior to Gaussian dropconnect and Bernoulli dropout <span class="citation" data-cites="ZhgOMnnD">[<a href="#ref-ZhgOMnnD" role="doc-biblioref">467</a>]</span>. Krueger et al. introduced Bayesian hypernetworks <span class="citation" data-cites="emWW7yUp">[<a href="#ref-emWW7yUp" role="doc-biblioref">468</a>]</span> as another framework for approximate Bayesian inference in deep learning, where an invertible generative hypernetwork maps isotropic Gaussian noise to parameters of the primary network allowing for computationally cheap sampling and efficient estimation of the posterior. Meanwhile, Lakshminarayanan et al. proposed using deep ensembles, which are traditionally used for boosting predictive performance, on standard (non-Bayesian) neural networks to obtain well-calibrated uncertainty estimates that are comparable to those obtained by Bayesian neural networks <span class="citation" data-cites="Jy2oz6xk">[<a href="#ref-Jy2oz6xk" role="doc-biblioref">469</a>]</span>. In cases where model uncertainty is known to be caused by a difference in training and testing distributions, domain adaptation-based techniques can help mitigate the problem <span class="citation" data-cites="nu0eLZr0">[<a href="#ref-nu0eLZr0" role="doc-biblioref">244</a>]</span>.</p>
<p>Despite the success and popularity of deep learning, some deep learning models can be surprisingly brittle. Researchers are actively working on modifications to deep learning frameworks to enable them to handle probability and embrace uncertainty. Most notably, Bayesian modeling and deep learning are being integrated with renewed enthusiasm. As a result, several opportunities for innovation arise: understanding the causes of model uncertainty can lead to novel optimization and regularization techniques, assessing the utility of uncertainty estimation techniques on various model architectures and structures can be very useful to practitioners, and extending Bayesian deep learning to unsupervised settings can be a significant breakthrough <span class="citation" data-cites="LxptBw9l">[<a href="#ref-LxptBw9l" role="doc-biblioref">470</a>]</span>. Unfortunately, uncertainty quantification techniques are underutilized in the computational biology communities and largely ignored in the current deep learning for biomedicine literature. Thus, the practical value of uncertainty quantification in biomedical domains is yet to be appreciated.</p>
<h3 id="interpretation">Interpretation</h3>
<p>As deep learning models achieve state-of-the-art performance in a variety of domains, there is a growing need to make the models more interpretable. Interpretability matters for two main reasons. First, a model that achieves breakthrough performance may have identified patterns in the data that practitioners in the field would like to understand. However, this would not be possible if the model is a black box. Second, interpretability is important for trust. If a model is making medical diagnoses, it is important to ensure the model is making decisions for reliable reasons and is not focusing on an artifact of the data. A motivating example of this can be found in Ba and Caruana <span class="citation" data-cites="1AhGoHZP9">[<a href="#ref-1AhGoHZP9" role="doc-biblioref">471</a>]</span>, where a model trained to predict the likelihood of death from pneumonia assigned lower risk to patients with asthma, but only because such patients were treated as higher priority by the hospital. In the context of deep learning, understanding the basis of a model’s output is particularly important as deep learning models are unusually susceptible to adversarial examples <span class="citation" data-cites="1AkF8Wsv7">[<a href="#ref-1AkF8Wsv7" role="doc-biblioref">472</a>]</span> and can output confidence scores over 99.99% for samples that resemble pure noise.</p>
<p>As the concept of interpretability is quite broad, many methods described as improving the interpretability of deep learning models take disparate and often complementary approaches.</p>
<h4 id="assigning-example-specific-importance-scores">Assigning example-specific importance scores</h4>
<p>Several approaches ascribe importance on an example-specific basis to the parts of the input that are responsible for a particular output. These can be broadly divided into perturbation-based approaches and backpropagation-based approaches.</p>
<p>Perturbation-based approaches change parts of the input and observe the impact on the output of the network. Alipanahi et al. <span class="citation" data-cites="jJHZHWrl">[<a href="#ref-jJHZHWrl" role="doc-biblioref">227</a>]</span> and Zhou &amp; Troyanskaya <span class="citation" data-cites="2UI1BZuD">[<a href="#ref-2UI1BZuD" role="doc-biblioref">235</a>]</span> scored genomic sequences by introducing virtual mutations at individual positions in the sequence and quantifying the change in the output. Umarov et al. <span class="citation" data-cites="as2HfoSh">[<a href="#ref-as2HfoSh" role="doc-biblioref">248</a>]</span> used a similar strategy, but with sliding windows where the sequence within each sliding window was substituted with a random sequence. Kelley et al. <span class="citation" data-cites="2CbHXoFn">[<a href="#ref-2CbHXoFn" role="doc-biblioref">253</a>]</span> inserted known protein-binding motifs into the centers of sequences and assessed the change in predicted accessibility. Ribeiro et al. <span class="citation" data-cites="QwXSJhr0">[<a href="#ref-QwXSJhr0" role="doc-biblioref">473</a>]</span> introduced LIME, which constructs a linear model to locally approximate the output of the network on perturbed versions of the input and assigns importance scores accordingly. For analyzing images, Zeiler and Fergus <span class="citation" data-cites="voh0OiT2">[<a href="#ref-voh0OiT2" role="doc-biblioref">474</a>]</span> applied constant-value masks to different input patches. More recently, marginalizing over the plausible values of an input has been suggested as a way to more accurately estimate contributions <span class="citation" data-cites="Kk20paR7">[<a href="#ref-Kk20paR7" role="doc-biblioref">475</a>]</span>.</p>
<p>A common drawback to perturbation-based approaches is computational efficiency: each perturbed version of an input requires a separate forward propagation through the network to compute the output. As noted by Shrikumar et al. <span class="citation" data-cites="zhmq9ktJ">[<a href="#ref-zhmq9ktJ" role="doc-biblioref">245</a>]</span>, such methods may also underestimate the impact of features that have saturated their contribution to the output, as can happen when multiple redundant features are present. To reduce the computational overhead of perturbation-based approaches, Fong and Vedaldi <span class="citation" data-cites="niTcKl83">[<a href="#ref-niTcKl83" role="doc-biblioref">476</a>]</span> solve an optimization problem using gradient descent to discover a minimal subset of inputs to perturb in order to decrease the predicted probability of a selected class. Their method converges in many fewer iterations but requires the perturbation to have a differentiable form.</p>
<p>Backpropagation-based methods, in which the signal from a target output neuron is propagated backwards to the input layer, are another way to interpret deep networks that sidestep inefficiencies of the perturbation-based methods. A classic example of this is calculating the gradients of the output with respect to the input <span class="citation" data-cites="1YcKYTvO">[<a href="#ref-1YcKYTvO" role="doc-biblioref">477</a>]</span> to compute a “saliency map”. Bach et al. <span class="citation" data-cites="au5CLIOH">[<a href="#ref-au5CLIOH" role="doc-biblioref">478</a>]</span> proposed a strategy called Layerwise Relevance Propagation, which was shown to be equivalent to the element-wise product of the gradient and input <span class="citation" data-cites="zhmq9ktJ b1sc0cgP">[<a href="#ref-zhmq9ktJ" role="doc-biblioref">245</a>,<a href="#ref-b1sc0cgP" role="doc-biblioref">479</a>]</span>. Networks with Rectified Linear Units (ReLUs) create nonlinearities that must be addressed. Several variants exist for handling this <span class="citation" data-cites="voh0OiT2 f2L6isRj">[<a href="#ref-voh0OiT2" role="doc-biblioref">474</a>,<a href="#ref-f2L6isRj" role="doc-biblioref">480</a>]</span>. Backpropagation-based methods are a highly active area of research. Researchers are still actively identifying weaknesses <span class="citation" data-cites="vjXoJqO3">[<a href="#ref-vjXoJqO3" role="doc-biblioref">481</a>]</span>, and new methods are being developed to address them <span class="citation" data-cites="RZsNSRDS WzFOJBiA zhmq9ktJ">[<a href="#ref-RZsNSRDS" role="doc-biblioref">482</a>,<a href="#ref-WzFOJBiA" role="doc-biblioref">483</a>,<a href="#ref-zhmq9ktJ" role="doc-biblioref">245</a>]</span>. Lundberg and Lee <span class="citation" data-cites="DeOI1oGf">[<a href="#ref-DeOI1oGf" role="doc-biblioref">484</a>]</span> noted that several importance scoring methods including integrated gradients and LIME could all be considered approximations to Shapely values <span class="citation" data-cites="YBJdA6LJ">[<a href="#ref-YBJdA6LJ" role="doc-biblioref">485</a>]</span>, which have a long history in game theory for assigning contributions to players in cooperative games.</p>
<h4 id="matching-or-exaggerating-the-hidden-representation">Matching or exaggerating the hidden representation</h4>
<p>Another approach to understanding the network’s predictions is to find artificial inputs that produce similar hidden representations to a chosen example. This can elucidate the features that the network uses for prediction and drop the features that the network is insensitive to. In the context of natural images, Mahendran and Vedaldi <span class="citation" data-cites="19mGl6pfy">[<a href="#ref-19mGl6pfy" role="doc-biblioref">486</a>]</span> introduced the “inversion” visualization, which uses gradient descent and backpropagation to reconstruct the input from its hidden representation. The method required placing a prior on the input to favor results that resemble natural images. For genomic sequence, Finnegan and Song <span class="citation" data-cites="VjsZbMSz">[<a href="#ref-VjsZbMSz" role="doc-biblioref">487</a>]</span> used a Markov chain Monte Carlo algorithm to find the maximum-entropy distribution of inputs that produced a similar hidden representation to the chosen input.</p>
<p>A related idea is “caricaturization”, where an initial image is altered to exaggerate patterns that the network searches for <span class="citation" data-cites="1FkT6C6oa">[<a href="#ref-1FkT6C6oa" role="doc-biblioref">488</a>]</span>. This is done by maximizing the response of neurons that are active in the network, subject to some regularizing constraints. Mordvintsev et al. <span class="citation" data-cites="XLHInhc1">[<a href="#ref-XLHInhc1" role="doc-biblioref">489</a>]</span> leveraged caricaturization to generate aesthetically pleasing images using neural networks.</p>
<h4 id="activation-maximization">Activation maximization</h4>
<p>Activation maximization can reveal patterns detected by an individual neuron in the network by generating images which maximally activate that neuron, subject to some regularizing constraints. This technique was first introduced in Ehran et al. <span class="citation" data-cites="UAAd9Uez">[<a href="#ref-UAAd9Uez" role="doc-biblioref">490</a>]</span> and applied in subsequent work <span class="citation" data-cites="1YcKYTvO XLHInhc1 17i18PMkR 1FkT6C6oa">[<a href="#ref-1YcKYTvO" role="doc-biblioref">477</a>,<a href="#ref-1FkT6C6oa" role="doc-biblioref">488</a>,<a href="#ref-XLHInhc1" role="doc-biblioref">489</a>,<a href="#ref-17i18PMkR" role="doc-biblioref">491</a>]</span>. Lanchantin et al. <span class="citation" data-cites="Dwi2eAvT">[<a href="#ref-Dwi2eAvT" role="doc-biblioref">230</a>]</span> applied class-based activation maximization to genomic sequence data. One drawback of this approach is that neural networks often learn highly distributed representations where several neurons cooperatively describe a pattern of interest. Thus, visualizing patterns learned by individual neurons may not always be informative.</p>
<h4 id="rnn-specific-approaches">RNN-specific approaches</h4>
<p>Several interpretation methods are specifically tailored to recurrent neural network architectures. The most common form of interpretability provided by RNNs is through attention mechanisms, which have been used in diverse problems such as image captioning and machine translation to select portions of the input to focus on generating a particular output <span class="citation" data-cites="haHzVaaz yHn4SDRI">[<a href="#ref-haHzVaaz" role="doc-biblioref">492</a>,<a href="#ref-yHn4SDRI" role="doc-biblioref">493</a>]</span>. Deming et al. <span class="citation" data-cites="SAvEOARL">[<a href="#ref-SAvEOARL" role="doc-biblioref">494</a>]</span> applied the attention mechanism to models trained on genomic sequence. Attention mechanisms provide insight into the model’s decision-making process by revealing which portions of the input are used by different outputs. Singh et al. used a hierarchy of attention layers to locate important genome positions and signals for predicting gene expression from histone modifications <span class="citation" data-cites="16MNknNBL">[<a href="#ref-16MNknNBL" role="doc-biblioref">186</a>]</span>. In the clinical domain, Choi et al. <span class="citation" data-cites="UcRbawKo">[<a href="#ref-UcRbawKo" role="doc-biblioref">495</a>]</span> leveraged attention mechanisms to highlight which aspects of a patient’s medical history were most relevant for making diagnoses. Choi et al. <span class="citation" data-cites="10nDTiETi">[<a href="#ref-10nDTiETi" role="doc-biblioref">496</a>]</span> later extended this work to take into account the structure of disease ontologies and found that the concepts represented by the model aligned with medical knowledge. Note that interpretation strategies that rely on an attention mechanism do not provide insight into the logic used by the attention layer.</p>
<p>Visualizing the activation patterns of the hidden state of a recurrent neural network can also be instructive. Early work by Ghosh and Karamcheti <span class="citation" data-cites="oc44yBj0">[<a href="#ref-oc44yBj0" role="doc-biblioref">497</a>]</span> used cluster analysis to study hidden states of comparatively small networks trained to recognize strings from a finite state machine. More recently, Karpathy et al. <span class="citation" data-cites="2cpYveR4">[<a href="#ref-2cpYveR4" role="doc-biblioref">498</a>]</span> showed the existence of individual cells in LSTMs that kept track of quotes and brackets in character-level language models. To facilitate such analyses, LSTMVis <span class="citation" data-cites="1Ad3UOefc">[<a href="#ref-1Ad3UOefc" role="doc-biblioref">499</a>]</span> allows interactive exploration of the hidden state of LSTMs on different inputs.</p>
<p>Another strategy, adopted by Lanchatin et al. <span class="citation" data-cites="Dwi2eAvT">[<a href="#ref-Dwi2eAvT" role="doc-biblioref">230</a>]</span> looks at how the output of a recurrent neural network changes as longer and longer subsequences are supplied as input to the network, where the subsequences begin with just the first position and end with the entire sequence. In a binary classification task, this can identify those positions which are responsible for flipping the output of the network from negative to positive. If the RNN is bidirectional, the same process can be repeated on the reverse sequence. As noted by the authors, this approach was less effective at identifying motifs compared to the gradient-based backpropagation approach of Simonyan et al. <span class="citation" data-cites="1YcKYTvO">[<a href="#ref-1YcKYTvO" role="doc-biblioref">477</a>]</span>, illustrating the need for more sophisticated strategies to assign importance scores in recurrent neural networks.</p>
<p>Murdoch and Szlam <span class="citation" data-cites="10ViHstXn">[<a href="#ref-10ViHstXn" role="doc-biblioref">500</a>]</span> showed that the output of an LSTM can be decomposed into a product of factors, where each factor can be interpreted as the contribution at a particular timestep. The contribution scores were then used to identify key phrases from a model trained for sentiment analysis and obtained superior results compared to scores derived via a gradient-based approach.</p>
<h4 id="latent-space-manipulation">Latent space manipulation</h4>
<p>Interpretation of embedded or latent space features learned through generative unsupervised models can reveal underlying patterns otherwise masked in the original input. Embedded feature interpretation has been emphasized mostly in image and text based applications <span class="citation" data-cites="136QC0Zul 1GhHIDxuW">[<a href="#ref-1GhHIDxuW" role="doc-biblioref">105</a>,<a href="#ref-136QC0Zul" role="doc-biblioref">501</a>]</span>, but applications to genomic and biomedical domains are increasing.</p>
<p>For example, Way and Greene trained a VAE on gene expression from The Cancer Genome Atlas (TCGA) <span class="citation" data-cites="AK17eOgD">[<a href="#ref-AK17eOgD" role="doc-biblioref">502</a>]</span> and use latent space arithmetic to rapidly isolate and interpret gene expression features descriptive of high grade serous ovarian cancer subtypes <span class="citation" data-cites="aWn0df1m">[<a href="#ref-aWn0df1m" role="doc-biblioref">503</a>]</span>. The most differentiating VAE features were representative of biological processes that are known to distinguish the subtypes. Latent space arithmetic with features derived using other compression algorithms were not as informative in this context <span class="citation" data-cites="rMz1OFc6">[<a href="#ref-rMz1OFc6" role="doc-biblioref">504</a>]</span>. Embedding discrete chemical structures with autoencoders and interpreting the learned continuous representations with latent space arithmetic has also facilitated predicting drug-like compounds <span class="citation" data-cites="qpmV0H2p">[<a href="#ref-qpmV0H2p" role="doc-biblioref">424</a>]</span>. Furthermore, embedding biomedical text into lower dimensional latent spaces have improved name entity recognition in a variety of tasks including annotating clinical abbreviations, genes, cell lines, and drug names <span class="citation" data-cites="OuoHShLI 14uLNRP38 pWpK5WUq LeLKNlsR">[<a href="#ref-OuoHShLI" role="doc-biblioref">78</a>,<a href="#ref-14uLNRP38" role="doc-biblioref">79</a>,<a href="#ref-pWpK5WUq" role="doc-biblioref">80</a>,<a href="#ref-LeLKNlsR" role="doc-biblioref">81</a>]</span>.</p>
<p>Other approaches have used interpolation through latent space embeddings learned by GANs to interpret unobserved intermediate states. For example, Osokin et al. trained GANs on two-channel fluorescent microscopy images to interpret intermediate states of protein localization in yeast cells <span class="citation" data-cites="zBCcUQOM">[<a href="#ref-zBCcUQOM" role="doc-biblioref">505</a>]</span>. Goldsborough et al. trained a GAN on fluorescent microscopy images and used latent space interpolation and arithmetic to reveal underlying responses to small molecule perturbations in cell lines <span class="citation" data-cites="1EdCkau6d">[<a href="#ref-1EdCkau6d" role="doc-biblioref">506</a>]</span>.</p>
<h4 id="miscellaneous-approaches">Miscellaneous approaches</h4>
<p>It can often be informative to understand how the training data affects model learning. Toward this end, Koh and Liang <span class="citation" data-cites="69wxD9y">[<a href="#ref-69wxD9y" role="doc-biblioref">507</a>]</span> used influence functions, a technique from robust statistics, to trace a model’s predictions back through the learning algorithm to identify the datapoints in the training set that had the most impact on a given prediction. A more free-form approach to interpretability is to visualize the activation patterns of the network on individual inputs and on subsets of the data. ActiVis and CNNvis <span class="citation" data-cites="QphVo2P2 AEc66xxR">[<a href="#ref-QphVo2P2" role="doc-biblioref">508</a>,<a href="#ref-AEc66xxR" role="doc-biblioref">509</a>]</span> are two frameworks that enable interactive visualization and exploration of large-scale deep learning models. An orthogonal strategy is to use a knowledge distillation approach to replace a deep learning model with a more interpretable model that achieves comparable performance. Towards this end, Che et al. <span class="citation" data-cites="14DAmZTDg">[<a href="#ref-14DAmZTDg" role="doc-biblioref">510</a>]</span> used gradient boosted trees to learn interpretable healthcare features from trained deep models.</p>
<p>Finally, it is sometimes possible to train the model to provide justifications for its predictions. Lei et al. <span class="citation" data-cites="ZUCVI5eU">[<a href="#ref-ZUCVI5eU" role="doc-biblioref">511</a>]</span> used a generator to identify “rationales”, which are short and coherent pieces of the input text that produce similar results to the whole input when passed through an encoder. The authors applied their approach to a sentiment analysis task and obtained substantially superior results compared to an attention-based method.</p>
<h4 id="future-outlook">Future outlook</h4>
<p>While deep learning lags behind most Bayesian models in terms of interpretability, the interpretability of deep learning is comparable to or exceeds that of many other widely-used machine learning methods such as random forests or SVMs. While it is possible to obtain importance scores for different inputs in a random forest, the same is true for deep learning. Similarly, SVMs trained with a nonlinear kernel are not easily interpretable because the use of the kernel means that one does not obtain an explicit weight matrix. Finally, it is worth noting that some simple machine learning methods are less interpretable in practice than one might expect. A linear model trained on heavily engineered features might be difficult to interpret as the input features themselves are difficult to interpret. Similarly, a decision tree with many nodes and branches may also be difficult for a human to make sense of.</p>
<p>There are several directions that might benefit the development of interpretability techniques. The first is the introduction of gold standard benchmarks that different interpretability approaches could be compared against, similar in spirit to how the ImageNet <span class="citation" data-cites="cBVeXnZx">[<a href="#ref-cBVeXnZx" role="doc-biblioref">46</a>]</span> and CIFAR <span class="citation" data-cites="IcdRxiDv">[<a href="#ref-IcdRxiDv" role="doc-biblioref">512</a>]</span> datasets spurred the development of deep learning for computer vision. It would also be helpful if the community placed more emphasis on domains outside of computer vision. Computer vision is often used as the example application of interpretability methods, but it is not the domain with the most pressing need. Finally, closer integration of interpretability approaches with popular deep learning frameworks would make it easier for practitioners to apply and experiment with different approaches to understanding their deep learning models.</p>
<h3 id="data-limitations">Data limitations</h3>
<p>A lack of large-scale, high-quality, correctly labeled training data has impacted deep learning in nearly all applications we have discussed. The challenges of training complex, high-parameter neural networks from few examples are obvious, but uncertainty in the labels of those examples can be just as problematic. In genomics labeled data may be derived from an experimental assay with known and unknown technical artifacts, biases, and error profiles. It is possible to weight training examples or construct Bayesian models to account for uncertainty or non-independence in the data, as described in the TF binding example above. As another example, Park et al. <span class="citation" data-cites="5tvnB4uW">[<a href="#ref-5tvnB4uW" role="doc-biblioref">513</a>]</span> estimated shared non-biological signal between datasets to correct for non-independence related to assay platform or other factors in a Bayesian integration of many datasets. However, such techniques are rarely placed front and center in any description of methods and may be easily overlooked.</p>
<p>For some types of data, especially images, it is straightforward to augment training datasets by splitting a single labeled example into multiple examples. For example, an image can easily be rotated, flipped, or translated and retain its label <span class="citation" data-cites="9G9Hv1Pp">[<a href="#ref-9G9Hv1Pp" role="doc-biblioref">43</a>]</span>. 3D MRI and 4D fMRI (with time as a dimension) data can be decomposed into sets of 2D images <span class="citation" data-cites="11NHbWB1V">[<a href="#ref-11NHbWB1V" role="doc-biblioref">514</a>]</span>. This can greatly expand the number of training examples but artificially treats such derived images as independent instances and sacrifices the structure inherent in the data. CellCnn trains a model to recognize rare cell populations in single-cell data by creating training instances that consist of subsets of cells that are randomly sampled with replacement from the full dataset <span class="citation" data-cites="r3Gbjksq">[<a href="#ref-r3Gbjksq" role="doc-biblioref">323</a>]</span>.</p>
<p>Simulated or semi-synthetic training data has been employed in multiple biomedical domains, though many of these ideas are not specific to deep learning. Training and evaluating on simulated data, for instance, generating synthetic TF binding sites with position weight matrices <span class="citation" data-cites="iEmvzeT8">[<a href="#ref-iEmvzeT8" role="doc-biblioref">233</a>]</span> or RNA-seq reads for predicting mRNA transcript boundaries <span class="citation" data-cites="2M3zXijc">[<a href="#ref-2M3zXijc" role="doc-biblioref">515</a>]</span>, is a standard practice in bioinformatics. This strategy can help benchmark algorithms when the available gold standard dataset is imperfect, but it should be paired with an evaluation on real data, as in the prior examples <span class="citation" data-cites="iEmvzeT8 2M3zXijc">[<a href="#ref-iEmvzeT8" role="doc-biblioref">233</a>,<a href="#ref-2M3zXijc" role="doc-biblioref">515</a>]</span>. In rare cases, models trained on simulated data have been successfully applied directly to real data <span class="citation" data-cites="2M3zXijc">[<a href="#ref-2M3zXijc" role="doc-biblioref">515</a>]</span>.</p>
<p>Data can be simulated to create negative examples when only positive training instances are available. DANN <span class="citation" data-cites="15E5yG1Ho">[<a href="#ref-15E5yG1Ho" role="doc-biblioref">35</a>]</span> adopts this approach to predict the pathogenicity of genetic variants using semi-synthetic training data from Combined Annotation-Dependent Depletion (CADD) <span class="citation" data-cites="KxEzGxJ6">[<a href="#ref-KxEzGxJ6" role="doc-biblioref">516</a>]</span>. Though our emphasis here is on the training strategy, it should be noted that logistic regression outperformed DANN when distinguishing known pathogenic mutations from likely benign variants in real data. Similarly, a somatic mutation caller has been trained by injecting mutations into real sequencing datasets <span class="citation" data-cites="ECTm1SuA">[<a href="#ref-ECTm1SuA" role="doc-biblioref">368</a>]</span>. This method detected mutations in other semi-synthetic datasets but was not validated on real data.</p>
<p>In settings where the experimental observations are biased toward positive instances, such as MHC protein and peptide ligand binding affinity <span class="citation" data-cites="1Hk3NTSn2">[<a href="#ref-1Hk3NTSn2" role="doc-biblioref">297</a>]</span>, or the negative instances vastly outnumber the positives, such as high-throughput chemical screening <span class="citation" data-cites="1E0x7QgLP">[<a href="#ref-1E0x7QgLP" role="doc-biblioref">421</a>]</span>, training datasets have been augmented by adding additional instances and assuming they are negative. There is some evidence that this can improve performance <span class="citation" data-cites="1E0x7QgLP">[<a href="#ref-1E0x7QgLP" role="doc-biblioref">421</a>]</span>, but in other cases it was only beneficial when the real training datasets were extremely small <span class="citation" data-cites="1Hk3NTSn2">[<a href="#ref-1Hk3NTSn2" role="doc-biblioref">297</a>]</span>. Overall, training with simulated and semi-simulated data is a valuable idea for overcoming limited sample sizes but one that requires more rigorous evaluation on real ground-truth datasets before we can recommend it for widespread use. There is a risk that a model will easily discriminate synthetic examples but not generalize to real data.</p>
<p>Multimodal, multi-task, and transfer learning, discussed in detail below, can also combat data limitations to some degree. There are also emerging network architectures, such as Diet Networks for high-dimensional SNP data <span class="citation" data-cites="15JUKBg9y">[<a href="#ref-15JUKBg9y" role="doc-biblioref">517</a>]</span>. These use multiple networks to drastically reduce the number of free parameters by first flipping the problem and training a network to predict parameters (weights) for each input (SNP) to learn a feature embedding. This embedding (e.g. from principal component analysis, per class histograms, or a Word2vec <span class="citation" data-cites="1GhHIDxuW">[<a href="#ref-1GhHIDxuW" role="doc-biblioref">105</a>]</span> generalization) can be learned directly from input data or take advantage of other datasets or domain knowledge. Additionally, in this task the features are the examples, an important advantage when it is typical to have 500 thousand or more SNPs and only a few thousand patients. Finally, this embedding is of a much lower dimension, allowing for a large reduction in the number of free parameters. In the example given, the number of free parameters was reduced from 30 million to 50 thousand, a factor of 600.</p>
<h3 id="hardware-limitations-and-scaling">Hardware limitations and scaling</h3>
<p>Efficiently scaling deep learning is challenging, and there is a high computational cost (e.g. time, memory, and energy) associated with training neural networks and using them to make predictions. This is one of the reasons why neural networks have only recently found widespread use <span class="citation" data-cites="BQS8ClV0">[<a href="#ref-BQS8ClV0" role="doc-biblioref">518</a>]</span>.</p>
<p>Many have sought to curb these costs, with methods ranging from the very applied (e.g. reduced numerical precision <span class="citation" data-cites="CKcJuj03 1G3owNNps ybP8QCqL 1GUizyE8e">[<a href="#ref-CKcJuj03" role="doc-biblioref">519</a>,<a href="#ref-1G3owNNps" role="doc-biblioref">520</a>,<a href="#ref-ybP8QCqL" role="doc-biblioref">521</a>,<a href="#ref-1GUizyE8e" role="doc-biblioref">522</a>]</span>) to the exotic and theoretic (e.g. training small networks to mimic large networks and ensembles <span class="citation" data-cites="1AhGoHZP9 1CRF3gAV">[<a href="#ref-1AhGoHZP9" role="doc-biblioref">471</a>,<a href="#ref-1CRF3gAV" role="doc-biblioref">523</a>]</span>). The largest gains in efficiency have come from computation with GPUs <span class="citation" data-cites="F3e4wfzQ NSgduYNT IULiPa6L 13KjSCKB2 1FocAi7N0 BQS8ClV0">[<a href="#ref-BQS8ClV0" role="doc-biblioref">518</a>,<a href="#ref-F3e4wfzQ" role="doc-biblioref">524</a>,<a href="#ref-NSgduYNT" role="doc-biblioref">525</a>,<a href="#ref-IULiPa6L" role="doc-biblioref">526</a>,<a href="#ref-13KjSCKB2" role="doc-biblioref">527</a>,<a href="#ref-1FocAi7N0" role="doc-biblioref">528</a>]</span>, which excel at the matrix and vector operations so central to deep learning. The massively parallel nature of GPUs allows additional optimizations, such as accelerated mini-batch gradient descent <span class="citation" data-cites="NSgduYNT IULiPa6L aClNvbyM fNkl8HFz">[<a href="#ref-NSgduYNT" role="doc-biblioref">525</a>,<a href="#ref-IULiPa6L" role="doc-biblioref">526</a>,<a href="#ref-aClNvbyM" role="doc-biblioref">529</a>,<a href="#ref-fNkl8HFz" role="doc-biblioref">530</a>]</span>. However, GPUs also have limited memory, making networks of useful size and complexity difficult to implement on a single GPU or machine <span class="citation" data-cites="F3e4wfzQ CCS5KSIM">[<a href="#ref-F3e4wfzQ" role="doc-biblioref">524</a>,<a href="#ref-CCS5KSIM" role="doc-biblioref">68</a>]</span>. This restriction has sometimes forced computational biologists to use workarounds or limit the size of an analysis. Chen et al. <span class="citation" data-cites="12QQw9p7v">[<a href="#ref-12QQw9p7v" role="doc-biblioref">184</a>]</span> inferred the expression level of all genes with a single neural network, but due to memory restrictions they randomly partitioned genes into two separately analyzed halves. In other cases, researchers limited the size of their neural network <span class="citation" data-cites="BhfjKSY3">[<a href="#ref-BhfjKSY3" role="doc-biblioref">29</a>]</span> or the total number of training instances <span class="citation" data-cites="qpmV0H2p">[<a href="#ref-qpmV0H2p" role="doc-biblioref">424</a>]</span>. Some have also chosen to use standard central processing unit (CPU) implementations rather than sacrifice network size or performance <span class="citation" data-cites="x0M6vals">[<a href="#ref-x0M6vals" role="doc-biblioref">531</a>]</span>.</p>
<p>While steady improvements in GPU hardware may alleviate this issue, it is unclear whether advances will occur quickly enough to keep pace with the growing biological datasets and increasingly complex neural networks. Much has been done to minimize the memory requirements of neural networks <span class="citation" data-cites="YwdqeYZi 1AhGoHZP9 CKcJuj03 1G3owNNps ybP8QCqL 15lYGmZpY 1GUizyE8e">[<a href="#ref-1AhGoHZP9" role="doc-biblioref">471</a>,<a href="#ref-CKcJuj03" role="doc-biblioref">519</a>,<a href="#ref-1G3owNNps" role="doc-biblioref">520</a>,<a href="#ref-ybP8QCqL" role="doc-biblioref">521</a>,<a href="#ref-1GUizyE8e" role="doc-biblioref">522</a>,<a href="#ref-YwdqeYZi" role="doc-biblioref">532</a>,<a href="#ref-15lYGmZpY" role="doc-biblioref">533</a>]</span>, but there is also growing interest in specialized hardware, such as field-programmable gate arrays (FPGAs) <span class="citation" data-cites="1FocAi7N0 9NKsJjSw">[<a href="#ref-1FocAi7N0" role="doc-biblioref">528</a>,<a href="#ref-9NKsJjSw" role="doc-biblioref">534</a>]</span> and application-specific integrated circuits (ASICs) <span class="citation" data-cites="ULagTifF">[<a href="#ref-ULagTifF" role="doc-biblioref">535</a>]</span>. Less software is available for such highly specialized hardware <span class="citation" data-cites="9NKsJjSw">[<a href="#ref-9NKsJjSw" role="doc-biblioref">534</a>]</span>. But specialized hardware promises improvements in deep learning at reduced time, energy, and memory <span class="citation" data-cites="1FocAi7N0">[<a href="#ref-1FocAi7N0" role="doc-biblioref">528</a>]</span>. Specialized hardware may be a difficult investment for those not solely interested in deep learning, but for those with a deep learning focus these solutions may become popular.</p>
<p>Distributed computing is a general solution to intense computational requirements and has enabled many large-scale deep learning efforts. Some types of distributed computation <span class="citation" data-cites="xE3EYmck 1XcexUAV">[<a href="#ref-xE3EYmck" role="doc-biblioref">536</a>,<a href="#ref-1XcexUAV" role="doc-biblioref">537</a>]</span> are not suitable for deep learning <span class="citation" data-cites="17cBimWgp">[<a href="#ref-17cBimWgp" role="doc-biblioref">538</a>]</span>, but much progress has been made. There now exist a number of algorithms <span class="citation" data-cites="17cBimWgp ybP8QCqL">[<a href="#ref-ybP8QCqL" role="doc-biblioref">521</a>,<a href="#ref-17cBimWgp" role="doc-biblioref">538</a>]</span>, tools <span class="citation" data-cites="rmJZ2Aui rZnxDitd hOeUlCvS">[<a href="#ref-rmJZ2Aui" role="doc-biblioref">539</a>,<a href="#ref-rZnxDitd" role="doc-biblioref">540</a>,<a href="#ref-hOeUlCvS" role="doc-biblioref">541</a>]</span>, and high-level libraries <span class="citation" data-cites="FwEK0msb y9IoEy4r">[<a href="#ref-FwEK0msb" role="doc-biblioref">542</a>,<a href="#ref-y9IoEy4r" role="doc-biblioref">543</a>]</span> for deep learning in a distributed environment, and it is possible to train very complex networks with limited infrastructure <span class="citation" data-cites="4MZ2tmZ8">[<a href="#ref-4MZ2tmZ8" role="doc-biblioref">544</a>]</span>. Besides handling very large networks, distributed or parallelized approaches offer other advantages, such as improved ensembling <span class="citation" data-cites="JUF9VoRD">[<a href="#ref-JUF9VoRD" role="doc-biblioref">545</a>]</span> or accelerated hyperparameter optimization <span class="citation" data-cites="wz83yfHF 1FSwIjR9s">[<a href="#ref-wz83yfHF" role="doc-biblioref">546</a>,<a href="#ref-1FSwIjR9s" role="doc-biblioref">547</a>]</span>.</p>
<p>Cloud computing, which has already seen wide adoption in genomics <span class="citation" data-cites="B6g0qKf4">[<a href="#ref-B6g0qKf4" role="doc-biblioref">548</a>]</span>, could facilitate easier sharing of the large datasets common to biology <span class="citation" data-cites="1E7bFCRV4 q0SsFrZd">[<a href="#ref-1E7bFCRV4" role="doc-biblioref">549</a>,<a href="#ref-q0SsFrZd" role="doc-biblioref">550</a>]</span>, and may be key to scaling deep learning. Cloud computing affords researchers flexibility, and enables the use of specialized hardware (e.g. FPGAs, ASICs, GPUs) without major investment. As such, it could be easier to address the different challenges associated with the multitudinous layers and architectures available <span class="citation" data-cites="ZSVsnPVO">[<a href="#ref-ZSVsnPVO" role="doc-biblioref">551</a>]</span>. Though many are reluctant to store sensitive data (e.g. patient electronic health records) in the cloud, secure, regulation-compliant cloud services do exist <span class="citation" data-cites="ObFN78yp">[<a href="#ref-ObFN78yp" role="doc-biblioref">552</a>]</span>.</p>
<h3 id="data-code-and-model-sharing">Data, code, and model sharing</h3>
<p>A robust culture of data, code, and model sharing would speed advances in this domain. The cultural barriers to data sharing in particular are perhaps best captured by the use of the term “research parasite” to describe scientists who use data from other researchers <span class="citation" data-cites="o0F1MXBC">[<a href="#ref-o0F1MXBC" role="doc-biblioref">553</a>]</span>. A field that honors only discoveries and not the hard work of generating useful data will have difficulty encouraging scientists to share their hard-won data. It’s precisely those data that would help to power deep learning in the domain. Efforts are underway to recognize those who promote an ecosystem of rigorous sharing and analysis <span class="citation" data-cites="194IoYUs3">[<a href="#ref-194IoYUs3" role="doc-biblioref">554</a>]</span>.</p>
<p>The sharing of high-quality, labeled datasets will be especially valuable. In addition, researchers who invest time to preprocess datasets to be suitable for deep learning can make the preprocessing code (e.g. Basset <span class="citation" data-cites="2CbHXoFn">[<a href="#ref-2CbHXoFn" role="doc-biblioref">253</a>]</span> and variationanalysis <span class="citation" data-cites="GSLRw2L5">[<a href="#ref-GSLRw2L5" role="doc-biblioref">366</a>]</span>) and cleaned data (e.g. MoleculeNet <span class="citation" data-cites="11QhcW8tX">[<a href="#ref-11QhcW8tX" role="doc-biblioref">433</a>]</span>) publicly available to catalyze further research. However, there are complex privacy and legal issues involved in sharing patient data that cannot be ignored. Solving these issues will require increased understanding of privacy risks and standards specifying acceptable levels. In some domains high-quality training data has been generated privately, i.e. high-throughput chemical screening data at pharmaceutical companies. One perspective is that there is little expectation or incentive for this private data to be shared. However, data are not inherently valuable. Instead, the insights that we glean from them are where the value lies. Private companies may establish a competitive advantage by releasing data sufficient for improved methods to be developed. Recently, Ramsundar et al. did this with an open source platform DeepChem, where they released four privately generated datasets <span class="citation" data-cites="wzHEnMZe">[<a href="#ref-wzHEnMZe" role="doc-biblioref">555</a>]</span>.</p>
<p>Code sharing and open source licensing is essential for continued progress in this domain. We strongly advocate following established best practices for sharing source code, archiving code in repositories that generate digital object identifiers, and open licensing <span class="citation" data-cites="gvyja7v1">[<a href="#ref-gvyja7v1" role="doc-biblioref">556</a>]</span> regardless of the minimal requirements, or lack thereof, set by journals, conferences, or preprint servers. In addition, it is important for authors to share not only code for their core models but also scripts and code used for data cleaning (see above) and hyperparameter optimization. These improve reproducibility and serve as documentation of the detailed decisions that impact model performance but may not be exhaustively captured in a manuscript’s methods text.</p>
<p>Because many deep learning models are often built using one of several popular software frameworks, it is also possible to directly share trained predictive models. The availability of pre-trained models can accelerate research, with image classifiers as an apt example. A pre-trained neural network can be quickly fine-tuned on new data and used in transfer learning, as discussed below. Taking this idea to the extreme, genomic data has been artificially encoded as images in order to benefit from pre-trained image classifiers <span class="citation" data-cites="FVfZESYP">[<a href="#ref-FVfZESYP" role="doc-biblioref">364</a>]</span>. “Model zoos”—collections of pre-trained models—are not yet common in biomedical domains but have started to appear in genomics applications <span class="citation" data-cites="19EJTHByG 117PEpTMe">[<a href="#ref-19EJTHByG" role="doc-biblioref">189</a>,<a href="#ref-117PEpTMe" role="doc-biblioref">557</a>]</span>. However, it is important to note that sharing models trained on individual data requires great care because deep learning models can be attacked to identify examples used in training. One possible solution to protect individual samples includes training models under differential privacy <span class="citation" data-cites="LiCxcgZp">[<a href="#ref-LiCxcgZp" role="doc-biblioref">155</a>]</span>, which has been used in the biomedical domain <span class="citation" data-cites="fbIH12yd">[<a href="#ref-fbIH12yd" role="doc-biblioref">158</a>]</span>. We discussed this issue as well as recent techniques to mitigate these concerns in the patient categorization section.</p>
<p>DeepChem <span class="citation" data-cites="P4ixsM8i Ytvk62dX 11QhcW8tX">[<a href="#ref-P4ixsM8i" role="doc-biblioref">429</a>,<a href="#ref-11QhcW8tX" role="doc-biblioref">433</a>,<a href="#ref-Ytvk62dX" role="doc-biblioref">435</a>]</span> and DragoNN (Deep RegulAtory GenOmic Neural Networks) <span class="citation" data-cites="117PEpTMe">[<a href="#ref-117PEpTMe" role="doc-biblioref">557</a>]</span> exemplify the benefits of sharing pre-trained models and code under an open source license. DeepChem, which targets drug discovery and quantum chemistry, has actively encouraged and received community contributions of learning algorithms and benchmarking datasets. As a consequence, it now supports a large suite of machine learning approaches, both deep learning and competing strategies, that can be run on diverse test cases. This realistic, continual evaluation will play a critical role in assessing which techniques are most promising for chemical screening and drug discovery. Like formal, organized challenges such as the ENCODE-DREAM <em>in vivo</em> Transcription Factor Binding Site Prediction Challenge <span class="citation" data-cites="wW6QbBXz">[<a href="#ref-wW6QbBXz" role="doc-biblioref">239</a>]</span>, DeepChem provides a forum for the fair, critical evaluations that are not always conducted in individual methodological papers, which can be biased toward favoring a new proposed algorithm. Likewise DragoNN offers not only code and a model zoo but also a detailed tutorial and partner package for simulating training data. These resources, especially the ability to simulate datasets that are sufficiently complex to demonstrate the challenges of training neural networks but small enough to train quickly on a CPU, are important for training students and attracting machine learning researchers to problems in genomics and healthcare.</p>
<h3 id="multimodal-multi-task-and-transfer-learning">Multimodal, multi-task, and transfer learning</h3>
<p>The fact that biomedical datasets often contain a limited number of instances or labels can cause poor performance of deep learning algorithms. These models are particularly prone to overfitting due to their high representational power. However, transfer learning techniques, also known as domain adaptation, enable transfer of extracted patterns between different datasets and even domains. This approach consists of training a model for the base task and subsequently reusing the trained model for the target problem. The first step allows a model to take advantage of a larger amount of data and/or labels to extract better feature representations. Transferring learned features in deep neural networks improves performance compared to randomly initialized features even when pre-training and target sets are dissimilar. However, transferability of features decreases as the distance between the base task and target task increases <span class="citation" data-cites="enhj7VT6">[<a href="#ref-enhj7VT6" role="doc-biblioref">558</a>]</span>.</p>
<p>In image analysis, previous examples of deep transfer learning applications proved large-scale natural image sets <span class="citation" data-cites="cBVeXnZx">[<a href="#ref-cBVeXnZx" role="doc-biblioref">46</a>]</span> to be useful for pre-training models that serve as generic feature extractors for various types of biological images <span class="citation" data-cites="HlDY7trA z3I2IudI irSe12Sm BMg062hc">[<a href="#ref-irSe12Sm" role="doc-biblioref">15</a>,<a href="#ref-BMg062hc" role="doc-biblioref">310</a>,<a href="#ref-HlDY7trA" role="doc-biblioref">559</a>,<a href="#ref-z3I2IudI" role="doc-biblioref">560</a>]</span>. More recently, deep learning models predicted protein sub-cellular localization for proteins not originally present in a training set <span class="citation" data-cites="2a7MHtAx">[<a href="#ref-2a7MHtAx" role="doc-biblioref">561</a>]</span>. Moreover, learned features performed reasonably well even when applied to images obtained using different fluorescent labels, imaging techniques, and different cell types <span class="citation" data-cites="DcnNfASG">[<a href="#ref-DcnNfASG" role="doc-biblioref">562</a>]</span>. However, there are no established theoretical guarantees for feature transferability between distant domains such as natural images and various modalities of biological imaging. Because learned patterns are represented in deep neural networks in a layer-wise hierarchical fashion, this issue is usually addressed by fixing an empirically chosen number of layers that preserve generic characteristics of both training and target datasets. The model is then fine-tuned by re-training top layers on the specific dataset in order to re-learn domain-specific high level concepts (e.g. fine-tuning for radiology image classification <span class="citation" data-cites="x6HXFAS4">[<a href="#ref-x6HXFAS4" role="doc-biblioref">58</a>]</span>). Fine-tuning on specific biological datasets enables more focused predictions.</p>
<p>In genomics, the Basset package <span class="citation" data-cites="2CbHXoFn">[<a href="#ref-2CbHXoFn" role="doc-biblioref">253</a>]</span> for predicting chromatin accessibility was shown to rapidly learn and accurately predict on new data by leveraging a model pre-trained on available public data. To simulate this scenario, authors put aside 15 of 164 cell type datasets and trained the Basset model on the remaining 149 datasets. Then, they fine-tuned the model with one training pass of each of the remaining datasets and achieved results close to the model trained on all 164 datasets together. In another example, Min et al. <span class="citation" data-cites="jV2YerUS">[<a href="#ref-jV2YerUS" role="doc-biblioref">254</a>]</span> demonstrated how training on the experimentally-validated FANTOM5 permissive enhancer dataset followed by fine-tuning on ENCODE enhancer datasets improved cell type-specific predictions, outperforming state-of-the-art results. In drug design, general RNN models trained to generate molecules from the ChEMBL database have been fine-tuned to produce drug-like compounds for specific targets <span class="citation" data-cites="8LWFFeYg 1EayJRsI">[<a href="#ref-8LWFFeYg" role="doc-biblioref">446</a>,<a href="#ref-1EayJRsI" role="doc-biblioref">449</a>]</span>.</p>
<p>Related to transfer learning, multimodal learning assumes simultaneous learning from various types of inputs, such as images and text. It can capture features that describe common concepts across input modalities. Generative graphical models like RBMs, deep Boltzmann machines, and DBNs, demonstrate successful extraction of more informative features for one modality (images or video) when jointly learned with other modalities (audio or text) <span class="citation" data-cites="1eN66lwn">[<a href="#ref-1eN66lwn" role="doc-biblioref">563</a>]</span>. Deep graphical models such as DBNs are well-suited for multimodal learning tasks because they learn a joint probability distribution from inputs. They can be pre-trained in an unsupervised fashion on large unlabeled data and then fine-tuned on a smaller number of labeled examples. When labels are available, convolutional neural networks are ubiquitously used because they can be trained end-to-end with backpropagation and demonstrate state-of-the-art performance in many discriminative tasks <span class="citation" data-cites="irSe12Sm">[<a href="#ref-irSe12Sm" role="doc-biblioref">15</a>]</span>.</p>
<p>Jha et al. <span class="citation" data-cites="N0HBi8MH">[<a href="#ref-N0HBi8MH" role="doc-biblioref">216</a>]</span> showed that integrated training delivered better performance than individual networks. They compared a number of feed-forward architectures trained on RNA-seq data with and without an additional set of CLIP-seq, knockdown, and over-expression based input features. The integrative deep model generalized well for combined data, offering a large performance improvement for alternative splicing event estimation. Chaudhary et al. <span class="citation" data-cites="obeRVckH">[<a href="#ref-obeRVckH" role="doc-biblioref">564</a>]</span> trained a deep autoencoder model jointly on RNA-seq, miRNA-seq, and methylation data from TCGA to predict survival subgroups of hepatocellular carcinoma patients. This multimodal approach that treated different omic data types as different modalities outperformed both traditional methods (principal component analysis) and single-omic models. Interestingly, multi-omic model performance did not improve when combined with clinical information, suggesting that the model was able to capture redundant contributions of clinical features through their correlated genomic features. Chen et al. <span class="citation" data-cites="rmjDc5rm">[<a href="#ref-rmjDc5rm" role="doc-biblioref">179</a>]</span> used deep belief networks to learn phosphorylation states of a common set of signaling proteins in primary cultured bronchial cells collected from rats and humans treated with distinct stimuli. By interpreting species as different modalities representing similar high-level concepts, they showed that DBNs were able to capture cross-species representation of signaling mechanisms in response to a common stimuli. Another application used DBNs for joint unsupervised feature learning from cancer datasets containing gene expression, DNA methylation, and miRNA expression data <span class="citation" data-cites="1EtavGKI4">[<a href="#ref-1EtavGKI4" role="doc-biblioref">187</a>]</span>. This approach allowed for the capture of intrinsic relationships in different modalities and for better clustering performance over conventional k-means.</p>
<p>Multimodal learning with CNNs is usually implemented as a collection of individual networks in which each learns representations from single data type. These individual representations are further concatenated before or within fully-connected layers. FIDDLE <span class="citation" data-cites="yOz8Ybj2">[<a href="#ref-yOz8Ybj2" role="doc-biblioref">565</a>]</span> is an example of a multimodal CNN that represents an ensemble of individual networks that take NET-seq, MNase-seq, ChIP-seq, RNA-seq, and raw DNA sequence as input to predict transcription start sites. The combined model radically improves performance over separately trained datatype-specific networks, suggesting that it learns the synergistic relationship between datasets.</p>
<p>Multi-task learning is an approach related to transfer learning. In a multi-task learning framework, a model learns a number of tasks simultaneously such that features are shared across them. DeepSEA <span class="citation" data-cites="2UI1BZuD">[<a href="#ref-2UI1BZuD" role="doc-biblioref">235</a>]</span> implemented multi-task joint learning of diverse chromatin factors from raw DNA sequence. This allowed a sequence feature that was effective in recognizing binding of a specific TF to be simultaneously used by another predictor for a physically interacting TF. Similarly, TFImpute <span class="citation" data-cites="Qbtqlmhf">[<a href="#ref-Qbtqlmhf" role="doc-biblioref">217</a>]</span> learned information shared across transcription factors and cell lines to predict cell-specific TF binding for TF-cell line combinations. Yoon et al. <span class="citation" data-cites="yUgE09ve">[<a href="#ref-yUgE09ve" role="doc-biblioref">104</a>]</span> demonstrated that predicting the primary cancer site from cancer pathology reports together with its laterality substantially improved the performance for the latter task, indicating that multi-task learning can effectively leverage the commonality between two tasks using a shared representation. Many studies employed multi-task learning to predict chemical bioactivity <span class="citation" data-cites="1Dzz0P0qr yAoN5gTU">[<a href="#ref-1Dzz0P0qr" role="doc-biblioref">413</a>,<a href="#ref-yAoN5gTU" role="doc-biblioref">417</a>]</span> and drug toxicity <span class="citation" data-cites="Y1D0SZrO 1BARarxfz">[<a href="#ref-Y1D0SZrO" role="doc-biblioref">418</a>,<a href="#ref-1BARarxfz" role="doc-biblioref">566</a>]</span>. Kearnes et al. <span class="citation" data-cites="uP7SgBVd">[<a href="#ref-uP7SgBVd" role="doc-biblioref">411</a>]</span> systematically compared single-task and multi-task models for ADMET properties and found that multi-task learning generally improved performance. Smaller datasets tended to benefit more than larger datasets.</p>
<p>Multi-task learning is complementary to multimodal and transfer learning. All three techniques can be used together in the same model. For example, Zhang et al. <span class="citation" data-cites="HlDY7trA">[<a href="#ref-HlDY7trA" role="doc-biblioref">559</a>]</span> combined deep model-based transfer and multi-task learning for cross-domain image annotation. One could imagine extending that approach to multimodal inputs as well. A common characteristic of these methods is better generalization of extracted features at various hierarchical levels of abstraction, which is attained by leveraging relationships between various inputs and task objectives.</p>
<p>Despite demonstrated improvements, transfer learning approaches pose challenges. There are no theoretically sound principles for pre-training and fine-tuning. Best practice recommendations are heuristic and must account for additional hyper-parameters that depend on specific deep architectures, sizes of the pre-training and target datasets, and similarity of domains. However, similarity of datasets and domains in transfer learning and relatedness of tasks in multi-task learning are difficult to access. Most studies address these limitations by empirical evaluation of the model. Unfortunately, negative results are typically not reported. A deep CNN trained on natural images boosts performance in radiographic images <span class="citation" data-cites="x6HXFAS4">[<a href="#ref-x6HXFAS4" role="doc-biblioref">58</a>]</span>. However, due to differences in imaging domains, the target task required either re-training the initial model from scratch with special pre-processing or fine-tuning of the whole network on radiographs with heavy data augmentation to avoid overfitting. Exclusively fine-tuning top layers led to much lower validation accuracy (81.4 versus 99.5). Fine-tuning the aforementioned Basset model with more than one pass resulted in overfitting <span class="citation" data-cites="2CbHXoFn">[<a href="#ref-2CbHXoFn" role="doc-biblioref">253</a>]</span>. DeepChem successfully improved results for low-data drug discovery with one-shot learning for related tasks. However, it clearly demonstrated the limitations of cross-task generalization across unrelated tasks in one-shot models, specifically nuclear receptor assays and patient adverse reactions <span class="citation" data-cites="P4ixsM8i">[<a href="#ref-P4ixsM8i" role="doc-biblioref">429</a>]</span>.</p>
<p>In the medical domain, multimodal, multi-task and transfer learning strategies not only inherit most methodological issues from natural image, text, and audio domains, but also pose domain-specific challenges. There is a compelling need for the development of privacy-preserving transfer learning algorithms, such as Private Aggregation of Teacher Ensembles <span class="citation" data-cites="b8DJ1u6W">[<a href="#ref-b8DJ1u6W" role="doc-biblioref">161</a>]</span>. We suggest that these types of models deserve deeper investigation to establish sound theoretical guarantees and determine limits for the transferability of features between various closely related and distant learning tasks.</p>
<h2 id="conclusions">Conclusions</h2>
<p>Deep learning-based methods now match or surpass the previous state of the art in a diverse array of tasks in patient and disease categorization, fundamental biological study, genomics, and treatment development. Returning to our central question: given this rapid progress, has deep learning transformed the study of human disease? Though the answer is highly dependent on the specific domain and problem being addressed, we conclude that deep learning has not yet realized its transformative potential or induced a strategic inflection point. Despite its dominance over competing machine learning approaches in many of the areas reviewed here and quantitative improvements in predictive performance, deep learning has not yet definitively “solved” these problems.</p>
<p>As an analogy, consider recent progress in conversational speech recognition. Since 2009 there have been drastic performance improvements with error rates dropping from more than 20% to less than 6% <span class="citation" data-cites="nyjAIan4">[<a href="#ref-nyjAIan4" role="doc-biblioref">567</a>]</span> and finally approaching or exceeding human performance in the past year <span class="citation" data-cites="M2OLWojE wKioubsT">[<a href="#ref-M2OLWojE" role="doc-biblioref">568</a>,<a href="#ref-wKioubsT" role="doc-biblioref">569</a>]</span>. The phenomenal improvements on benchmark datasets are undeniable, but greatly reducing the error rate on these benchmarks did not fundamentally transform the domain. Widespread adoption of conversational speech technologies will require solving the problem, i.e. methods that surpass human performance, and persuading users to adopt them <span class="citation" data-cites="nyjAIan4">[<a href="#ref-nyjAIan4" role="doc-biblioref">567</a>]</span>. We see parallels in healthcare, where achieving the full potential of deep learning will require outstanding predictive performance as well as acceptance and adoption by biologists and clinicians. These experts will rightfully demand rigorous evidence that deep learning has impacted their respective disciplines—elucidated new biological mechanisms and improved patient outcomes—to be convinced that the promises of deep learning are more substantive than those of previous generations of artificial intelligence.</p>
<p>Some of the areas we have discussed are closer to surpassing this lofty bar than others, generally those that are more similar to the non-biomedical tasks that are now monopolized by deep learning. In medical imaging, diabetic retinopathy <span class="citation" data-cites="1mJW6umJ">[<a href="#ref-1mJW6umJ" role="doc-biblioref">50</a>]</span>, diabetic macular edema <span class="citation" data-cites="1mJW6umJ">[<a href="#ref-1mJW6umJ" role="doc-biblioref">50</a>]</span>, tuberculosis <span class="citation" data-cites="Qve94Jra">[<a href="#ref-Qve94Jra" role="doc-biblioref">59</a>]</span>, and skin lesion <span class="citation" data-cites="XnYNYoYB">[<a href="#ref-XnYNYoYB" role="doc-biblioref">5</a>]</span> classifiers are highly accurate and comparable to clinician performance.</p>
<p>In other domains, perfect accuracy will not be required because deep learning will primarily prioritize experiments and assist discovery. For example, in chemical screening for drug discovery, a deep learning system that successfully identifies dozens or hundreds of target-specific, active small molecules from a massive search space would have immense practical value even if its overall precision is modest. In medical imaging, deep learning can point an expert to the most challenging cases that require manual review <span class="citation" data-cites="Qve94Jra">[<a href="#ref-Qve94Jra" role="doc-biblioref">59</a>]</span>, though the risk of false negatives must be addressed. In protein structure prediction, errors in individual residue-residue contacts can be tolerated when using the contacts jointly for 3D structure modeling. Improved contact map predictions <span class="citation" data-cites="BhfjKSY3">[<a href="#ref-BhfjKSY3" role="doc-biblioref">29</a>]</span> have led to notable improvements in fold and 3D structure prediction for some of the most challenging proteins, such as membrane proteins <span class="citation" data-cites="39RPiE10">[<a href="#ref-39RPiE10" role="doc-biblioref">276</a>]</span>.</p>
<p>Conversely, the most challenging tasks may be those in which predictions are used directly for downstream modeling or decision-making, especially in the clinic. As an example, errors in sequence variant calling will be amplified if they are used directly for GWAS. In addition, the stochasticity and complexity of biological systems implies that for some problems, for instance predicting gene regulation in disease, perfect accuracy will be unattainable.</p>
<p>We are witnessing deep learning models achieving human-level performance across a number of biomedical domains. However, machine learning algorithms, including deep neural networks, are also prone to mistakes that humans are much less likely to make, such as misclassification of adversarial examples <span class="citation" data-cites="1Fel6Bdb8 UtcyntjF">[<a href="#ref-1Fel6Bdb8" role="doc-biblioref">570</a>,<a href="#ref-UtcyntjF" role="doc-biblioref">571</a>]</span>, a reminder that these algorithms do not understand the semantics of the objects presented. It may be impossible to guarantee that a model is not susceptible to adversarial examples, but work in this area is continuing <span class="citation" data-cites="AsLAb71x 18lZK7fxH">[<a href="#ref-AsLAb71x" role="doc-biblioref">572</a>,<a href="#ref-18lZK7fxH" role="doc-biblioref">573</a>]</span>. Cooperation between human experts and deep learning algorithms addresses many of these challenges and can achieve better performance than either individually <span class="citation" data-cites="mbEp6jNr">[<a href="#ref-mbEp6jNr" role="doc-biblioref">65</a>]</span>. For sample and patient classification tasks, we expect deep learning methods to augment clinicians and biomedical researchers.</p>
<p>We are optimistic about the future of deep learning in biology and medicine. It is by no means inevitable that deep learning will revolutionize these domains, but given how rapidly the field is evolving, we are confident that its full potential in biomedicine has not been explored. We have highlighted numerous challenges beyond improving training and predictive accuracy, such as preserving patient privacy and interpreting models. Ongoing research has begun to address these problems and shown that they are not insurmountable. Deep learning offers the flexibility to model data in its most natural form, for example, longer DNA sequences instead of k-mers for transcription factor binding prediction and molecular graphs instead of pre-computed bit vectors for drug discovery. These flexible input feature representations have spurred creative modeling approaches that would be infeasible with other machine learning techniques. Unsupervised methods are currently less-developed than their supervised counterparts, but they may have the most potential because of how expensive and time-consuming it is to label large amounts of biomedical data. If future deep learning algorithms can summarize very large collections of input data into interpretable models that spur scientists to ask questions that they did not know how to ask, it will be clear that deep learning has transformed biology and medicine.</p>
<h2 id="methods">Methods</h2>
<h3 id="continuous-collaborative-manuscript-drafting">Continuous collaborative manuscript drafting</h3>
<p>We recognized that deep learning in precision medicine is a rapidly developing area. Hence, diverse expertise was required to provide a forward-looking perspective. Accordingly, we collaboratively wrote this review in the open, enabling anyone with expertise to contribute. We wrote the manuscript in markdown and tracked changes using git. Contributions were handled through GitHub, with individuals submitting “pull requests” to suggest additions to the manuscript.</p>
<p>To facilitate citation, we <a href="https://github.com/greenelab/deep-review/blob/master/USAGE.md#citations">defined</a> a markdown citation syntax. We supported citations to the following identifier types (in order of preference): DOIs, PubMed Central IDs, PubMed IDs, arXiv IDs, and URLs. References were automatically generated from citation metadata by querying APIs to generate <a href="http://citationstyles.org/">Citation Style Language</a> (CSL) JSON items for each reference. <a href="http://pandoc.org/">Pandoc</a> and <a href="https://github.com/jgm/pandoc-citeproc">pandoc-citeproc</a> converted the markdown to HTML and PDF, while rendering the formatted citations and references. In total, referenced works consisted of 391 DOIs, 6 PubMed Central records, 132 arXiv manuscripts, and 48 URLs (webpages as well as manuscripts lacking standardized identifiers).</p>
<p>We implemented continuous analysis so the manuscript was automatically regenerated whenever the source changed <span class="citation" data-cites="Qh7xTLwz">[<a href="#ref-Qh7xTLwz" role="doc-biblioref">150</a>]</span>. We configured Travis CI—a continuous integration service—to fetch new citation metadata and rebuild the manuscript for every commit. Accordingly, formatting or citation errors in pull requests would cause the Travis CI build to fail, automating quality control. In addition, the build process renders templated variables, such as the reference counts mentioned above, to automate the updating of dynamic content. When contributions were merged into the <code>master</code> branch, Travis CI deployed the built manuscript by committing back to the GitHub repository. As a result, the latest manuscript version is always available at https://greenelab.github.io/deep-review. To ensure a consistent software environment, we defined a versioned <a href="https://conda.io">conda</a> environment of the software dependencies.</p>
<p>In addition, we instructed the Travis CI deployment script to perform blockchain timestamping <span class="citation" data-cites="6MR50hyY QBWMEuxW">[<a href="#ref-6MR50hyY" role="doc-biblioref">574</a>,<a href="#ref-QBWMEuxW" role="doc-biblioref">575</a>]</span>. Using <a href="https://opentimestamps.org/">OpenTimestamps</a>, we submitted hashes for the manuscript and the source git commit for timestamping in the Bitcoin blockchain <span class="citation" data-cites="igA8jklc">[<a href="#ref-igA8jklc" role="doc-biblioref">576</a>]</span>. These timestamps attest that a given version of this manuscript (and its history) existed at a given point in time. The ability to irrefutably prove manuscript existence at a past time could be important to establish scientific precedence and enforce an immutable record of authorship.</p>
<h3 id="author-contributions">Author contributions</h3>
<p>We created an open repository on the GitHub version control platform (<a href="https://github.com/greenelab/deep-review"><code>greenelab/deep-review</code></a>) <span class="citation" data-cites="yLr4pV4G">[<a href="#ref-yLr4pV4G" role="doc-biblioref">577</a>]</span>. Here, we engaged with numerous authors from papers within and outside of the area. The manuscript was drafted via GitHub commits by 36 individuals who met the ICMJE standards of authorship. These were individuals who contributed to the review of the literature; drafted the manuscript or provided substantial critical revisions; approved the final manuscript draft; and agreed to be accountable in all aspects of the work. Individuals who did not contribute in all of these ways, but who did participate, are acknowledged below. We grouped authors into the following four classes of approximately equal contributions and randomly ordered authors within each contribution class. Drafted multiple sub-sections along with extensive editing, pull request reviews, or discussion: A.A.K., B.K.B., B.T.D., D.S.H., E.F., G.P.W., M.M.H., M.Z., P.A., T.C. Drafted one or more sub-sections: A.E.C., A.M.A., A.S., B.J.L., C.A.L., E.M.C., G.L.R., J.I., J.L., J.X., S.C.T., S.W., W.X., Z.L. Revised specific sub-sections or supervised drafting one or more sub-sections: A.H., A.K., D.D., D.J.H., L.K.W., M.H.S.S., S.J.S., S.M.B., Y.P., Y.Q. Drafted sub-sections, edited the manuscript, reviewed pull requests, and coordinated co-authors: A.G., C.S.G.</p>
<h3 id="competing-interests">Competing interests</h3>
<p>A.K. is on the Advisory Board of Deep Genomics Inc. E.F. is a full-time employee of GlaxoSmithKline. The remaining authors have no competing interests to declare.</p>
<h3 id="acknowledgements">Acknowledgements</h3>
<p>We gratefully acknowledge Christof Angermueller, Kumardeep Chaudhary, Gökcen Eraslan, Mikael Huss, Bharath Ramsundar and Xun Zhu for their discussion of the manuscript and reviewed papers on GitHub. We would like to thank Aaron Sheldon, who contributed text but did not formally approve the manuscript. We would like to thank Anna Greene for a careful proofreading of the manuscript in advance of the first submission. We would like to thank Sebastian Raschka for clarifying edits to the abstract and introduction. We would like to thank Robert Gieseke, Ruibang Luo, Stephen Ra, Sourav Singh, and GitHub user snikumbh for correcting typos, formatting, and references.</p>
<h3 id="funding-statement">Funding statement</h3>
<p>We acknowledge funding from the Gordon and Betty Moore Foundation awards GBMF4552 (C.S.G. and D.S.H.) and GBMF4563 (D.J.H.); the Howard Hughes Medical Institute (S.C.T.); the National Institutes of Health awards DP2GM123485 (A.K.), P30CA051008 (S.M.B.), R01AI116794 (B.K.B.), R01GM089652 (A.E.C.), R01GM089753 (J.X.), R01LM012222 (S.J.S.), R01LM012482 (S.J.S.), R21CA220398 (S.M.B.), T32GM007753 (B.T.D.), T32HG000046 (G.P.W.), and U54AI117924 (A.G.); the National Institutes of Health Intramural Research Program and National Library of Medicine (Y.P. and Z.L.); the National Science Foundation awards 1245632 (G.L.R.), 1531594 (E.M.C.), and 1564955 (J.X.); the Natural Sciences and Engineering Research Council of Canada award RGPIN-2015-3948 (M.M.H.); and the Roy and Diana Vagelos Scholars Program in the Molecular Life Sciences (M.Z.).</p>
<h2 id="references" class="page_break_before">References</h2>
<!-- Explicitly insert bibliography here -->
<div id="refs" role="doc-bibliography">
<div id="ref-13bxiY1vo">
<p>1. <strong>Big Data: Astronomical or Genomical?</strong><br />
Zachary D. Stephens, Skylar Y. Lee, Faraz Faghri, Roy H. Campbell, Chengxiang Zhai, Miles J. Efron, Ravishankar Iyer, Michael C. Schatz, Saurabh Sinha, Gene E. Robinson<br />
<em>PLOS Biology</em> (2015-07-07) <a href="https://doi.org/bdzf">https://doi.org/bdzf</a><br />
DOI: <a href="https://doi.org/10.1371/journal.pbio.1002195">10.1371/journal.pbio.1002195</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26151137">26151137</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4494865">PMC4494865</a></p>
</div>
<div id="ref-BeijBSRE">
<p>2. <strong>Deep learning</strong><br />
Yann LeCun, Yoshua Bengio, Geoffrey Hinton<br />
<em>Nature</em> (2015-05) <a href="https://doi.org/bmqp">https://doi.org/bmqp</a><br />
DOI: <a href="https://doi.org/10.1038/nature14539">10.1038/nature14539</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26017442">26017442</a></p>
</div>
<div id="ref-TDruxF1s">
<p>3. <strong>Searching for exotic particles in high-energy physics with deep learning</strong><br />
P. Baldi, P. Sadowski, D. Whiteson<br />
<em>Nature Communications</em> (2014-07-02) <a href="https://doi.org/f6c8jp">https://doi.org/f6c8jp</a><br />
DOI: <a href="https://doi.org/10.1038/ncomms5308">10.1038/ncomms5308</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/24986233">24986233</a></p>
</div>
<div id="ref-zCt6PUXj">
<p>4. <strong>Deep learning for computational chemistry</strong><br />
Garrett B. Goh, Nathan O. Hodas, Abhinav Vishnu<br />
<em>Journal of Computational Chemistry</em> (2017-03-08) <a href="https://doi.org/f959s6">https://doi.org/f959s6</a><br />
DOI: <a href="https://doi.org/10.1002/jcc.24764">10.1002/jcc.24764</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28272810">28272810</a></p>
</div>
<div id="ref-XnYNYoYB">
<p>5. <strong>Dermatologist-level classification of skin cancer with deep neural networks</strong><br />
Andre Esteva, Brett Kuprel, Roberto A. Novoa, Justin Ko, Susan M. Swetter, Helen M. Blau, Sebastian Thrun<br />
<em>Nature</em> (2017-01-25) <a href="https://doi.org/bxwn">https://doi.org/bxwn</a><br />
DOI: <a href="https://doi.org/10.1038/nature21056">10.1038/nature21056</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28117445">28117445</a></p>
</div>
<div id="ref-4TK06zOf">
<p>6. <strong>Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</strong><br />
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, … Jeffrey Dean<br />
<em>arXiv</em> (2016-09-26) <a href="https://arxiv.org/abs/1609.08144v2">https://arxiv.org/abs/1609.08144v2</a></p>
</div>
<div id="ref-1HVDhhwpK">
<p>7. <strong>A logical calculus of the ideas immanent in nervous activity</strong><br />
Warren S. McCulloch, Walter Pitts<br />
<em>The Bulletin of Mathematical Biophysics</em> (1943-12) <a href="https://doi.org/djsbj6">https://doi.org/djsbj6</a><br />
DOI: <a href="https://doi.org/10.1007/bf02478259">10.1007/bf02478259</a></p>
</div>
<div id="ref-1G5eCiq4d">
<p>8. <strong>Analysis of a Four-Layer Series-Coupled Perceptron. II</strong><br />
H. D. Block, B. W. Knight, F. Rosenblatt<br />
<em>Reviews of Modern Physics</em> (1962-01-01) <a href="https://doi.org/fhx4tr">https://doi.org/fhx4tr</a><br />
DOI: <a href="https://doi.org/10.1103/revmodphys.34.135">10.1103/revmodphys.34.135</a></p>
</div>
<div id="ref-IiNJE32f">
<p>9. <strong>Building high-level features using large scale unsupervised learning</strong><br />
Quoc Le, Marc’Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg Corrado, Jeff Dean, Andrew Ng<br />
<em>International Conference in Machine Learning</em> (2012)</p>
</div>
<div id="ref-3qm8sXnB">
<p>10. <strong>HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent</strong><br />
Feng Niu, Benjamin Recht, Christopher Re, Stephen J. Wright<br />
<em>arXiv</em> (2011-06-28) <a href="https://arxiv.org/abs/1106.5730v2">https://arxiv.org/abs/1106.5730v2</a></p>
</div>
<div id="ref-yg8NW0K7">
<p>11. <strong>Deep Learning</strong><br />
Ian Goodfellow, Yoshua Bengio, Aaron Courville<br />
(2016) <a href="http://www.deeplearningbook.org/">http://www.deeplearningbook.org/</a></p>
</div>
<div id="ref-mAXsmd43">
<p>12. <strong>Academy of Management: Andrew S. Grove</strong><br />
Andrew S. Grove<br />
(1998-08-09) <a href="http://www.intel.com/pressroom/archive/speeches/ag080998.htm">http://www.intel.com/pressroom/archive/speeches/ag080998.htm</a></p>
</div>
<div id="ref-yXqhuueV">
<p>13. <strong>Deep learning for regulatory genomics</strong><br />
Yongjin Park, Manolis Kellis<br />
<em>Nature Biotechnology</em> (2015-08) <a href="https://doi.org/gcgk8b">https://doi.org/gcgk8b</a><br />
DOI: <a href="https://doi.org/10.1038/nbt.3313">10.1038/nbt.3313</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26252139">26252139</a></p>
</div>
<div id="ref-1VZjheOA">
<p>14. <strong>Applications of Deep Learning in Biomedicine</strong><br />
Polina Mamoshina, Armando Vieira, Evgeny Putin, Alex Zhavoronkov<br />
<em>Molecular Pharmaceutics</em> (2016-03-29) <a href="https://doi.org/f8mvtj">https://doi.org/f8mvtj</a><br />
DOI: <a href="https://doi.org/10.1021/acs.molpharmaceut.5b00982">10.1021/acs.molpharmaceut.5b00982</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27007977">27007977</a></p>
</div>
<div id="ref-irSe12Sm">
<p>15. <strong>Deep learning for computational biology</strong><br />
Christof Angermueller, Tanel Pärnamaa, Leopold Parts, Oliver Stegle<br />
<em>Molecular Systems Biology</em> (2016-07) <a href="https://doi.org/f8xtvh">https://doi.org/f8xtvh</a><br />
DOI: <a href="https://doi.org/10.15252/msb.20156651">10.15252/msb.20156651</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27474269">27474269</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4965871">PMC4965871</a></p>
</div>
<div id="ref-G00xvi94">
<p>16. <strong>Deep learning in bioinformatics</strong><br />
Seonwoo Min, Byunghan Lee, Sungroh Yoon<br />
<em>Briefings in Bioinformatics</em> (2016-07-29) <a href="https://doi.org/gcgk8v">https://doi.org/gcgk8v</a><br />
DOI: <a href="https://doi.org/10.1093/bib/bbw068">10.1093/bib/bbw068</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27473064">27473064</a></p>
</div>
<div id="ref-MmRGFVUu">
<p>17. <strong>Computer vision for high content screening</strong><br />
Oren Z. Kraus, Brendan J. Frey<br />
<em>Critical Reviews in Biochemistry and Molecular Biology</em> (2016-01-24) <a href="https://doi.org/gcgmc2">https://doi.org/gcgmc2</a><br />
DOI: <a href="https://doi.org/10.3109/10409238.2015.1135868">10.3109/10409238.2015.1135868</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26806341">26806341</a></p>
</div>
<div id="ref-11I7bLcP3">
<p>18. <strong>Deep learning for healthcare: review, opportunities and challenges</strong><br />
Riccardo Miotto, Fei Wang, Shuang Wang, Xiaoqian Jiang, Joel T Dudley<br />
<em>Briefings in Bioinformatics</em> (2017-05-06) <a href="https://doi.org/gcgk8x">https://doi.org/gcgk8x</a><br />
DOI: <a href="https://doi.org/10.1093/bib/bbx044">10.1093/bib/bbx044</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28481991">28481991</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6455466">PMC6455466</a></p>
</div>
<div id="ref-1FkYUUryG">
<p>19. <strong>A survey on deep learning in medical image analysis</strong><br />
Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud Arindra Adiyoso Setio, Francesco Ciompi, Mohsen Ghafoorian, Jeroen A. W.M. van der Laak, Bram van Ginneken, Clara I. Sánchez<br />
<em>Medical Image Analysis</em> (2017-12) <a href="https://doi.org/gcm56p">https://doi.org/gcm56p</a><br />
DOI: <a href="https://doi.org/10.1016/j.media.2017.07.005">10.1016/j.media.2017.07.005</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28778026">28778026</a></p>
</div>
<div id="ref-brPjEjYw">
<p>20. <strong>Deep Learning in Pharmacogenomics: From Gene Regulation to Patient Stratification</strong><br />
Alexandr A. Kalinin, Gerald A. Higgins, Narathip Reamaroon, S. M. Reza Soroushmehr, Ari Allyn-Feuer, Ivo D. Dinov, Kayvan Najarian, Brian D. Athey<br />
<em>arXiv</em> (2018-01-25) <a href="https://arxiv.org/abs/1801.08570v2">https://arxiv.org/abs/1801.08570v2</a><br />
DOI: <a href="https://doi.org/10.2217/pgs-2018-0008">10.2217/pgs-2018-0008</a></p>
</div>
<div id="ref-gJE0ExFr">
<p>21. <strong>Deep Learning in Drug Discovery</strong><br />
Erik Gawehn, Jan A. Hiss, Gisbert Schneider<br />
<em>Molecular Informatics</em> (2015-12-30) <a href="https://doi.org/f3kg55">https://doi.org/f3kg55</a><br />
DOI: <a href="https://doi.org/10.1002/minf.201501008">10.1002/minf.201501008</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27491648">27491648</a></p>
</div>
<div id="ref-1DTUK3YyI">
<p>22. <strong>Virtual Screening: A Challenge for Deep Learning</strong><br />
Javier Pérez-Sianes, Horacio Pérez-Sánchez, Fernando Díaz<br />
<em>Advances in Intelligent Systems and Computing</em> (2016) <a href="https://doi.org/gcgk7k">https://doi.org/gcgk7k</a><br />
DOI: <a href="https://doi.org/10.1007/978-3-319-40126-3_2">10.1007/978-3-319-40126-3_2</a></p>
</div>
<div id="ref-xPkT1z7D">
<p>23. <strong>A renaissance of neural networks in drug discovery</strong><br />
Igor I. Baskin, David Winkler, Igor V. Tetko<br />
<em>Expert Opinion on Drug Discovery</em> (2016-07-04) <a href="https://doi.org/gcgk8s">https://doi.org/gcgk8s</a><br />
DOI: <a href="https://doi.org/10.1080/17460441.2016.1201262">10.1080/17460441.2016.1201262</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27295548">27295548</a></p>
</div>
<div id="ref-lnK82Ey6">
<p>24. <strong>Supervised Risk Predictor of Breast Cancer Based on Intrinsic Subtypes</strong><br />
Joel S. Parker, Michael Mullins, Maggie C. U. Cheang, Samuel Leung, David Voduc, Tammi Vickery, Sherri Davies, Christiane Fauron, Xiaping He, Zhiyuan Hu, … Philip S. Bernard<br />
<em>Journal of Clinical Oncology</em> (2009-03-10) <a href="https://doi.org/c2688w">https://doi.org/c2688w</a><br />
DOI: <a href="https://doi.org/10.1200/jco.2008.18.1370">10.1200/jco.2008.18.1370</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/19204204">19204204</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2667820">PMC2667820</a></p>
</div>
<div id="ref-pEIw87Mp">
<p>25. <strong>New Strategies for Triple-Negative Breast Cancer–Deciphering the Heterogeneity</strong><br />
I. A. Mayer, V. G. Abramson, B. D. Lehmann, J. A. Pietenpol<br />
<em>Clinical Cancer Research</em> (2014-02-15) <a href="https://doi.org/f5tgp5">https://doi.org/f5tgp5</a><br />
DOI: <a href="https://doi.org/10.1158/1078-0432.ccr-13-0583">10.1158/1078-0432.ccr-13-0583</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/24536073">24536073</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3962777">PMC3962777</a></p>
</div>
<div id="ref-PBiRSdXv">
<p>26. <strong>UNSUPERVISED FEATURE CONSTRUCTION AND KNOWLEDGE EXTRACTION FROM GENOME-WIDE ASSAYS OF BREAST CANCER WITH DENOISING AUTOENCODERS</strong><br />
JIE TAN, MATTHEW UNG, CHAO CHENG, CASEY S GREENE<br />
<em>Biocomputing 2015</em> (2014-11) <a href="https://doi.org/gcgmbs">https://doi.org/gcgmbs</a><br />
DOI: <a href="https://doi.org/10.1142/9789814644730_0014">10.1142/9789814644730_0014</a></p>
</div>
<div id="ref-koEdZRcY">
<p>27. <strong>Mitosis Detection in Breast Cancer Histology Images with Deep Neural Networks</strong><br />
Dan C. Cireşan, Alessandro Giusti, Luca M. Gambardella, Jürgen Schmidhuber<br />
<em>Medical Image Computing and Computer-Assisted Intervention – MICCAI 2013</em> (2013) <a href="https://doi.org/gcgk7t">https://doi.org/gcgk7t</a><br />
DOI: <a href="https://doi.org/10.1007/978-3-642-40763-5_51">10.1007/978-3-642-40763-5_51</a></p>
</div>
<div id="ref-YUms527e">
<p>28. <strong>End effector target position learning using feedforward with error back-propagation and recurrent neural networks</strong><br />
J. Zurada<br />
<em>Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN’94)</em> (1994) <a href="https://doi.org/10.1109/icnn.1994.374637">https://doi.org/10.1109/icnn.1994.374637</a><br />
DOI: <a href="https://doi.org/10.1109/icnn.1994.374637">10.1109/icnn.1994.374637</a></p>
</div>
<div id="ref-BhfjKSY3">
<p>29. <strong>Accurate De Novo Prediction of Protein Contact Map by Ultra-Deep Learning Model</strong><br />
Sheng Wang, Siqi Sun, Zhen Li, Renyu Zhang, Jinbo Xu<br />
<em>PLOS Computational Biology</em> (2017-01-05) <a href="https://doi.org/f9ktjn">https://doi.org/f9ktjn</a><br />
DOI: <a href="https://doi.org/10.1371/journal.pcbi.1005324">10.1371/journal.pcbi.1005324</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28056090">28056090</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5249242">PMC5249242</a></p>
</div>
<div id="ref-ZzaRyGuJ">
<p>30. <strong>A Deep Learning Network Approach to <italic>ab initio</italic> Protein Secondary Structure Prediction</strong><br />
Matt Spencer, Jesse Eickholt, Jianlin Cheng<br />
<em>IEEE/ACM Transactions on Computational Biology and Bioinformatics</em> (2015-01-01) <a href="https://doi.org/gcgmbc">https://doi.org/gcgmbc</a><br />
DOI: <a href="https://doi.org/10.1109/tcbb.2014.2343960">10.1109/tcbb.2014.2343960</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/25750595">25750595</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4348072">PMC4348072</a></p>
</div>
<div id="ref-UO8L6nd">
<p>31. <strong>Protein Secondary Structure Prediction Using Deep Convolutional Neural Fields</strong><br />
Sheng Wang, Jian Peng, Jianzhu Ma, Jinbo Xu<br />
<em>Scientific Reports</em> (2016-01-11) <a href="https://doi.org/f76r5q">https://doi.org/f76r5q</a><br />
DOI: <a href="https://doi.org/10.1038/srep18962">10.1038/srep18962</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26752681">26752681</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4707437">PMC4707437</a></p>
</div>
<div id="ref-s5sy4AOi">
<p>32. <strong>PEDLA: predicting enhancers with a deep learning-based algorithmic framework</strong><br />
Feng Liu, Hao Li, Chao Ren, Xiaochen Bo, Wenjie Shu<br />
<em>Cold Spring Harbor Laboratory</em> (2016-01-07) <a href="https://doi.org/gcgk85">https://doi.org/gcgk85</a><br />
DOI: <a href="https://doi.org/10.1101/036129">10.1101/036129</a></p>
</div>
<div id="ref-17B2QAA1k">
<p>33. <strong>Deep Feature Selection: Theory and Application to Identify Enhancers and Promoters</strong><br />
Yifeng Li, Chih-Yu Chen, Wyeth W. Wasserman<br />
<em>Lecture Notes in Computer Science</em> (2015) <a href="https://doi.org/gcgk7g">https://doi.org/gcgk7g</a><br />
DOI: <a href="https://doi.org/10.1007/978-3-319-16706-0_20">10.1007/978-3-319-16706-0_20</a></p>
</div>
<div id="ref-12aqvAgz6">
<p>34. <strong>DEEP: a general computational framework for predicting enhancers</strong><br />
Dimitrios Kleftogiannis, Panos Kalnis, Vladimir B. Bajic<br />
<em>Nucleic Acids Research</em> (2014-11-05) <a href="https://doi.org/gcgk83">https://doi.org/gcgk83</a><br />
DOI: <a href="https://doi.org/10.1093/nar/gku1058">10.1093/nar/gku1058</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/25378307">25378307</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4288148">PMC4288148</a></p>
</div>
<div id="ref-15E5yG1Ho">
<p>35. <strong>DANN: a deep learning approach for annotating the pathogenicity of genetic variants</strong><br />
Daniel Quang, Yifei Chen, Xiaohui Xie<br />
<em>Bioinformatics</em> (2014-10-22) <a href="https://doi.org/f67rhm">https://doi.org/f67rhm</a><br />
DOI: <a href="https://doi.org/10.1093/bioinformatics/btu703">10.1093/bioinformatics/btu703</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/25338716">25338716</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4341060">PMC4341060</a></p>
</div>
<div id="ref-Z7fd0BYf">
<p>36. <strong>AtomNet: A Deep Convolutional Neural Network for Bioactivity Prediction in Structure-based Drug Discovery</strong><br />
Izhar Wallach, Michael Dzamba, Abraham Heifets<br />
<em>arXiv</em> (2015-10-10) <a href="https://arxiv.org/abs/1510.02855v1">https://arxiv.org/abs/1510.02855v1</a></p>
</div>
<div id="ref-EMDwvRGb">
<p>37. <strong>Deep Learning Applications for Predicting Pharmacological Properties of Drugs and Drug Repurposing Using Transcriptomic Data</strong><br />
Alexander Aliper, Sergey Plis, Artem Artemov, Alvaro Ulloa, Polina Mamoshina, Alex Zhavoronkov<br />
<em>Molecular Pharmaceutics</em> (2016-06-08) <a href="https://doi.org/gcgk77">https://doi.org/gcgk77</a><br />
DOI: <a href="https://doi.org/10.1021/acs.molpharmaceut.6b00248">10.1021/acs.molpharmaceut.6b00248</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27200455">27200455</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4965264">PMC4965264</a></p>
</div>
<div id="ref-1AU7wzPqa">
<p>38. <strong>Predicting drug-target interactions using restricted Boltzmann machines</strong><br />
Yuhao Wang, Jianyang Zeng<br />
<em>Bioinformatics</em> (2013-06-19) <a href="https://doi.org/gbddzn">https://doi.org/gbddzn</a><br />
DOI: <a href="https://doi.org/10.1093/bioinformatics/btt234">10.1093/bioinformatics/btt234</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/23812976">23812976</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3694663">PMC3694663</a></p>
</div>
<div id="ref-oTF8O79C">
<p>39. <strong>Deep-Learning-Based Drug–Target Interaction Prediction</strong><br />
Ming Wen, Zhimin Zhang, Shaoyu Niu, Haozhi Sha, Ruihan Yang, Yonghuan Yun, Hongmei Lu<br />
<em>Journal of Proteome Research</em> (2017-03-13) <a href="https://doi.org/f9ttp4">https://doi.org/f9ttp4</a><br />
DOI: <a href="https://doi.org/10.1021/acs.jproteome.6b00618">10.1021/acs.jproteome.6b00618</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28264154">28264154</a></p>
</div>
<div id="ref-yEstnIOT">
<p>40. <strong>Deep Learning in Medical Image Analysis</strong><br />
Dinggang Shen, Guorong Wu, Heung-Il Suk<br />
<em>Annual Review of Biomedical Engineering</em> (2017-06-21) <a href="https://doi.org/gcgmb4">https://doi.org/gcgmb4</a><br />
DOI: <a href="https://doi.org/10.1146/annurev-bioeng-071516-044442">10.1146/annurev-bioeng-071516-044442</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28301734">28301734</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5479722">PMC5479722</a></p>
</div>
<div id="ref-VFw1VXDP">
<p>41. <strong>Deep Learning and Structured Prediction for the Segmentation of Mass in Mammograms</strong><br />
Neeraj Dhungel, Gustavo Carneiro, Andrew P. Bradley<br />
<em>Lecture Notes in Computer Science</em> (2015) <a href="https://doi.org/gcgk7h">https://doi.org/gcgk7h</a><br />
DOI: <a href="https://doi.org/10.1007/978-3-319-24553-9_74">10.1007/978-3-319-24553-9_74</a></p>
</div>
<div id="ref-JK8NuXy3">
<p>42. <strong>The Automated Learning of Deep Features for Breast Mass Classification from Mammograms</strong><br />
Neeraj Dhungel, Gustavo Carneiro, Andrew P. Bradley<br />
<em>Medical Image Computing and Computer-Assisted Intervention – MICCAI 2016</em> (2016) <a href="https://doi.org/gcgk7q">https://doi.org/gcgk7q</a><br />
DOI: <a href="https://doi.org/10.1007/978-3-319-46723-8_13">10.1007/978-3-319-46723-8_13</a></p>
</div>
<div id="ref-9G9Hv1Pp">
<p>43. <strong>Deep Multi-instance Networks with Sparse Label Assignment for Whole Mammogram Classification</strong><br />
Wentao Zhu, Qi Lou, Yeeleng Scott Vang, Xiaohui Xie<br />
<em>Cold Spring Harbor Laboratory</em> (2016-12-20) <a href="https://doi.org/gcgk9r">https://doi.org/gcgk9r</a><br />
DOI: <a href="https://doi.org/10.1101/095794">10.1101/095794</a></p>
</div>
<div id="ref-Xxb4t3zO">
<p>44. <strong>Adversarial Deep Structural Networks for Mammographic Mass Segmentation</strong><br />
Wentao Zhu, Xiaohui Xie<br />
<em>Cold Spring Harbor Laboratory</em> (2016-12-20) <a href="https://doi.org/gcgk9q">https://doi.org/gcgk9q</a><br />
DOI: <a href="https://doi.org/10.1101/095786">10.1101/095786</a></p>
</div>
<div id="ref-5kfDbGhA">
<p>45. <strong>A deep learning approach for the analysis of masses in mammograms with minimal user intervention</strong><br />
Neeraj Dhungel, Gustavo Carneiro, Andrew P. Bradley<br />
<em>Medical Image Analysis</em> (2017-04) <a href="https://doi.org/f932tm">https://doi.org/f932tm</a><br />
DOI: <a href="https://doi.org/10.1016/j.media.2017.01.009">10.1016/j.media.2017.01.009</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28171807">28171807</a></p>
</div>
<div id="ref-cBVeXnZx">
<p>46. <strong>ImageNet Large Scale Visual Recognition Challenge</strong><br />
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, … Li Fei-Fei<br />
<em>International Journal of Computer Vision</em> (2015-04-11) <a href="https://doi.org/gcgk7w">https://doi.org/gcgk7w</a><br />
DOI: <a href="https://doi.org/10.1007/s11263-015-0816-y">10.1007/s11263-015-0816-y</a></p>
</div>
<div id="ref-ayTsooEM">
<p>47. <strong>Convolutional Neural Networks for Diabetic Retinopathy</strong><br />
Harry Pratt, Frans Coenen, Deborah M. Broadbent, Simon P. Harding, Yalin Zheng<br />
<em>Procedia Computer Science</em> (2016) <a href="https://doi.org/gcgk75">https://doi.org/gcgk75</a><br />
DOI: <a href="https://doi.org/10.1016/j.procs.2016.07.014">10.1016/j.procs.2016.07.014</a></p>
</div>
<div id="ref-e3vyHBV2">
<p>48. <strong>Improved Automated Detection of Diabetic Retinopathy on a Publicly Available Dataset Through Integration of Deep Learning</strong><br />
Michael David Abràmoff, Yiyue Lou, Ali Erginay, Warren Clarida, Ryan Amelon, James C. Folk, Meindert Niemeijer<br />
<em>Investigative Opthalmology &amp; Visual Science</em> (2016-10-04) <a href="https://doi.org/f9mr6r">https://doi.org/f9mr6r</a><br />
DOI: <a href="https://doi.org/10.1167/iovs.16-19964">10.1167/iovs.16-19964</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27701631">27701631</a></p>
</div>
<div id="ref-14Ovc5nPg">
<p>49. <strong>Leveraging uncertainty information from deep neural networks for disease detection</strong><br />
Christian Leibig, Vaneeda Allken, Murat Seckin Ayhan, Philipp Berens, Siegfried Wahl<br />
<em>Cold Spring Harbor Laboratory</em> (2016-10-28) <a href="https://doi.org/gcgk9h">https://doi.org/gcgk9h</a><br />
DOI: <a href="https://doi.org/10.1101/084210">10.1101/084210</a></p>
</div>
<div id="ref-1mJW6umJ">
<p>50. <strong>Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs</strong><br />
Varun Gulshan, Lily Peng, Marc Coram, Martin C. Stumpe, Derek Wu, Arunachalam Narayanaswamy, Subhashini Venugopalan, Kasumi Widner, Tom Madams, Jorge Cuadros, … Dale R. Webster<br />
<em>JAMA</em> (2016-12-13) <a href="https://doi.org/gcgk7d">https://doi.org/gcgk7d</a><br />
DOI: <a href="https://doi.org/10.1001/jama.2016.17216">10.1001/jama.2016.17216</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27898976">27898976</a></p>
</div>
<div id="ref-sLPsrfbl">
<p>51. <strong>Deep Learning Ensembles for Melanoma Recognition in Dermoscopy Images</strong><br />
Noel Codella, Quoc-Bao Nguyen, Sharath Pankanti, David Gutman, Brian Helba, Allan Halpern, John R. Smith<br />
<em>arXiv</em> (2016-10-14) <a href="https://arxiv.org/abs/1610.04662v2">https://arxiv.org/abs/1610.04662v2</a></p>
</div>
<div id="ref-phRCihNB">
<p>52. <strong>Automated Melanoma Recognition in Dermoscopy Images via Very Deep Residual Networks</strong><br />
Lequan Yu, Hao Chen, Qi Dou, Jing Qin, Pheng-Ann Heng<br />
<em>IEEE Transactions on Medical Imaging</em> (2017-04) <a href="https://doi.org/gcgmbh">https://doi.org/gcgmbh</a><br />
DOI: <a href="https://doi.org/10.1109/tmi.2016.2642839">10.1109/tmi.2016.2642839</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28026754">28026754</a></p>
</div>
<div id="ref-18f8olBNy">
<p>53. <strong>Extraction of skin lesions from non-dermoscopic images for surgical excision of melanoma</strong><br />
M. Hossein Jafari, Ebrahim Nasr-Esfahani, Nader Karimi, S. M. Reza Soroushmehr, Shadrokh Samavi, Kayvan Najarian<br />
<em>International Journal of Computer Assisted Radiology and Surgery</em> (2017-03-24) <a href="https://doi.org/gbhjfj">https://doi.org/gbhjfj</a><br />
DOI: <a href="https://doi.org/10.1007/s11548-017-1567-8">10.1007/s11548-017-1567-8</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28342106">28342106</a></p>
</div>
<div id="ref-O39LDkX">
<p>54. <strong>Melanoma detection by analysis of clinical images using convolutional neural network</strong><br />
E. Nasr-Esfahani, S. Samavi, N. Karimi, S. M.R. Soroushmehr, M. H. Jafari, K. Ward, K. Najarian<br />
<em>2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</em> (2016-08) <a href="https://doi.org/gcgk97">https://doi.org/gcgk97</a><br />
DOI: <a href="https://doi.org/10.1109/embc.2016.7590963">10.1109/embc.2016.7590963</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28268581">28268581</a></p>
</div>
<div id="ref-iBPOt78R">
<p>55. <strong>Detection of age-related macular degeneration via deep learning</strong><br />
P. Burlina, D. E. Freund, N. Joshi, Y. Wolfson, N. M. Bressler<br />
<em>2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)</em> (2016-04) <a href="https://doi.org/gcgmbb">https://doi.org/gcgmbb</a><br />
DOI: <a href="https://doi.org/10.1109/isbi.2016.7493240">10.1109/isbi.2016.7493240</a></p>
</div>
<div id="ref-1Fy5bcnCI">
<p>56. <strong>Deep learning with non-medical training used for chest pathology identification</strong><br />
Yaniv Bar, Idit Diamant, Lior Wolf, Hayit Greenspan<br />
<em>Medical Imaging 2015: Computer-Aided Diagnosis</em> (2015-03-20) <a href="https://doi.org/gcgmbm">https://doi.org/gcgmbm</a><br />
DOI: <a href="https://doi.org/10.1117/12.2083124">10.1117/12.2083124</a></p>
</div>
<div id="ref-1GAyqYBNZ">
<p>57. <strong>Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning</strong><br />
Hoo-Chang Shin, Holger R. Roth, Mingchen Gao, Le Lu, Ziyue Xu, Isabella Nogues, Jianhua Yao, Daniel Mollura, Ronald M. Summers<br />
<em>IEEE Transactions on Medical Imaging</em> (2016-05) <a href="https://doi.org/gcgmbg">https://doi.org/gcgmbg</a><br />
DOI: <a href="https://doi.org/10.1109/tmi.2016.2528162">10.1109/tmi.2016.2528162</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26886976">26886976</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4890616">PMC4890616</a></p>
</div>
<div id="ref-x6HXFAS4">
<p>58. <strong>High-Throughput Classification of Radiographs Using Deep Convolutional Neural Networks</strong><br />
Alvin Rajkomar, Sneha Lingam, Andrew G. Taylor, Michael Blum, John Mongan<br />
<em>Journal of Digital Imaging</em> (2016-10-11) <a href="https://doi.org/gcgk7v">https://doi.org/gcgk7v</a><br />
DOI: <a href="https://doi.org/10.1007/s10278-016-9914-9">10.1007/s10278-016-9914-9</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27730417">27730417</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5267603">PMC5267603</a></p>
</div>
<div id="ref-Qve94Jra">
<p>59. <strong>Deep Learning at Chest Radiography: Automated Classification of Pulmonary Tuberculosis by Using Convolutional Neural Networks</strong><br />
Paras Lakhani, Baskaran Sundaram<br />
<em>Radiology</em> (2017-08) <a href="https://doi.org/gbp274">https://doi.org/gbp274</a><br />
DOI: <a href="https://doi.org/10.1148/radiol.2017162326">10.1148/radiol.2017162326</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28436741">28436741</a></p>
</div>
<div id="ref-SOi9mAC2">
<p>60. <strong>Classification of breast MRI lesions using small-size training sets: comparison of deep learning approaches</strong><br />
Guy Amit, Rami Ben-Ari, Omer Hadad, Einat Monovich, Noa Granot, Sharbell Hashoul<br />
<em>Medical Imaging 2017: Computer-Aided Diagnosis</em> (2017-03-03) <a href="https://doi.org/gcgmbn">https://doi.org/gcgmbn</a><br />
DOI: <a href="https://doi.org/10.1117/12.2249981">10.1117/12.2249981</a></p>
</div>
<div id="ref-KseoWN2w">
<p>61. <strong>Improving Computer-Aided Detection Using_newlineConvolutional Neural Networks and Random View Aggregation</strong><br />
Holger R. Roth, Le Lu, Jiamin Liu, Jianhua Yao, Ari Seff, Kevin Cherry, Lauren Kim, Ronald M. Summers<br />
<em>IEEE Transactions on Medical Imaging</em> (2016-05) <a href="https://doi.org/gcgmbf">https://doi.org/gcgmbf</a><br />
DOI: <a href="https://doi.org/10.1109/tmi.2015.2482920">10.1109/tmi.2015.2482920</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26441412">26441412</a></p>
</div>
<div id="ref-18EpaZ7QB">
<p>62. <strong>3D Deep Learning for Multi-modal Imaging-Guided Survival Time Prediction of Brain Tumor Patients</strong><br />
Dong Nie, Han Zhang, Ehsan Adeli, Luyan Liu, Dinggang Shen<br />
<em>Medical Image Computing and Computer-Assisted Intervention – MICCAI 2016</em> (2016) <a href="https://doi.org/gcgk7r">https://doi.org/gcgk7r</a><br />
DOI: <a href="https://doi.org/10.1007/978-3-319-46723-8_25">10.1007/978-3-319-46723-8_25</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28149967">28149967</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5278791">PMC5278791</a></p>
</div>
<div id="ref-18cZbigDD">
<p>63. <strong>Large scale deep learning for computer aided detection of mammographic lesions</strong><br />
Thijs Kooi, Geert Litjens, Bram van Ginneken, Albert Gubern-Mérida, Clara I. Sánchez, Ritse Mann, Ard den Heeten, Nico Karssemeijer<br />
<em>Medical Image Analysis</em> (2017-01) <a href="https://doi.org/gcgk74">https://doi.org/gcgk74</a><br />
DOI: <a href="https://doi.org/10.1016/j.media.2016.07.007">10.1016/j.media.2016.07.007</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27497072">27497072</a></p>
</div>
<div id="ref-dQCjqq0Q">
<p>64. <strong>Deep learning as a tool for increased accuracy and efficiency of histopathological diagnosis</strong><br />
Geert Litjens, Clara I. Sánchez, Nadya Timofeeva, Meyke Hermsen, Iris Nagtegaal, Iringo Kovacs, Christina Hulsbergen - van de Kaa, Peter Bult, Bram van Ginneken, Jeroen van der Laak<br />
<em>Scientific Reports</em> (2016-05-23) <a href="https://doi.org/f8mqzq">https://doi.org/f8mqzq</a><br />
DOI: <a href="https://doi.org/10.1038/srep26286">10.1038/srep26286</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27212078">27212078</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4876324">PMC4876324</a></p>
</div>
<div id="ref-mbEp6jNr">
<p>65. <strong>Deep Learning for Identifying Metastatic Breast Cancer</strong><br />
Dayong Wang, Aditya Khosla, Rishab Gargeya, Humayun Irshad, Andrew H. Beck<br />
<em>arXiv</em> (2016-06-18) <a href="https://arxiv.org/abs/1606.05718v1">https://arxiv.org/abs/1606.05718v1</a></p>
</div>
<div id="ref-4L1QgpXP">
<p>66. <strong>Deep Convolutional Neural Networks for Breast Cancer Histology Image Analysis</strong><br />
Alexander Rakhlin, Alexey Shvets, Vladimir Iglovikov, Alexandr Kalinin<br />
<em>Cold Spring Harbor Laboratory</em> (2018-02-05) <a href="https://doi.org/gc3cfc">https://doi.org/gc3cfc</a><br />
DOI: <a href="https://doi.org/10.1101/259911">10.1101/259911</a></p>
</div>
<div id="ref-SxsZyrVM">
<p>67. <strong>Deep learning is effective for the classification of OCT images of normal versus Age-related Macular Degeneration</strong><br />
Cecilia S Lee, Doug M Baughman, Aaron Y Lee<br />
<em>Cold Spring Harbor Laboratory</em> (2016-12-14) <a href="https://doi.org/gcgk9n">https://doi.org/gcgk9n</a><br />
DOI: <a href="https://doi.org/10.1101/094276">10.1101/094276</a></p>
</div>
<div id="ref-CCS5KSIM">
<p>68. <strong>ImageNet Classification with Deep Convolutional Neural Networks</strong><br />
Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton<br />
<em>Proceedings of the 25th International Conference on Neural Information Processing Systems</em> (2012) <a href="http://dl.acm.org/citation.cfm?id=2999134.2999257">http://dl.acm.org/citation.cfm?id=2999134.2999257</a></p>
</div>
<div id="ref-rw9nA3Y7">
<p>69. <strong>A shared task involving multi-label classification of clinical free text</strong><br />
John P. Pestian, Christopher Brew, Paweł Matykiewicz, D. J. Hovermale, Neil Johnson, K. Bretonnel Cohen, Włodzisław Duch<br />
<em>Proceedings of the Workshop on BioNLP 2007 Biological, Translational, and Clinical Language Processing - BioNLP ’07</em> (2007) <a href="https://doi.org/b4zv3c">https://doi.org/b4zv3c</a><br />
DOI: <a href="https://doi.org/10.3115/1572392.1572411">10.3115/1572392.1572411</a></p>
</div>
<div id="ref-PGi9g7yV">
<p>70. <strong>ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases</strong><br />
Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, Ronald M. Summers<br />
<em>arXiv</em> (2017-05-05) <a href="https://arxiv.org/abs/1705.02315v5">https://arxiv.org/abs/1705.02315v5</a><br />
DOI: <a href="https://doi.org/10.1109/CVPR.2017.369">10.1109/cvpr.2017.369</a></p>
</div>
<div id="ref-gYxBO26g">
<p>71. <strong>NegBio: a high-performance tool for negation and uncertainty detection in radiology reports</strong><br />
Yifan Peng, Xiaosong Wang, Le Lu, Mohammadhadi Bagheri, Ronald Summers, Zhiyong Lu<br />
<em>arXiv</em> (2017-12-16) <a href="https://arxiv.org/abs/1712.05898v2">https://arxiv.org/abs/1712.05898v2</a></p>
</div>
<div id="ref-JmHFuXEM">
<p>72. <strong>Classification evaluation</strong><br />
Jake Lever, Martin Krzywinski, Naomi Altman<br />
<em>Nature Methods</em> (2016-07-28) <a href="https://doi.org/gcgk8h">https://doi.org/gcgk8h</a><br />
DOI: <a href="https://doi.org/10.1038/nmeth.3945">10.1038/nmeth.3945</a></p>
</div>
<div id="ref-odFR7ptt">
<p>73. <strong>NIH Chest X-ray Dataset</strong><br />
NIH Clinical Center<br />
(2017-09-07) <a href="https://nihcc.app.box.com/v/ChestXray-NIHCC">https://nihcc.app.box.com/v/ChestXray-NIHCC</a></p>
</div>
<div id="ref-jTh3Ds6m">
<p>74. <strong>Pediatric Bone Age Assessment Using Deep Convolutional Neural Networks</strong><br />
Vladimir Iglovikov, Alexander Rakhlin, Alexandr A. Kalinin, Alexey Shvets<br />
<em>Cold Spring Harbor Laboratory</em> (2017-12-14) <a href="https://doi.org/gc3cfb">https://doi.org/gc3cfb</a><br />
DOI: <a href="https://doi.org/10.1101/234120">10.1101/234120</a></p>
</div>
<div id="ref-11YUuHulp">
<p>75. <strong>TaggerOne: joint named entity recognition and normalization with semi-Markov Models</strong><br />
Robert Leaman, Zhiyong Lu<br />
<em>Bioinformatics</em> (2016-06-09) <a href="https://doi.org/f855dg">https://doi.org/f855dg</a><br />
DOI: <a href="https://doi.org/10.1093/bioinformatics/btw343">10.1093/bioinformatics/btw343</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27283952">27283952</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5018376">PMC5018376</a></p>
</div>
<div id="ref-19G2RXgfp">
<p>76. <strong>tmVar: a text mining approach for extracting sequence variants in biomedical literature</strong><br />
C.-H. Wei, B. R. Harris, H.-Y. Kao, Z. Lu<br />
<em>Bioinformatics</em> (2013-04-05) <a href="https://doi.org/f4zfd9">https://doi.org/f4zfd9</a><br />
DOI: <a href="https://doi.org/10.1093/bioinformatics/btt156">10.1093/bioinformatics/btt156</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/23564842">23564842</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3661051">PMC3661051</a></p>
</div>
<div id="ref-vtuZ3Wx7">
<p>77. <strong>DNorm: disease name normalization with pairwise learning to rank</strong><br />
R. Leaman, R. Islamaj Dogan, Z. Lu<br />
<em>Bioinformatics</em> (2013-08-21) <a href="https://doi.org/f5gj9n">https://doi.org/f5gj9n</a><br />
DOI: <a href="https://doi.org/10.1093/bioinformatics/btt474">10.1093/bioinformatics/btt474</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/23969135">23969135</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3810844">PMC3810844</a></p>
</div>
<div id="ref-OuoHShLI">
<p>78. <strong>Effects of Semantic Features on Machine Learning-Based Drug Name Recognition Systems: Word Embeddings vs. Manually Constructed Dictionaries</strong><br />
Shengyu Liu, Buzhou Tang, Qingcai Chen, Xiaolong Wang<br />
<em>Information</em> (2015-12-11) <a href="https://doi.org/gcgnzj">https://doi.org/gcgnzj</a><br />
DOI: <a href="https://doi.org/10.3390/info6040848">10.3390/info6040848</a></p>
</div>
<div id="ref-14uLNRP38">
<p>79. <strong>Evaluating Word Representation Features in Biomedical Named Entity Recognition Tasks</strong><br />
Buzhou Tang, Hongxin Cao, Xiaolong Wang, Qingcai Chen, Hua Xu<br />
<em>BioMed Research International</em> (2014) <a href="https://doi.org/gb76d4">https://doi.org/gb76d4</a><br />
DOI: <a href="https://doi.org/10.1155/2014/240403">10.1155/2014/240403</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/24729964">24729964</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3963372">PMC3963372</a></p>
</div>
<div id="ref-pWpK5WUq">
<p>80. <strong>Clinical Abbreviation Disambiguation Using Neural Word Embeddings</strong><br />
yonghui wu, Jun Xu, Yaoyun Zhang, Hua Xu<br />
<em>Proceedings of BioNLP 15</em> (2015) <a href="https://doi.org/gcgnx7">https://doi.org/gcgnx7</a><br />
DOI: <a href="https://doi.org/10.18653/v1/w15-3822">10.18653/v1/w15-3822</a></p>
</div>
<div id="ref-LeLKNlsR">
<p>81. <strong>Exploiting Task-Oriented Resources to Learn Word Embeddings for Clinical Abbreviation Expansion</strong><br />
Yue Liu, Tao Ge, Kusum Mathews, Heng Ji, Deborah McGuinness<br />
<em>Proceedings of BioNLP 15</em> (2015) <a href="https://doi.org/gcgnx5">https://doi.org/gcgnx5</a><br />
DOI: <a href="https://doi.org/10.18653/v1/w15-3810">10.18653/v1/w15-3810</a></p>
</div>
<div id="ref-c6gbPCdT">
<p>82. <strong>A Comprehensive Benchmark of Kernel Methods to Extract Protein–Protein Interactions from Literature</strong><br />
Domonkos Tikk, Philippe Thomas, Peter Palaga, Jörg Hakenberg, Ulf Leser<br />
<em>PLoS Computational Biology</em> (2010-07-01) <a href="https://doi.org/dsnc6j">https://doi.org/dsnc6j</a><br />
DOI: <a href="https://doi.org/10.1371/journal.pcbi.1000837">10.1371/journal.pcbi.1000837</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/20617200">20617200</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2895635">PMC2895635</a></p>
</div>
<div id="ref-Sn8TwSQK">
<p>83. <strong>Improving chemical disease relation extraction with rich features and weakly labeled data</strong><br />
Yifan Peng, Chih-Hsuan Wei, Zhiyong Lu<br />
<em>Journal of Cheminformatics</em> (2016-10-07) <a href="https://doi.org/gcgnx3">https://doi.org/gcgnx3</a><br />
DOI: <a href="https://doi.org/10.1186/s13321-016-0165-z">10.1186/s13321-016-0165-z</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28316651">28316651</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5054544">PMC5054544</a></p>
</div>
<div id="ref-X909h5sz">
<p>84. <strong>Evaluation of linguistic features useful in extraction of interactions from PubMed; Application to annotating known, high-throughput and predicted interactions in I2D</strong><br />
Yun Niu, David Otasek, Igor Jurisica<br />
<em>Bioinformatics</em> (2009-10-22) <a href="https://doi.org/fv4nfs">https://doi.org/fv4nfs</a><br />
DOI: <a href="https://doi.org/10.1093/bioinformatics/btp602">10.1093/bioinformatics/btp602</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/19850753">19850753</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2796811">PMC2796811</a></p>
</div>
<div id="ref-17ydlqmVI">
<p>85. <strong>Joint Models for Extracting Adverse Drug Events from Biomedical Text</strong><br />
Fei Li, Yue Zhang, Meishan Zhang, Donghong Ji<br />
<em>Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence</em> (2016) <a href="http://dl.acm.org/citation.cfm?id=3060832.3061018">http://dl.acm.org/citation.cfm?id=3060832.3061018</a><br />
ISBN: <a href="https://worldcat.org/isbn/978-1-57735-770-4">978-1-57735-770-4</a></p>
</div>
<div id="ref-1F5aZYjOB">
<p>86. <strong>A neural joint model for entity and relation extraction from biomedical text</strong><br />
Fei Li, Meishan Zhang, Guohong Fu, Donghong Ji<br />
<em>BMC Bioinformatics</em> (2017-03-31) <a href="https://doi.org/gcgnx2">https://doi.org/gcgnx2</a><br />
DOI: <a href="https://doi.org/10.1186/s12859-017-1609-9">10.1186/s12859-017-1609-9</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28359255">28359255</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5374588">PMC5374588</a></p>
</div>
<div id="ref-1H4fyFU1f">
<p>87. <strong>Deep learning for extracting protein-protein interactions from biomedical literature</strong><br />
Yifan Peng, Zhiyong Lu<br />
<em>BioNLP 2017</em> (2017) <a href="https://doi.org/gcgnzc">https://doi.org/gcgnzc</a><br />
DOI: <a href="https://doi.org/10.18653/v1/w17-2304">10.18653/v1/w17-2304</a></p>
</div>
<div id="ref-19r6xFsZQ">
<p>88. <strong>A Shortest Dependency Path Based Convolutional Neural Network for Protein-Protein Relation Extraction</strong><br />
Lei Hua, Chanqin Quan<br />
<em>BioMed Research International</em> (2016) <a href="https://doi.org/f9jkh9">https://doi.org/f9jkh9</a><br />
DOI: <a href="https://doi.org/10.1155/2016/8479587">10.1155/2016/8479587</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27493967">27493967</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4963603">PMC4963603</a></p>
</div>
<div id="ref-ULZPgbOq">
<p>89. <strong>Multichannel Convolutional Neural Network for Biological Relation Extraction</strong><br />
Chanqin Quan, Lei Hua, Xiao Sun, Wenjun Bai<br />
<em>BioMed Research International</em> (2016) <a href="https://doi.org/f9kqr3">https://doi.org/f9kqr3</a><br />
DOI: <a href="https://doi.org/10.1155/2016/1850404">10.1155/2016/1850404</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28053977">28053977</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5174749">PMC5174749</a></p>
</div>
<div id="ref-MY6FXgFn">
<p>90. <strong>A general protein-protein interaction extraction architecture based on word representation and feature selection</strong><br />
Zhenchao Jiang, shuang Li, Degen Huang<br />
<em>International Journal of Data Mining and Bioinformatics</em> (2016) <a href="https://doi.org/gcgnx4">https://doi.org/gcgnx4</a><br />
DOI: <a href="https://doi.org/10.1504/ijdmb.2016.074878">10.1504/ijdmb.2016.074878</a></p>
</div>
<div id="ref-14afj7TT1">
<p>91. <strong>Chemical-induced disease relation extraction via convolutional neural network</strong><br />
Jinghang Gu, Fuqing Sun, Longhua Qian, Guodong Zhou<br />
<em>Database</em> (2017-01-01) <a href="https://doi.org/f92754">https://doi.org/f92754</a><br />
DOI: <a href="https://doi.org/10.1093/database/bax024">10.1093/database/bax024</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28415073">28415073</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5467558">PMC5467558</a></p>
</div>
<div id="ref-8NrcroGt">
<p>92. <strong>Drug drug interaction extraction from biomedical literature using syntax convolutional neural network</strong><br />
Zhehuan Zhao, Zhihao Yang, Ling Luo, Hongfei Lin, Jian Wang<br />
<em>Bioinformatics</em> (2016-07-27) <a href="https://doi.org/f9nsq7">https://doi.org/f9nsq7</a><br />
DOI: <a href="https://doi.org/10.1093/bioinformatics/btw486">10.1093/bioinformatics/btw486</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27466626">27466626</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5181565">PMC5181565</a></p>
</div>
<div id="ref-zUPTZa6w">
<p>93. <strong>Extracting Drug-Drug Interactions with Attention CNNs</strong><br />
Masaki Asada, Makoto Miwa, Yutaka Sasaki<br />
<em>BioNLP 2017</em> (2017) <a href="https://doi.org/gcgnzb">https://doi.org/gcgnzb</a><br />
DOI: <a href="https://doi.org/10.18653/v1/w17-2302">10.18653/v1/w17-2302</a></p>
</div>
<div id="ref-M6JCKCLX">
<p>94. <strong>Drug-drug Interaction Extraction via Recurrent Neural Network with Multiple Attention Layers</strong><br />
Zibo Yi, Shasha Li, Jie Yu, Qingbo Wu<br />
<em>arXiv</em> (2017-05-09) <a href="https://arxiv.org/abs/1705.03261v2">https://arxiv.org/abs/1705.03261v2</a></p>
</div>
<div id="ref-ztw1ugBP">
<p>95. <strong>DUTIR in BioNLP-ST 2016: Utilizing Convolutional Network and Distributed Representation to Extract Complicate Relations</strong><br />
Honglei Li, Jianhai Zhang, Jian Wang, Hongfei Lin, Zhihao Yang<br />
<em>Proceedings of the 4th BioNLP Shared Task Workshop</em> (2016) <a href="https://doi.org/gcgnx9">https://doi.org/gcgnx9</a><br />
DOI: <a href="https://doi.org/10.18653/v1/w16-3012">10.18653/v1/w16-3012</a></p>
</div>
<div id="ref-1AkznVzFs">
<p>96. <strong>Deep Learning with Minimal Training Data: TurkuNLP Entry in the BioNLP Shared Task 2016</strong><br />
Farrokh Mehryary, Jari Björne, Sampo Pyysalo, Tapio Salakoski, Filip Ginter<br />
<em>Proceedings of the 4th BioNLP Shared Task Workshop</em> (2016) <a href="https://doi.org/gcgnx8">https://doi.org/gcgnx8</a><br />
DOI: <a href="https://doi.org/10.18653/v1/w16-3009">10.18653/v1/w16-3009</a></p>
</div>
<div id="ref-E0XYyYBe">
<p>97. <strong>Using word embedding for bio-event extraction</strong><br />
Chen Li, Runqing Song, Maria Liakata, Andreas Vlachos, Stephanie Seneff, Xiangrong Zhang<br />
<em>Proceedings of BioNLP 15</em> (2015) <a href="https://doi.org/gcgnx6">https://doi.org/gcgnx6</a><br />
DOI: <a href="https://doi.org/10.18653/v1/w15-3814">10.18653/v1/w15-3814</a></p>
</div>
<div id="ref-XbkgrcMy">
<p>98. <strong>Embedding assisted prediction architecture for event trigger identification</strong><br />
Yifan Nie, Wenge Rong, Yiyuan Zhang, Yuanxin Ouyang, Zhang Xiong<br />
<em>Journal of Bioinformatics and Computational Biology</em> (2015-06) <a href="https://doi.org/gcgnxz">https://doi.org/gcgnxz</a><br />
DOI: <a href="https://doi.org/10.1142/s0219720015410012">10.1142/s0219720015410012</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/25669328">25669328</a></p>
</div>
<div id="ref-z8hmfrmY">
<p>99. <strong>Biomedical Event Trigger Identification Using Bidirectional Recurrent Neural Network Based Models</strong><br />
Patchigolla V S S Rahul, Sunil Kumar Sahu, Ashish Anand<br />
<em>arXiv</em> (2017-05-26) <a href="https://arxiv.org/abs/1705.09516v1">https://arxiv.org/abs/1705.09516v1</a></p>
</div>
<div id="ref-zPnsAqyX">
<p>100. <strong>Deep Learning for Biomedical Information Retrieval: Learning Textual Relevance from Click Logs</strong><br />
Sunil Mohan, Nicolas Fiorini, Sun Kim, Zhiyong Lu<br />
<em>BioNLP 2017</em> (2017) <a href="https://doi.org/gcgnzd">https://doi.org/gcgnzd</a><br />
DOI: <a href="https://doi.org/10.18653/v1/w17-2328">10.18653/v1/w17-2328</a></p>
</div>
<div id="ref-uDaRUyh9">
<p>101. <strong>Realizing the full potential of electronic health records: the role of natural language processing</strong><br />
Lucila Ohno-Machado<br />
<em>Journal of the American Medical Informatics Association</em> (2011-09) <a href="https://doi.org/cx3q3f">https://doi.org/cx3q3f</a><br />
DOI: <a href="https://doi.org/10.1136/amiajnl-2011-000501">10.1136/amiajnl-2011-000501</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/21846784">21846784</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3168331">PMC3168331</a></p>
</div>
<div id="ref-sG3iVOTS">
<p>102. <strong>Machine-learned solutions for three stages of clinical information extraction: the state of the art at i2b2 2010</strong><br />
Berry de Bruijn, Colin Cherry, Svetlana Kiritchenko, Joel Martin, Xiaodan Zhu<br />
<em>Journal of the American Medical Informatics Association</em> (2011-09) <a href="https://doi.org/dk7jfw">https://doi.org/dk7jfw</a><br />
DOI: <a href="https://doi.org/10.1136/amiajnl-2011-000150">10.1136/amiajnl-2011-000150</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/21565856">21565856</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3168309">PMC3168309</a></p>
</div>
<div id="ref-dO844vZn">
<p>103. <strong>Bidirectional LSTM-CRF for Clinical Concept Extraction</strong><br />
Raghavendra Chalapathy, Ehsan Zare Borzeshi, Massimo Piccardi<br />
<em>arXiv</em> (2016-11-25) <a href="https://arxiv.org/abs/1611.08373v1">https://arxiv.org/abs/1611.08373v1</a></p>
</div>
<div id="ref-yUgE09ve">
<p>104. <strong>Multi-task Deep Neural Networks for Automated Extraction of Primary Site and Laterality Information from Cancer Pathology Reports</strong><br />
Hong-Jun Yoon, Arvind Ramanathan, Georgia Tourassi<br />
<em>Advances in Big Data</em> (2016-10-08) <a href="https://doi.org/gcgk7s">https://doi.org/gcgk7s</a><br />
DOI: <a href="https://doi.org/10.1007/978-3-319-47898-2_21">10.1007/978-3-319-47898-2_21</a></p>
</div>
<div id="ref-1GhHIDxuW">
<p>105. <strong>Efficient Estimation of Word Representations in Vector Space</strong><br />
Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean<br />
<em>arXiv</em> (2013-01-16) <a href="https://arxiv.org/abs/1301.3781v3">https://arxiv.org/abs/1301.3781v3</a></p>
</div>
<div id="ref-sePDg4mZ">
<p>106. <strong>Exploring the Application of Deep Learning Techniques on Medical Text Corpora</strong><br />
Minarro-Giménez José Antonio, Marín-Alonso Oscar, Samwald Matthias<br />
<em>Studies in Health Technology and Informatics</em> (2014) <a href="https://doi.org/gcgnzh">https://doi.org/gcgnzh</a><br />
DOI: <a href="https://doi.org/10.3233/978-1-61499-432-9-584">10.3233/978-1-61499-432-9-584</a></p>
</div>
<div id="ref-XQtuRkTU">
<p>107. <strong>Medical Semantic Similarity with a Neural Language Model</strong><br />
Lance De Vine, Guido Zuccon, Bevan Koopman, Laurianne Sitbon, Peter Bruza<br />
<em>Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management - CIKM ’14</em> (2014) <a href="https://doi.org/gcgmbx">https://doi.org/gcgmbx</a><br />
DOI: <a href="https://doi.org/10.1145/2661829.2661974">10.1145/2661829.2661974</a></p>
</div>
<div id="ref-4QDXEv4C">
<p>108. <strong>Automatic Diagnosis Coding of Radiology Reports: A Comparison of Deep Learning and Conventional Classification Methods</strong><br />
Sarvnaz Karimi, Xiang Dai, Hamedh Hassanzadeh, Anthony Nguyen<br />
<em>BioNLP 2017</em> (2017) <a href="https://doi.org/gcgnzg">https://doi.org/gcgnzg</a><br />
DOI: <a href="https://doi.org/10.18653/v1/w17-2342">10.18653/v1/w17-2342</a></p>
</div>
<div id="ref-zVoUcFPZ">
<p>109. <strong>WHO | International Classification of Diseases, 11th Revision (ICD-11)</strong><br />
WHO<br />
<a href="http://www.who.int/classifications/icd/en/">http://www.who.int/classifications/icd/en/</a></p>
</div>
<div id="ref-TwvauiTv">
<p>110. <strong>Multi-layer Representation Learning for Medical Concepts</strong><br />
Edward Choi, Mohammad Taha Bahadori, Elizabeth Searles, Catherine Coffey, Michael Thompson, James Bost, Javier Tejedor-Sojo, Jimeng Sun<br />
<em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD ’16</em> (2016) <a href="https://doi.org/gcgmb3">https://doi.org/gcgmb3</a><br />
DOI: <a href="https://doi.org/10.1145/2939672.2939823">10.1145/2939672.2939823</a></p>
</div>
<div id="ref-1G2xP5yOM">
<p>111. <strong>Large-Scale Discovery of Disease-Disease and Disease-Gene Associations</strong><br />
Djordje Gligorijevic, Jelena Stojanovic, Nemanja Djuric, Vladan Radosavljevic, Mihajlo Grbovic, Rob J. Kulathinal, Zoran Obradovic<br />
<em>Scientific Reports</em> (2016-08-31) <a href="https://doi.org/f8znpb">https://doi.org/f8znpb</a><br />
DOI: <a href="https://doi.org/10.1038/srep32404">10.1038/srep32404</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27578529">27578529</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5006166">PMC5006166</a></p>
</div>
<div id="ref-rqGoVCuH">
<p>112. <strong>Bidirectional RNN for Medical Event Detection in Electronic Health Records</strong><br />
Abhyuday N Jagannatha, Hong Yu<br />
<em>Proceedings of the conference. Association for Computational Linguistics. North American Chapter. Meeting</em> (2016-06) <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5119627/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5119627/</a><br />
PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27885364">27885364</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5119627">PMC5119627</a></p>
</div>
<div id="ref-8YmxYueq">
<p>113. <strong>Representations of Time Expressions for Temporal Relation Extraction with Convolutional Neural Networks</strong><br />
Chen Lin, Timothy Miller, Dmitriy Dligach, Steven Bethard, Guergana Savova<br />
<em>BioNLP 2017</em> (2017) <a href="https://doi.org/gcgnzf">https://doi.org/gcgnzf</a><br />
DOI: <a href="https://doi.org/10.18653/v1/w17-2341">10.18653/v1/w17-2341</a></p>
</div>
<div id="ref-FLX0o7bL">
<p>114. <strong>Computational Phenotype Discovery Using Unsupervised Feature Learning over Noisy, Sparse, and Irregular Clinical Data</strong><br />
Thomas A. Lasko, Joshua C. Denny, Mia A. Levy<br />
<em>PLoS ONE</em> (2013-06-24) <a href="https://doi.org/f49g5g">https://doi.org/f49g5g</a><br />
DOI: <a href="https://doi.org/10.1371/journal.pone.0066341">10.1371/journal.pone.0066341</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/23826094">23826094</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3691199">PMC3691199</a></p>
</div>
<div id="ref-5x3uMSKi">
<p>115. <strong>Semi-supervised learning of the electronic health record for phenotype stratification</strong><br />
Brett K. Beaulieu-Jones, Casey S. Greene<br />
<em>Journal of Biomedical Informatics</em> (2016-12) <a href="https://doi.org/gbw7bv">https://doi.org/gbw7bv</a><br />
DOI: <a href="https://doi.org/10.1016/j.jbi.2016.10.007">10.1016/j.jbi.2016.10.007</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27744022">27744022</a></p>
</div>
<div id="ref-WrNCJ9sO">
<p>116. <strong>Deep Patient: An Unsupervised Representation to Predict the Future of Patients from the Electronic Health Records</strong><br />
Riccardo Miotto, Li Li, Brian A. Kidd, Joel T. Dudley<br />
<em>Scientific Reports</em> (2016-05) <a href="https://doi.org/f8n6sg">https://doi.org/f8n6sg</a><br />
DOI: <a href="https://doi.org/10.1038/srep26094">10.1038/srep26094</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27185194">27185194</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4869115">PMC4869115</a></p>
</div>
<div id="ref-11tMRPqto">
<p>117. <strong>Doctor AI: Predicting Clinical Events via Recurrent Neural Networks</strong><br />
Edward Choi, Mohammad Taha Bahadori, Andy Schuetz, Walter F. Stewart, Jimeng Sun<br />
<em>arXiv</em> (2015-11-18) <a href="https://arxiv.org/abs/1511.05942v11">https://arxiv.org/abs/1511.05942v11</a></p>
</div>
<div id="ref-HRXii6Ni">
<p>118. <strong>DeepCare: A Deep Dynamic Memory Model for Predictive Medicine</strong><br />
Trang Pham, Truyen Tran, Dinh Phung, Svetha Venkatesh<br />
<em>arXiv</em> (2016-02-01) <a href="https://arxiv.org/abs/1602.00357v2">https://arxiv.org/abs/1602.00357v2</a></p>
</div>
<div id="ref-1Fiy543WZ">
<p>119. <strong>Deepr: A Convolutional Net for Medical Records</strong><br />
Phuoc Nguyen, Truyen Tran, Nilmini Wickramasinghe, Svetha Venkatesh<br />
<em>IEEE Journal of Biomedical and Health Informatics</em> (2017-01) <a href="https://doi.org/10.1109/jbhi.2016.2633963">https://doi.org/10.1109/jbhi.2016.2633963</a><br />
DOI: <a href="https://doi.org/10.1109/jbhi.2016.2633963">10.1109/jbhi.2016.2633963</a></p>
</div>
<div id="ref-c6MfDdWP">
<p>120. <strong>Multi-task Prediction of Disease Onsets from Longitudinal Lab Tests</strong><br />
Narges Razavian, Jake Marcus, David Sontag<br />
<em>arXiv</em> (2016-08-02) <a href="https://arxiv.org/abs/1608.00647v3">https://arxiv.org/abs/1608.00647v3</a></p>
</div>
<div id="ref-qXdO2aMm">
<p>121. <strong>Deep Survival Analysis</strong><br />
Rajesh Ranganath, Adler Perotte, Noémie Elhadad, David Blei<br />
<em>arXiv</em> (2016-08-06) <a href="https://arxiv.org/abs/1608.02158v2">https://arxiv.org/abs/1608.02158v2</a></p>
</div>
<div id="ref-1921Mctzh">
<p>122. <strong>Comparison of the performance of neural network methods and Cox regression for censored survival data</strong><br />
Anny Xiang, Pablo Lapuerta, Alex Ryutov, Jonathan Buckley, Stanley Azen<br />
<em>Computational Statistics &amp; Data Analysis</em> (2000-08) <a href="https://doi.org/bsgrc8">https://doi.org/bsgrc8</a><br />
DOI: <a href="https://doi.org/10.1016/s0167-9473(99)00098-5">10.1016/s0167-9473(99)00098-5</a></p>
</div>
<div id="ref-1FE0F2pQ">
<p>123. <strong>DeepSurv: Personalized Treatment Recommender System Using A Cox Proportional Hazards Deep Neural Network</strong><br />
Jared Katzman, Uri Shaham, Jonathan Bates, Alexander Cloninger, Tingting Jiang, Yuval Kluger<br />
<em>arXiv</em> (2016-06-02) <a href="https://arxiv.org/abs/1606.00931v3">https://arxiv.org/abs/1606.00931v3</a><br />
DOI: <a href="https://doi.org/10.1186/s12874-018-0482-1">10.1186/s12874-018-0482-1</a></p>
</div>
<div id="ref-pxdeuhMS">
<p>124. <strong>Deep Exponential Families</strong><br />
Rajesh Ranganath, Linpeng Tang, Laurent Charlin, David M. Blei<br />
<em>arXiv</em> (2014-11-10) <a href="https://arxiv.org/abs/1411.2581v1">https://arxiv.org/abs/1411.2581v1</a></p>
</div>
<div id="ref-8RAYEOPl">
<p>125. <strong>Stochastic Variational Inference</strong><br />
Matt Hoffman, David M. Blei, Chong Wang, John Paisley<br />
<em>arXiv</em> (2012-06-29) <a href="https://arxiv.org/abs/1206.7051v3">https://arxiv.org/abs/1206.7051v3</a></p>
</div>
<div id="ref-15lbUf0as">
<p>126. <strong>Hierarchical Variational Models</strong><br />
Rajesh Ranganath, Dustin Tran, David M. Blei<br />
<em>arXiv</em> (2015-11-07) <a href="https://arxiv.org/abs/1511.02386v2">https://arxiv.org/abs/1511.02386v2</a></p>
</div>
<div id="ref-1Ar4f4vfR">
<p>127. <strong>A machine learning-based framework to identify type 2 diabetes through electronic health records</strong><br />
Tao Zheng, Wei Xie, Liling Xu, Xiaoying He, Ya Zhang, Mingrong You, Gong Yang, You Chen<br />
<em>International Journal of Medical Informatics</em> (2017-01) <a href="https://doi.org/f9hkxb">https://doi.org/f9hkxb</a><br />
DOI: <a href="https://doi.org/10.1016/j.ijmedinf.2016.09.014">10.1016/j.ijmedinf.2016.09.014</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27919371">27919371</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5144921">PMC5144921</a></p>
</div>
<div id="ref-ziudr6hx">
<p>128. <strong>Implementations by Phenotype | PheKB</strong><a href="https://phekb.org/implementations">https://phekb.org/implementations</a></p>
</div>
<div id="ref-A9JeoGV8">
<p>129. <strong>Electronic medical record phenotyping using the anchor and learn framework</strong><br />
Yoni Halpern, Steven Horng, Youngduck Choi, David Sontag<br />
<em>Journal of the American Medical Informatics Association</em> (2016-04-23) <a href="https://doi.org/f84xjd">https://doi.org/f84xjd</a><br />
DOI: <a href="https://doi.org/10.1093/jamia/ocw011">10.1093/jamia/ocw011</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27107443">27107443</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4926745">PMC4926745</a></p>
</div>
<div id="ref-5Il3kN32">
<p>130. <strong>Data Programming: Creating Large Training Sets, Quickly</strong><br />
Alexander Ratner, Christopher De Sa, Sen Wu, Daniel Selsam, Christopher Ré<br />
<em>arXiv</em> (2016-05-25) <a href="https://arxiv.org/abs/1605.07723v3">https://arxiv.org/abs/1605.07723v3</a></p>
</div>
<div id="ref-6fE0Vrba">
<p>131. <strong>Data is the New Oil</strong><br />
Michael Palmer<br />
<em>ANA Marketing Maestros</em> (2006-11) <a href="http://ana.blogs.com/maestros/2006/11/data_is_the_new.html">http://ana.blogs.com/maestros/2006/11/data_is_the_new.html</a></p>
</div>
<div id="ref-o8mib4CN">
<p>132. <strong>“Data is the New Oil” — A Ludicrous Proposition - Project 2030</strong><br />
Michael Haupt<br />
<em>Medium</em> (2018-04-23) <a href="https://medium.com/project-2030/data-is-the-new-oil-a-ludicrous-proposition-1d91bba4f294">https://medium.com/project-2030/data-is-the-new-oil-a-ludicrous-proposition-1d91bba4f294</a></p>
</div>
<div id="ref-hfcf5Hmi">
<p>133. <strong>Data Programming: Machine Learning with Weak Supervision</strong><br />
Alex Ratner, Stephen Bach, Chris Ré<br />
(2016-09-19) <a href="http://hazyresearch.github.io/snorkel/blog/weak_supervision.html">http://hazyresearch.github.io/snorkel/blog/weak_supervision.html</a></p>
</div>
<div id="ref-FkSZ1qmz">
<p>134. <strong>Mining electronic health records: towards better research applications and clinical care</strong><br />
Peter B. Jensen, Lars J. Jensen, Søren Brunak<br />
<em>Nature Reviews Genetics</em> (2012-05-02) <a href="https://doi.org/bdzn">https://doi.org/bdzn</a><br />
DOI: <a href="https://doi.org/10.1038/nrg3208">10.1038/nrg3208</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/22549152">22549152</a></p>
</div>
<div id="ref-odRiFxnB">
<p>135. <strong>Methods and dimensions of electronic health record data quality assessment: enabling reuse for clinical research</strong><br />
N. G. Weiskopf, C. Weng<br />
<em>Journal of the American Medical Informatics Association</em> (2013-01-01) <a href="https://doi.org/f4mbpb">https://doi.org/f4mbpb</a><br />
DOI: <a href="https://doi.org/10.1136/amiajnl-2011-000681">10.1136/amiajnl-2011-000681</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/22733976">22733976</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3555312">PMC3555312</a></p>
</div>
<div id="ref-7s3dpUrT">
<p>136. <strong>Impact of electronic health record systems on information integrity: quality and safety implications</strong><br />
Sue Bowman<br />
<em>Perspectives in health information management</em> (2013-10-01) <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3797550/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3797550/</a><br />
PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/24159271">24159271</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3797550">PMC3797550</a></p>
</div>
<div id="ref-RoOhUFKU">
<p>137. <strong>Secondary Use of EHR: Data Quality Issues and Informatics Opportunities</strong><br />
Taxiarchis Botsis, Gunnar Hartvigsen, Fei Chen, Chunhua Weng<br />
<em>Summit on translational bioinformatics</em> (2010-03-01) <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3041534/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3041534/</a><br />
PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/21347133">21347133</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3041534">PMC3041534</a></p>
</div>
<div id="ref-7BctyA7f">
<p>138. <strong>Have DRG-based prospective payment systems influenced the number of secondary diagnoses in health care administrative data?</strong><br />
Lisbeth Serdén, Rikard Lindqvist, Måns Rosén<br />
<em>Health Policy</em> (2003-08) <a href="https://doi.org/dpgs86">https://doi.org/dpgs86</a><br />
DOI: <a href="https://doi.org/10.1016/s0168-8510(02)00208-7">10.1016/s0168-8510(02)00208-7</a></p>
</div>
<div id="ref-eAP2kxzn">
<p>139. <strong>Why Patient Matching Is a Challenge: Research on Master Patient Index (MPI) Data Discrepancies in Key Identifying Fields</strong><br />
Beth Haenke Just, David Marc, Megan Munns, Ryan Sandefer<br />
<em>Perspectives in health information management</em> (2016-04-01) <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4832129/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4832129/</a><br />
PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27134610">27134610</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4832129">PMC4832129</a></p>
</div>
<div id="ref-1C97CTU9S">
<p>140. <strong>Identifying and mitigating biases in EHR laboratory tests</strong><br />
Rimma Pivovarov, David J. Albers, Jorge L. Sepulveda, Noémie Elhadad<br />
<em>Journal of Biomedical Informatics</em> (2014-10) <a href="https://doi.org/f6mgds">https://doi.org/f6mgds</a><br />
DOI: <a href="https://doi.org/10.1016/j.jbi.2014.03.016">10.1016/j.jbi.2014.03.016</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/24727481">24727481</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4194228">PMC4194228</a></p>
</div>
<div id="ref-13filvWwr">
<p>141. <strong>Using electronic health records for clinical research: The case of the EHR4CR project</strong><br />
Georges De Moor, Mats Sundgren, Dipak Kalra, Andreas Schmidt, Martin Dugas, Brecht Claerhout, Töresin Karakoyun, Christian Ohmann, Pierre-Yves Lastic, Nadir Ammour, … Pascal Coorevits<br />
<em>Journal of Biomedical Informatics</em> (2015-02) <a href="https://doi.org/gcgk73">https://doi.org/gcgk73</a><br />
DOI: <a href="https://doi.org/10.1016/j.jbi.2014.10.006">10.1016/j.jbi.2014.10.006</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/25463966">25463966</a></p>
</div>
<div id="ref-1CWhXZxos">
<p>142. <strong>Healthcare Interoperability Standards Compliance Handbook</strong><br />
Frank Oemig, Robert Snelick<br />
<em>Springer International Publishing</em> (2016) <a href="https://doi.org/gcgk7m">https://doi.org/gcgk7m</a><br />
DOI: <a href="https://doi.org/10.1007/978-3-319-44839-8">10.1007/978-3-319-44839-8</a></p>
</div>
<div id="ref-Fx5qVQlk">
<p>143. <strong>How sample size influences research outcomes</strong><br />
Jorge Faber, Lilian Martins Fonseca<br />
<em>Dental Press Journal of Orthodontics</em> (2014-08) <a href="https://doi.org/gcgmcx">https://doi.org/gcgmcx</a><br />
DOI: <a href="https://doi.org/10.1590/2176-9451.19.4.027-029.ebo">10.1590/2176-9451.19.4.027-029.ebo</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/25279518">25279518</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4296634">PMC4296634</a></p>
</div>
<div id="ref-11OyzMl87">
<p>144. <strong>A review of approaches to identifying patient phenotype cohorts using electronic health records</strong><br />
Chaitanya Shivade, Preethi Raghavan, Eric Fosler-Lussier, Peter J Embi, Noemie Elhadad, Stephen B Johnson, Albert M Lai<br />
<em>Journal of the American Medical Informatics Association</em> (2014-03) <a href="https://doi.org/f5tsfq">https://doi.org/f5tsfq</a><br />
DOI: <a href="https://doi.org/10.1136/amiajnl-2013-001935">10.1136/amiajnl-2013-001935</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/24201027">24201027</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3932460">PMC3932460</a></p>
</div>
<div id="ref-qe90c1CL">
<p>145. <strong>STRATEGIES FOR EQUITABLE PHARMACOGENOMIC-GUIDED WARFARIN DOSING AMONG EUROPEAN AND AFRICAN AMERICAN INDIVIDUALS IN A CLINICAL POPULATION</strong><br />
LAURA K. WILEY, JACOB P. VANHOUTEN, DAVID C. SAMUELS, MELINDA C. ALDRICH, DAN M. RODEN, JOSH F. PETERSON, JOSHUA C. DENNY<br />
<em>Biocomputing 2017</em> (2016-11-22) <a href="https://doi.org/gcgmbr">https://doi.org/gcgmbr</a><br />
DOI: <a href="https://doi.org/10.1142/9789813207813_0050">10.1142/9789813207813_0050</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27897005">27897005</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5389380">PMC5389380</a></p>
</div>
<div id="ref-CVnO5njl">
<p>146. <strong>Epidemiological research labelled as a violation of privacy: the case of Estonia</strong><br />
M. Rahu, M. McKee<br />
<em>International Journal of Epidemiology</em> (2008-02-26) <a href="https://doi.org/bszgkf">https://doi.org/bszgkf</a><br />
DOI: <a href="https://doi.org/10.1093/ije/dyn022">10.1093/ije/dyn022</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/18304955">18304955</a></p>
</div>
<div id="ref-2cYqPKf1">
<p>147. <strong>Harnessing next-generation informatics for personalizing medicine: a report from AMIA’s 2014 Health Policy Invitational Meeting</strong><br />
Laura K Wiley, Peter Tarczy-Hornoch, Joshua C Denny, Robert R Freimuth, Casey L Overby, Nigam Shah, Ross D Martin, Indra Neil Sarkar<br />
<em>Journal of the American Medical Informatics Association</em> (2016-02-05) <a href="https://doi.org/f84nww">https://doi.org/f84nww</a><br />
DOI: <a href="https://doi.org/10.1093/jamia/ocv111">10.1093/jamia/ocv111</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26911808">26911808</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6457095">PMC6457095</a></p>
</div>
<div id="ref-SfxIiPJ1">
<p>148. <strong>DataSHIELD: taking the analysis to the data, not the data to the analysis</strong><br />
Amadou Gaye, Yannick Marcon, Julia Isaeva, Philippe LaFlamme, Andrew Turner, Elinor M Jones, Joel Minion, Andrew W Boyd, Christopher J Newby, Marja-Liisa Nuotio, … Paul R Burton<br />
<em>International Journal of Epidemiology</em> (2014-09-27) <a href="https://doi.org/gcgk82">https://doi.org/gcgk82</a><br />
DOI: <a href="https://doi.org/10.1093/ije/dyu188">10.1093/ije/dyu188</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/25261970">25261970</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4276062">PMC4276062</a></p>
</div>
<div id="ref-1D6b3tMu9">
<p>149. <strong>ViPAR: a software platform for the Virtual Pooling and Analysis of Research Data</strong><br />
Kim W Carter, KW Carter, RW Francis, M Bresnahan, M Gissler, TK Grønborg, R Gross, N Gunnes, G Hammond, M Hornig, … Z Yusof<br />
<em>International Journal of Epidemiology</em> (2015-10-08) <a href="https://doi.org/f8pzfm">https://doi.org/f8pzfm</a><br />
DOI: <a href="https://doi.org/10.1093/ije/dyv193">10.1093/ije/dyv193</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26452388">26452388</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4864874">PMC4864874</a></p>
</div>
<div id="ref-Qh7xTLwz">
<p>150. <strong>Reproducibility of computational workflows is automated using continuous analysis</strong><br />
Brett K Beaulieu-Jones, Casey S Greene<br />
<em>Nature Biotechnology</em> (2017-03-13) <a href="https://doi.org/f9ttx6">https://doi.org/f9ttx6</a><br />
DOI: <a href="https://doi.org/10.1038/nbt.3780">10.1038/nbt.3780</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28288103">28288103</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6103790">PMC6103790</a></p>
</div>
<div id="ref-ULSPV0rh">
<p>151. <strong>Stealing Machine Learning Models via Prediction APIs</strong><br />
Florian Tramèr, Fan Zhang, Ari Juels, Michael K. Reiter, Thomas Ristenpart<br />
<em>arXiv</em> (2016-09-09) <a href="https://arxiv.org/abs/1609.02943v2">https://arxiv.org/abs/1609.02943v2</a></p>
</div>
<div id="ref-v8Lp4ibI">
<p>152. <strong>The Algorithmic Foundations of Differential Privacy</strong><br />
Cynthia Dwork, Aaron Roth<br />
<em>Foundations and Trends® in Theoretical Computer Science</em> (2013) <a href="https://doi.org/gcgmcw">https://doi.org/gcgmcw</a><br />
DOI: <a href="https://doi.org/10.1561/0400000042">10.1561/0400000042</a></p>
</div>
<div id="ref-1HbRTExaU">
<p>153. <strong>Membership Inference Attacks against Machine Learning Models</strong><br />
Reza Shokri, Marco Stronati, Congzheng Song, Vitaly Shmatikov<br />
<em>arXiv</em> (2016-10-18) <a href="https://arxiv.org/abs/1610.05820v2">https://arxiv.org/abs/1610.05820v2</a></p>
</div>
<div id="ref-6XtEfQMC">
<p>154. <strong>Enabling Privacy-Preserving GWASs in Heterogeneous Human Populations</strong><br />
Sean Simmons, Cenk Sahinalp, Bonnie Berger<br />
<em>Cell Systems</em> (2016-07) <a href="https://doi.org/gcgk72">https://doi.org/gcgk72</a><br />
DOI: <a href="https://doi.org/10.1016/j.cels.2016.04.013">10.1016/j.cels.2016.04.013</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27453444">27453444</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4994706">PMC4994706</a></p>
</div>
<div id="ref-LiCxcgZp">
<p>155. <strong>Deep Learning with Differential Privacy</strong><br />
Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, Li Zhang<br />
<em>Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security - CCS’16</em> (2016) <a href="https://doi.org/gcrnp3">https://doi.org/gcrnp3</a><br />
DOI: <a href="https://doi.org/10.1145/2976749.2978318">10.1145/2976749.2978318</a></p>
</div>
<div id="ref-xl1ijigK">
<p>156. <strong>Generating Multi-label Discrete Electronic Health Records using Generative Adversarial Networks</strong><br />
Edward Choi, Siddharth Biswal, Bradley Malin, Jon Duke, Walter F. Stewart, Jimeng Sun<br />
<em>arXiv</em> (2017-03-19) <a href="https://arxiv.org/abs/1703.06490v1">https://arxiv.org/abs/1703.06490v1</a></p>
</div>
<div id="ref-1988BRJe3">
<p>157. <strong>Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs</strong><br />
Cristóbal Esteban, Stephanie L. Hyland, Gunnar Rätsch<br />
<em>arXiv</em> (2017-06-08) <a href="https://arxiv.org/abs/1706.02633v1">https://arxiv.org/abs/1706.02633v1</a></p>
</div>
<div id="ref-fbIH12yd">
<p>158. <strong>Privacy-preserving generative deep neural networks support clinical data sharing</strong><br />
Brett K. Beaulieu-Jones, Zhiwei Steven Wu, Chris Williams, Ran Lee, Sanjeev P Bhavnani, James Brian Byrd, Casey S. Greene<br />
<em>Cold Spring Harbor Laboratory</em> (2017-07-05) <a href="https://doi.org/gcnzrn">https://doi.org/gcnzrn</a><br />
DOI: <a href="https://doi.org/10.1101/159756">10.1101/159756</a></p>
</div>
<div id="ref-U0ySdznJ">
<p>159. <strong>Communication-Efficient Learning of Deep Networks from Decentralized Data</strong><br />
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, Blaise Aguera y Arcas<br />
<em>Artificial Intelligence and Statistics</em> (2017-04-10) <a href="http://proceedings.mlr.press/v54/mcmahan17a.html">http://proceedings.mlr.press/v54/mcmahan17a.html</a></p>
</div>
<div id="ref-1GprsH3DV">
<p>160. <strong>Practical Secure Aggregation for Privacy Preserving Machine Learning</strong><br />
Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. Brendan McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, Karn Seth<br />
(2017) <a href="https://eprint.iacr.org/2017/281">https://eprint.iacr.org/2017/281</a></p>
</div>
<div id="ref-b8DJ1u6W">
<p>161. <strong>Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data</strong><br />
Nicolas Papernot, Martín Abadi, Úlfar Erlingsson, Ian Goodfellow, Kunal Talwar<br />
(2016-11-02) <a href="https://openreview.net/forum?id=HkwoSDPgg">https://openreview.net/forum?id=HkwoSDPgg</a></p>
</div>
<div id="ref-7yE9K08a">
<p>162. <strong>European Union regulations on algorithmic decision-making and a “right to explanation”</strong><br />
Bryce Goodman, Seth Flaxman<br />
<em>arXiv</em> (2016-06-28) <a href="https://arxiv.org/abs/1606.08813v3">https://arxiv.org/abs/1606.08813v3</a><br />
DOI: <a href="https://doi.org/10.1609/aimag.v38i3.2741">10.1609/aimag.v38i3.2741</a></p>
</div>
<div id="ref-10shRODux">
<p>163. <strong>Overcoming the Winner’s Curse: Estimating Penetrance Parameters from Case-Control Data</strong><br />
Sebastian Zöllner, Jonathan K. Pritchard<br />
<em>The American Journal of Human Genetics</em> (2007-04) <a href="https://doi.org/fk7jsx">https://doi.org/fk7jsx</a><br />
DOI: <a href="https://doi.org/10.1086/512821">10.1086/512821</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/17357068">17357068</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1852705">PMC1852705</a></p>
</div>
<div id="ref-sOBzMC57">
<p>164. <strong>Sex bias in neuroscience and biomedical research</strong><br />
Annaliese K. Beery, Irving Zucker<br />
<em>Neuroscience &amp; Biobehavioral Reviews</em> (2011-01) <a href="https://doi.org/ff34pz">https://doi.org/ff34pz</a><br />
DOI: <a href="https://doi.org/10.1016/j.neubiorev.2010.07.002">10.1016/j.neubiorev.2010.07.002</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/20620164">20620164</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3008499">PMC3008499</a></p>
</div>
<div id="ref-dKwyEWWF">
<p>165. <strong>Generalization and Dilution of Association Results from European GWAS in Populations of Non-European Ancestry: The PAGE Study</strong><br />
Christopher S. Carlson, Tara C. Matise, Kari E. North, Christopher A. Haiman, Megan D. Fesinmeyer, Steven Buyske, Fredrick R. Schumacher, Ulrike Peters, Nora Franceschini, Marylyn D. Ritchie, … <br />
<em>PLoS Biology</em> (2013-09-17) <a href="https://doi.org/f6b4xt">https://doi.org/f6b4xt</a><br />
DOI: <a href="https://doi.org/10.1371/journal.pbio.1001661">10.1371/journal.pbio.1001661</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/24068893">24068893</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3775722">PMC3775722</a></p>
</div>
<div id="ref-T3GG8iJN">
<p>166. <strong>New approaches to population stratification in genome-wide association studies</strong><br />
Alkes L. Price, Noah A. Zaitlen, David Reich, Nick Patterson<br />
<em>Nature Reviews Genetics</em> (2010-06-15) <a href="https://doi.org/bw853v">https://doi.org/bw853v</a><br />
DOI: <a href="https://doi.org/10.1038/nrg2813">10.1038/nrg2813</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/20548291">20548291</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2975875">PMC2975875</a></p>
</div>
<div id="ref-DmQPI43R">
<p>167. <strong>Retraction</strong><br />
P. Sebastiani, N. Solovieff, A. Puca, S. W. Hartley, E. Melista, S. Andersen, D. A. Dworkis, J. B. Wilk, R. H. Myers, M. H. Steinberg, … T. T. Perls<br />
<em>Science</em> (2011-07-21) <a href="https://doi.org/bn9rxq">https://doi.org/bn9rxq</a><br />
DOI: <a href="https://doi.org/10.1126/science.333.6041.404-a">10.1126/science.333.6041.404-a</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/21778381">21778381</a></p>
</div>
<div id="ref-889LsjDi">
<p>168. <strong>Leakage in data mining</strong><br />
Shachar Kaufman, Saharon Rosset, Claudia Perlich, Ori Stitelman<br />
<em>ACM Transactions on Knowledge Discovery from Data</em> (2012-12-01) <a href="https://doi.org/gcgmbv">https://doi.org/gcgmbv</a><br />
DOI: <a href="https://doi.org/10.1145/2382577.2382579">10.1145/2382577.2382579</a></p>
</div>
<div id="ref-6co0adq">
<p>169. <strong>To predict and serve?</strong><br />
Kristian Lum, William Isaac<br />
<em>Significance</em> (2016-10) <a href="https://doi.org/gcgmbk">https://doi.org/gcgmbk</a><br />
DOI: <a href="https://doi.org/10.1111/j.1740-9713.2016.00960.x">10.1111/j.1740-9713.2016.00960.x</a></p>
</div>
<div id="ref-1ENxzq6pT">
<p>170. <strong>Equality of Opportunity in Supervised Learning</strong><br />
Moritz Hardt, Eric Price, Nathan Srebro<br />
<em>arXiv</em> (2016-10-07) <a href="https://arxiv.org/abs/1610.02413v1">https://arxiv.org/abs/1610.02413v1</a></p>
</div>
<div id="ref-11aqfNfQx">
<p>171. <strong>Fair Algorithms for Infinite and Contextual Bandits</strong><br />
Matthew Joseph, Michael Kearns, Jamie Morgenstern, Seth Neel, Aaron Roth<br />
<em>arXiv</em> (2016-10-29) <a href="https://arxiv.org/abs/1610.09559v4">https://arxiv.org/abs/1610.09559v4</a></p>
</div>
<div id="ref-N96QKgly">
<p>172. <strong>The Framingham Heart Study and the epidemiology of cardiovascular disease: a historical perspective</strong><br />
Syed S Mahmood, Daniel Levy, Ramachandran S Vasan, Thomas J Wang<br />
<em>The Lancet</em> (2014-03) <a href="https://doi.org/f2qmmn">https://doi.org/f2qmmn</a><br />
DOI: <a href="https://doi.org/10.1016/s0140-6736(13)61752-3">10.1016/s0140-6736(13)61752-3</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/24084292">24084292</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4159698">PMC4159698</a></p>
</div>
<div id="ref-1FjSxrV1k">
<p>173. <strong>Children of the 90s: Coming of age</strong><br />
Helen Pearson<br />
<em>Nature</em> (2012-04) <a href="https://doi.org/gcgk79">https://doi.org/gcgk79</a><br />
DOI: <a href="https://doi.org/10.1038/484155a">10.1038/484155a</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/22498607">22498607</a></p>
</div>
<div id="ref-6RHepB1T">
<p>174. <strong>Nonparametric Estimation from Incomplete Observations</strong><br />
E. L. Kaplan, Paul Meier<br />
<em>Journal of the American Statistical Association</em> (1958-06) <a href="https://doi.org/fscrh2">https://doi.org/fscrh2</a><br />
DOI: <a href="https://doi.org/10.1080/01621459.1958.10501452">10.1080/01621459.1958.10501452</a></p>
</div>
<div id="ref-ogs3PPp7">
<p>175. <strong>Temporal disease trajectories condensed from population-wide registry data covering 6.2 million patients</strong><br />
Anders Boeck Jensen, Pope L. Moseley, Tudor I. Oprea, Sabrina Gade Ellesøe, Robert Eriksson, Henriette Schmock, Peter Bjødstrup Jensen, Lars Juhl Jensen, Søren Brunak<br />
<em>Nature Communications</em> (2014-06-24) <a href="https://doi.org/f2zs65">https://doi.org/f2zs65</a><br />
DOI: <a href="https://doi.org/10.1038/ncomms5022">10.1038/ncomms5022</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/24959948">24959948</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4090719">PMC4090719</a></p>
</div>
<div id="ref-Ohd1Q9Xw">
<p>176. <strong>Deepr: A Convolutional Net for Medical Records</strong><br />
Phuoc Nguyen, Truyen Tran, Nilmini Wickramasinghe, Svetha Venkatesh<br />
<em>arXiv</em> (2016-07-26) <a href="https://arxiv.org/abs/1607.07519v1">https://arxiv.org/abs/1607.07519v1</a></p>
</div>
<div id="ref-ru0hjGeQ">
<p>177. <strong>Curiosity Creates Cures: The Value and Impact of Basic Research</strong><br />
NIH<br />
(2012-05) <a href="https://www.nigms.nih.gov/Education/Documents/curiosity.pdf">https://www.nigms.nih.gov/Education/Documents/curiosity.pdf</a></p>
</div>
<div id="ref-8SMDF816">
<p>178. <strong>Multi-omics integration accurately predicts cellular state in unexplored conditions for Escherichia coli</strong><br />
Minseung Kim, Navneet Rai, Violeta Zorraquino, Ilias Tagkopoulos<br />
<em>Nature Communications</em> (2016-10-07) <a href="https://doi.org/gcgk8c">https://doi.org/gcgk8c</a><br />
DOI: <a href="https://doi.org/10.1038/ncomms13090">10.1038/ncomms13090</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27713404">27713404</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5059772">PMC5059772</a></p>
</div>
<div id="ref-rmjDc5rm">
<p>179. <strong>Trans-species learning of cellular signaling systems with bimodal deep belief networks</strong><br />
Lujia Chen, Chunhui Cai, Vicky Chen, Xinghua Lu<br />
<em>Bioinformatics</em> (2015-05-20) <a href="https://doi.org/f7sb7n">https://doi.org/f7sb7n</a><br />
DOI: <a href="https://doi.org/10.1093/bioinformatics/btv315">10.1093/bioinformatics/btv315</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/25995230">25995230</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4668779">PMC4668779</a></p>
</div>
<div id="ref-AnenJOuU">
<p>180. <strong>Learning structure in gene expression data using deep architectures, with an application to gene clustering</strong><br />
Aman Gupta, Haohan Wang, Madhavi Ganapathiraju<br />
<em>Cold Spring Harbor Laboratory</em> (2015-11-16) <a href="https://doi.org/gcgk84">https://doi.org/gcgk84</a><br />
DOI: <a href="https://doi.org/10.1101/031906">10.1101/031906</a></p>
</div>
<div id="ref-yVBx9Qx4">
<p>181. <strong>Learning a hierarchical representation of the yeast transcriptomic machinery using an autoencoder model</strong><br />
Lujia Chen, Chunhui Cai, Vicky Chen, Xinghua Lu<br />
<em>BMC Bioinformatics</em> (2016-01-11) <a href="https://doi.org/gcgmb8">https://doi.org/gcgmb8</a><br />
DOI: <a href="https://doi.org/10.1186/s12859-015-0852-1">10.1186/s12859-015-0852-1</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26818848">26818848</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4895523">PMC4895523</a></p>
</div>
<div id="ref-1CFhfCyWN">
<p>182. <strong>ADAGE-Based Integration of Publicly Available Pseudomonas aeruginosa Gene Expression Data with Denoising Autoencoders Illuminates Microbe-Host Interactions</strong><br />
Jie Tan, John H. Hammond, Deborah A. Hogan, Casey S. Greene<br />
<em>mSystems</em> (2016-01-19) <a href="https://doi.org/gcgmbq">https://doi.org/gcgmbq</a><br />
DOI: <a href="https://doi.org/10.1128/msystems.00025-15">10.1128/msystems.00025-15</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27822512">27822512</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5069748">PMC5069748</a></p>
</div>
<div id="ref-zuLdSQx3">
<p>183. <strong>Unsupervised extraction of stable expression signatures from public compendia with eADAGE</strong><br />
Jie Tan, Georgia Doing, Kimberley A Lewis, Courtney E Price, Kathleen M Chen, Kyle C Cady, Barret Perchuk, Michael T Laub, Deborah A Hogan, Casey S Greene<br />
<em>Cold Spring Harbor Laboratory</em> (2016-10-03) <a href="https://doi.org/gcgk9c">https://doi.org/gcgk9c</a><br />
DOI: <a href="https://doi.org/10.1101/078659">10.1101/078659</a></p>
</div>
<div id="ref-12QQw9p7v">
<p>184. <strong>Gene expression inference with deep learning</strong><br />
Yifei Chen, Yi Li, Rajiv Narayan, Aravind Subramanian, Xiaohui Xie<br />
<em>Bioinformatics</em> (2016-02-11) <a href="https://doi.org/f8vmtt">https://doi.org/f8vmtt</a><br />
DOI: <a href="https://doi.org/10.1093/bioinformatics/btw074">10.1093/bioinformatics/btw074</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26873929">26873929</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4908320">PMC4908320</a></p>
</div>
<div id="ref-G10wkFHt">
<p>185. <strong>DeepChrome: Deep-learning for predicting gene expression from histone modifications</strong><br />
Ritambhara Singh, Jack Lanchantin, Gabriel Robins, Yanjun Qi<br />
<em>arXiv</em> (2016-07-07) <a href="https://arxiv.org/abs/1607.02078v1">https://arxiv.org/abs/1607.02078v1</a></p>
</div>
<div id="ref-16MNknNBL">
<p>186. <strong>Attend and Predict: Understanding Gene Regulation by Selective Attention on Chromatin</strong><br />
Ritambhara Singh, Jack Lanchantin, Arshdeep Sekhon, Yanjun Qi<br />
<em>arXiv</em> (2017-08-01) <a href="https://arxiv.org/abs/1708.00339v3">https://arxiv.org/abs/1708.00339v3</a></p>
</div>
<div id="ref-1EtavGKI4">
<p>187. <strong>Integrative Data Analysis of Multi-Platform Cancer Data with a Multimodal Deep Learning Approach</strong><br />
Muxuan Liang, Zhizhong Li, Ting Chen, Jianyang Zeng<br />
<em>IEEE/ACM Transactions on Computational Biology and Bioinformatics</em> (2015-07-01) <a href="https://doi.org/gcgmbd">https://doi.org/gcgmbd</a><br />
DOI: <a href="https://doi.org/10.1109/tcbb.2014.2377729">10.1109/tcbb.2014.2377729</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26357333">26357333</a></p>
</div>
<div id="ref-17sl0Ti0w">
<p>188. <strong>DeepSignal: detecting DNA methylation state from Nanopore sequencing reads using deep-learning</strong><br />
Peng Ni, Neng Huang, Feng Luo, Jianxin Wang<br />
<em>Cold Spring Harbor Laboratory</em> (2018-08-06) <a href="https://doi.org/gf2f8t">https://doi.org/gf2f8t</a><br />
DOI: <a href="https://doi.org/10.1101/385849">10.1101/385849</a></p>
</div>
<div id="ref-19EJTHByG">
<p>189. <strong>DeepCpG: accurate prediction of single-cell DNA methylation states using deep learning</strong><br />
Christof Angermueller, Heather J. Lee, Wolf Reik, Oliver Stegle<br />
<em>Genome Biology</em> (2017-04-11) <a href="https://doi.org/gcgmcc">https://doi.org/gcgmcc</a><br />
DOI: <a href="https://doi.org/10.1186/s13059-017-1189-z">10.1186/s13059-017-1189-z</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28395661">28395661</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5387360">PMC5387360</a></p>
</div>
<div id="ref-WWSBQM3">
<p>190. <strong>A deep learning framework for imputing missing values in genomic data</strong><br />
Yeping Lina Qiu, Hong Zheng, Olivier Gevaert<br />
<em>Cold Spring Harbor Laboratory</em> (2018-08-31) <a href="https://doi.org/gf2f8v">https://doi.org/gf2f8v</a><br />
DOI: <a href="https://doi.org/10.1101/406066">10.1101/406066</a></p>
</div>
<div id="ref-NzYX9e9i">
<p>191. <strong>MRCNN: a deep learning model for regression of genome-wide DNA methylation</strong><br />
Qi Tian, Jianxiao Zou, Jianxiong Tang, Yuan Fang, Zhongli Yu, Shicai Fan<br />
<em>BMC Genomics</em> (2019-04) <a href="https://doi.org/gf48g6">https://doi.org/gf48g6</a><br />
DOI: <a href="https://doi.org/10.1186/s12864-019-5488-5">10.1186/s12864-019-5488-5</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/30967120">30967120</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6457069">PMC6457069</a></p>
</div>
<div id="ref-gXQfsrAl">
<p>192. <strong>A deep belief network system for prediction of DNA methylation</strong><br />
Mohammed Khwaja, Melpomeni Kalofonou, Chris Toumazou<br />
<em>2017 IEEE Biomedical Circuits and Systems Conference (BioCAS)</em> (2017-10) <a href="https://doi.org/gf2f8z">https://doi.org/gf2f8z</a><br />
DOI: <a href="https://doi.org/10.1109/biocas.2017.8325078">10.1109/biocas.2017.8325078</a></p>
</div>
<div id="ref-1AfEMOdlu">
<p>193. <strong>Predicting DNA Methylation State of CpG Dinucleotide Using Genome Topological Features and Deep Networks</strong><br />
Yiheng Wang, Tong Liu, Dong Xu, Huidong Shi, Chaoyang Zhang, Yin-Yuan Mo, Zheng Wang<br />
<em>Scientific Reports</em> (2016-01-22) <a href="https://doi.org/f77m9h">https://doi.org/f77m9h</a><br />
DOI: <a href="https://doi.org/10.1038/srep19598">10.1038/srep19598</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26797014">26797014</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4726425">PMC4726425</a></p>
</div>
<div id="ref-fUGhxx3W">
<p>194. <strong>Predicting DNA methylation states with hybrid information based deep-learning model</strong><br />
Laiyi Fu, Qinke Peng, Ling Chai<br />
<em>IEEE/ACM Transactions on Computational Biology and Bioinformatics</em> (2019) <a href="https://doi.org/gf2f82">https://doi.org/gf2f82</a><br />
DOI: <a href="https://doi.org/10.1109/tcbb.2019.2909237">10.1109/tcbb.2019.2909237</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/30951477">30951477</a></p>
</div>
<div id="ref-L0qjpS0U">
<p>195. <strong>E2M: A Deep Learning Framework for Associating Combinatorial Methylation Patterns with Gene Expression: Supplementary</strong><br />
Jianhao Peng, Idoia Ochoa, Olgica Milenkovic<br />
<em>Cold Spring Harbor Laboratory</em> (2019-01-22) <a href="https://doi.org/gf2hjm">https://doi.org/gf2hjm</a><br />
DOI: <a href="https://doi.org/10.1101/527044">10.1101/527044</a></p>
</div>
<div id="ref-1szm3mxk">
<p>196. <strong>Predicting Methylation from Sequence and Gene Expression Using Deep Learning with Attention</strong><br />
Alona Levy-Jurgenson, Xavier Tekpli, Vessela N. Kristensen, Zohar Yakhini<br />
<em>Cold Spring Harbor Laboratory</em> (2018-12-09) <a href="https://doi.org/gf2f8x">https://doi.org/gf2f8x</a><br />
DOI: <a href="https://doi.org/10.1101/491357">10.1101/491357</a></p>
</div>
<div id="ref-GbgdMBjb">
<p>197. <strong>D-GPM: a deep learning method for gene promoter methylation inference</strong><br />
Xingxin Pan, Biao Liu, Xingzhao Wen, Yulu Liu, Xiuqing Zhang, Shengbin Li, Shuaicheng Li<br />
<em>Cold Spring Harbor Laboratory</em> (2018-10-09) <a href="https://doi.org/gfh798">https://doi.org/gfh798</a><br />
DOI: <a href="https://doi.org/10.1101/438218">10.1101/438218</a></p>
</div>
<div id="ref-DSTA8ohX">
<p>198. <strong>Deep Recurrent Attention Models for Histopathological Image Analysis</strong><br />
Alexandre Momeni, Marc Thibault, Olivier Gevaert<br />
<em>Cold Spring Harbor Laboratory</em> (2018-10-14) <a href="https://doi.org/gf2f8w">https://doi.org/gf2f8w</a><br />
DOI: <a href="https://doi.org/10.1101/438341">10.1101/438341</a></p>
</div>
<div id="ref-54x3LbFb">
<p>199. <strong>Residual Deep Convolutional Neural Network Predicts MGMT Methylation Status</strong><br />
Panagiotis Korfiatis, Timothy L. Kline, Daniel H. Lachance, Ian F. Parney, Jan C. Buckner, Bradley J. Erickson<br />
<em>Journal of Digital Imaging</em> (2017-08-07) <a href="https://doi.org/gf2f8q">https://doi.org/gf2f8q</a><br />
DOI: <a href="https://doi.org/10.1007/s10278-017-0009-z">10.1007/s10278-017-0009-z</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28785873">28785873</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5603430">PMC5603430</a></p>
</div>
<div id="ref-1GRpO6k1o">
<p>200. <strong>A deep neural network based regression model for triglyceride concentrations prediction using epigenome-wide DNA methylation profiles</strong><br />
Md. Mohaiminul Islam, Ye Tian, Yan Cheng, Yang Wang, Pingzhao Hu<br />
<em>BMC Proceedings</em> (2018-09) <a href="https://doi.org/gf2f84">https://doi.org/gf2f84</a><br />
DOI: <a href="https://doi.org/10.1186/s12919-018-0121-1">10.1186/s12919-018-0121-1</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/30263040">30263040</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157031">PMC6157031</a></p>
</div>
<div id="ref-bnD7Q1cQ">
<p>201. <strong>Data mining and machine learning approaches for the integration of genome-wide association and methylation data: methodology and main conclusions from GAW20</strong><br />
Burcu Darst, Corinne D. Engelman, Ye Tian, Justo Lorenzo Bermejo<br />
<em>BMC Genetics</em> (2018-09) <a href="https://doi.org/gf2f83">https://doi.org/gf2f83</a><br />
DOI: <a href="https://doi.org/10.1186/s12863-018-0646-3">10.1186/s12863-018-0646-3</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/30255774">30255774</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157271">PMC6157271</a></p>
</div>
<div id="ref-GLXmFOus">
<p>202. <strong>Convolutional Neural Networks In Classifying Cancer Through DNA Methylation</strong><br />
Soham Chatterjee, Archana Iyer, Satya Avva, Abhai Kollara, Malaikannan Sankarasubbu<br />
<em>arXiv</em> (2018-07-24) <a href="https://arxiv.org/abs/1807.09617v1">https://arxiv.org/abs/1807.09617v1</a></p>
</div>
<div id="ref-EXGRa0DD">
<p>203. <strong>A Deep Autoencoder System for Differentiation of Cancer Types Based on DNA Methylation State</strong><br />
Mohammed Khwaja, Melpomeni Kalofonou, Chris Toumazou<br />
<em>arXiv</em> (2018-10-02) <a href="https://arxiv.org/abs/1810.01243v2">https://arxiv.org/abs/1810.01243v2</a></p>
</div>
<div id="ref-epufolpH">
<p>204. <strong>MethylNet: A Modular Deep Learning Approach to Methylation Prediction</strong><br />
Joshua J. Levy, Alexander J. Titus, Curtis L. Petersen, Youdinghuan Chen, Lucas A. Salas, Brock C. Christensen<br />
<em>Cold Spring Harbor Laboratory</em> (2019-07-04) <a href="https://doi.org/gf48g5">https://doi.org/gf48g5</a><br />
DOI: <a href="https://doi.org/10.1101/692665">10.1101/692665</a></p>
</div>
<div id="ref-1CvlEI6Kb">
<p>205. <strong>Deep Neural Network for Analysis of DNA Methylation Data</strong><br />
Hong Yu, Zhanyu Ma<br />
<em>arXiv</em> (2018-08-02) <a href="https://arxiv.org/abs/1808.01359v1">https://arxiv.org/abs/1808.01359v1</a></p>
</div>
<div id="ref-71snAYRy">
<p>206. <strong>A New Dimension of Breast Cancer Epigenetics - Applications of Variational Autoencoders with DNA Methylation</strong><br />
Alexander J. Titus, Carly A. Bobak, Brock C. Christensen<br />
<em>Proceedings of the 11th International Joint Conference on Biomedical Engineering Systems and Technologies</em> (2018) <a href="https://doi.org/gd58sg">https://doi.org/gd58sg</a><br />
DOI: <a href="https://doi.org/10.5220/0006636401400145">10.5220/0006636401400145</a></p>
</div>
<div id="ref-7V2oCvtf">
<p>207. <strong>Extracting a biologically relevant latent space from cancer transcriptomes with variational autoencoders</strong><br />
Gregory P. Way, Casey S. Greene<br />
<em>Biocomputing 2018</em> (2017-11-17) <a href="https://doi.org/gfspsd">https://doi.org/gfspsd</a><br />
DOI: <a href="https://doi.org/10.1142/9789813235533_0008">10.1142/9789813235533_0008</a></p>
</div>
<div id="ref-7pBVQEZ4">
<p>208. <strong>Unsupervised deep learning with variational autoencoders applied to breast tumor genome-wide DNA methylation data with biologic feature extraction</strong><br />
Alexander J. Titus, Owen M. Wilkins, Carly A. Bobak, Brock C. Christensen<br />
<em>Cold Spring Harbor Laboratory</em> (2018-10-02) <a href="https://doi.org/gf2hfb">https://doi.org/gf2hfb</a><br />
DOI: <a href="https://doi.org/10.1101/433763">10.1101/433763</a></p>
</div>
<div id="ref-s3I1CsTr">
<p>209. <strong>Exploring DNA Methylation Data of Lung Cancer Samples with Variational Autoencoders</strong><br />
Zhenxing Wang, Yadong Wang<br />
<em>2018 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</em> (2018-12) <a href="https://doi.org/gf2hff">https://doi.org/gf2hff</a><br />
DOI: <a href="https://doi.org/10.1109/bibm.2018.8621365">10.1109/bibm.2018.8621365</a></p>
</div>
<div id="ref-5CsWRjfp">
<p>210. <strong>Parameter tuning is a key part of dimensionality reduction via deep variational autoencoders for single cell RNA transcriptomics</strong><br />
Qiwen Hu, Casey S. Greene<br />
<em>Cold Spring Harbor Laboratory</em> (2018-08-05) <a href="https://doi.org/gdxxjf">https://doi.org/gdxxjf</a><br />
DOI: <a href="https://doi.org/10.1101/385534">10.1101/385534</a></p>
</div>
<div id="ref-QFK6GapR">
<p>211. <strong>RNA mis-splicing in disease</strong><br />
Marina M. Scotti, Maurice S. Swanson<br />
<em>Nature Reviews Genetics</em> (2015-11-23) <a href="https://doi.org/gcgk8m">https://doi.org/gcgk8m</a><br />
DOI: <a href="https://doi.org/10.1038/nrg.2015.3">10.1038/nrg.2015.3</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26593421">26593421</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5993438">PMC5993438</a></p>
</div>
<div id="ref-b6p6wxpC">
<p>212. <strong>RNA splicing is a primary link between genetic variation and disease</strong><br />
Yang I. Li, Bryce van de Geijn, Anil Raj, David A. Knowles, Allegra A. Petti, David Golan, Yoav Gilad, Jonathan K. Pritchard<br />
<em>Science</em> (2016-04-28) <a href="https://doi.org/f8j95d">https://doi.org/f8j95d</a><br />
DOI: <a href="https://doi.org/10.1126/science.aad9417">10.1126/science.aad9417</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27126046">27126046</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5182069">PMC5182069</a></p>
</div>
<div id="ref-11ETDdRKr">
<p>213. <strong>Deciphering the splicing code</strong><br />
Yoseph Barash, John A. Calarco, Weijun Gao, Qun Pan, Xinchen Wang, Ofer Shai, Benjamin J. Blencowe, Brendan J. Frey<br />
<em>Nature</em> (2010-05) <a href="https://doi.org/cvk585">https://doi.org/cvk585</a><br />
DOI: <a href="https://doi.org/10.1038/nature09000">10.1038/nature09000</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/20445623">20445623</a></p>
</div>
<div id="ref-8VPGUHcf">
<p>214. <strong>Bayesian prediction of tissue-regulated splicing using RNA sequence and cellular context</strong><br />
Hui Yuan Xiong, Yoseph Barash, Brendan J. Frey<br />
<em>Bioinformatics</em> (2011-07-29) <a href="https://doi.org/c9jnt2">https://doi.org/c9jnt2</a><br />
DOI: <a href="https://doi.org/10.1093/bioinformatics/btr444">10.1093/bioinformatics/btr444</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/21803804">21803804</a></p>
</div>
<div id="ref-17sgPdcMT">
<p>215. <strong>The human splicing code reveals new insights into the genetic determinants of disease</strong><br />
H. Y. Xiong, B. Alipanahi, L. J. Lee, H. Bretschneider, D. Merico, R. K. C. Yuen, Y. Hua, S. Gueroussov, H. S. Najafabadi, T. R. Hughes, … B. J. Frey<br />
<em>Science</em> (2014-12-18) <a href="https://doi.org/f6wzj2">https://doi.org/f6wzj2</a><br />
DOI: <a href="https://doi.org/10.1126/science.1254806">10.1126/science.1254806</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/25525159">25525159</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4362528">PMC4362528</a></p>
</div>
<div id="ref-N0HBi8MH">
<p>216. <strong>Integrative Deep Models for Alternative Splicing</strong><br />
Anupama Jha, Matthew R Gazzara, Yoseph Barash<br />
<em>Cold Spring Harbor Laboratory</em> (2017-01-31) <a href="https://doi.org/gcgk9v">https://doi.org/gcgk9v</a><br />
DOI: <a href="https://doi.org/10.1101/104869">10.1101/104869</a></p>
</div>
<div id="ref-Qbtqlmhf">
<p>217. <strong>Imputation for transcription factor binding predictions based on deep learning</strong><br />
Qian Qin, Jianxing Feng<br />
<em>PLOS Computational Biology</em> (2017-02-24) <a href="https://doi.org/f9rgsc">https://doi.org/f9rgsc</a><br />
DOI: <a href="https://doi.org/10.1371/journal.pcbi.1005403">10.1371/journal.pcbi.1005403</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28234893">28234893</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5345877">PMC5345877</a></p>
</div>
<div id="ref-mlqKTlZY">
<p>218. <strong>Learning the Sequence Determinants of Alternative Splicing from Millions of Random Sequences</strong><br />
Alexander B. Rosenberg, Rupali P. Patwardhan, Jay Shendure, Georg Seelig<br />
<em>Cell</em> (2015-10) <a href="https://doi.org/f7x2wb">https://doi.org/f7x2wb</a><br />
DOI: <a href="https://doi.org/10.1016/j.cell.2015.09.054">10.1016/j.cell.2015.09.054</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26496609">26496609</a></p>
</div>
<div id="ref-CNz9HwZ3">
<p>219. <strong>MECHANISMS IN ENDOCRINOLOGY: Alternative splicing: the new frontier in diabetes research</strong><br />
Jonàs Juan-Mateu, Olatz Villate, Décio L Eizirik<br />
<em>European Journal of Endocrinology</em> (2016-05) <a href="https://doi.org/f8nccm">https://doi.org/f8nccm</a><br />
DOI: <a href="https://doi.org/10.1530/eje-15-0916">10.1530/eje-15-0916</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26628584">26628584</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5331159">PMC5331159</a></p>
</div>
<div id="ref-15c9V9Hm1">
<p>220. <strong>Absence of a simple code: how transcription factors read the genome</strong><br />
Matthew Slattery, Tianyin Zhou, Lin Yang, Ana Carolina Dantas Machado, Raluca Gordân, Remo Rohs<br />
<em>Trends in Biochemical Sciences</em> (2014-09) <a href="https://doi.org/xrn">https://doi.org/xrn</a><br />
DOI: <a href="https://doi.org/10.1016/j.tibs.2014.07.002">10.1016/j.tibs.2014.07.002</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/25129887">25129887</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4149858">PMC4149858</a></p>
</div>
<div id="ref-15J98V2qM">
<p>221. <strong>An integrated encyclopedia of DNA elements in the human genome</strong><em>Nature</em> (2012-09) <a href="https://doi.org/bg9d">https://doi.org/bg9d</a><br />
DOI: <a href="https://doi.org/10.1038/nature11247">10.1038/nature11247</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/22955616">22955616</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3439153">PMC3439153</a></p>
</div>
<div id="ref-ywDQIvZJ">
<p>222. <strong>DNA binding sites: representation and discovery</strong><br />
G. D. Stormo<br />
<em>Bioinformatics</em> (2000-01-01) <a href="https://doi.org/bdqmmh">https://doi.org/bdqmmh</a><br />
DOI: <a href="https://doi.org/10.1093/bioinformatics/16.1.16">10.1093/bioinformatics/16.1.16</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/10812473">10812473</a></p>
</div>
<div id="ref-dwj6qSn3">
<p>223. <strong>MEME SUITE: tools for motif discovery and searching</strong><br />
T. L. Bailey, M. Boden, F. A. Buske, M. Frith, C. E. Grant, L. Clementi, J. Ren, W. W. Li, W. S. Noble<br />
<em>Nucleic Acids Research</em> (2009-05-20) <a href="https://doi.org/bzkmbr">https://doi.org/bzkmbr</a><br />
DOI: <a href="https://doi.org/10.1093/nar/gkp335">10.1093/nar/gkp335</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/19458158">19458158</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2703892">PMC2703892</a></p>
</div>
<div id="ref-fv4f2GgS">
<p>224. <strong>Evaluation of methods for modeling transcription factor sequence specificity</strong><br />
Matthew T WeirauchAtina Cote, Raquel Norel, Matti Annala, Yue Zhao, Todd R Riley, Julio Saez-Rodriguez, Thomas Cokelaer, Anastasia Vedenko, … Timothy R Hughes<br />
<em>Nature Biotechnology</em> (2013-01-27) <a href="https://doi.org/f4pprj">https://doi.org/f4pprj</a><br />
DOI: <a href="https://doi.org/10.1038/nbt.2486">10.1038/nbt.2486</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/23354101">23354101</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3687085">PMC3687085</a></p>
</div>
<div id="ref-EPo6SPty">
<p>225. <strong>High Resolution Models of Transcription Factor-DNA Affinities Improve In Vitro and In Vivo Binding Predictions</strong><br />
Phaedra Agius, Aaron Arvey, William Chang, William Stafford Noble, Christina Leslie<br />
<em>PLoS Computational Biology</em> (2010-09-09) <a href="https://doi.org/c8n9s7">https://doi.org/c8n9s7</a><br />
DOI: <a href="https://doi.org/10.1371/journal.pcbi.1000916">10.1371/journal.pcbi.1000916</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/20838582">20838582</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2936517">PMC2936517</a></p>
</div>
<div id="ref-JxuQvvyk">
<p>226. <strong>Enhanced Regulatory Sequence Prediction Using Gapped k-mer Features</strong><br />
Mahmoud Ghandi, Dongwon Lee, Morteza Mohammad-Noori, Michael A. Beer<br />
<em>PLoS Computational Biology</em> (2014-07-17) <a href="https://doi.org/gcgmcf">https://doi.org/gcgmcf</a><br />
DOI: <a href="https://doi.org/10.1371/journal.pcbi.1003711">10.1371/journal.pcbi.1003711</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/25033408">25033408</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4102394">PMC4102394</a></p>
</div>
<div id="ref-jJHZHWrl">
<p>227. <strong>Predicting the sequence specificities of DNA- and RNA-binding proteins by deep learning</strong><br />
Babak Alipanahi, Andrew Delong, Matthew T Weirauch, Brendan J Frey<br />
<em>Nature Biotechnology</em> (2015-07-27) <a href="https://doi.org/f7mkrd">https://doi.org/f7mkrd</a><br />
DOI: <a href="https://doi.org/10.1038/nbt.3300">10.1038/nbt.3300</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26213851">26213851</a></p>
</div>
<div id="ref-qnKdqG0P">
<p>228. <strong>RNA-protein binding motifs mining with a new hybrid deep learning based cross-domain knowledge integration approach</strong><br />
Xiaoyong Pan, Hong-Bin Shen<br />
<em>BMC Bioinformatics</em> (2017-02-28) <a href="https://doi.org/gcgmb9">https://doi.org/gcgmb9</a><br />
DOI: <a href="https://doi.org/10.1186/s12859-017-1561-8">10.1186/s12859-017-1561-8</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28245811">28245811</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5331642">PMC5331642</a></p>
</div>
<div id="ref-182UhQqzp">
<p>229. <strong>Convolutional neural network architectures for predicting DNA–protein binding</strong><br />
Haoyang Zeng, Matthew D. Edwards, Ge Liu, David K. Gifford<br />
<em>Bioinformatics</em> (2016-06-15) <a href="https://doi.org/gcgk8z">https://doi.org/gcgk8z</a><br />
DOI: <a href="https://doi.org/10.1093/bioinformatics/btw255">10.1093/bioinformatics/btw255</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27307608">27307608</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4908339">PMC4908339</a></p>
</div>
<div id="ref-Dwi2eAvT">
<p>230. <strong>Deep Motif Dashboard: Visualizing and Understanding Genomic Sequences Using Deep Neural Networks</strong><br />
Jack Lanchantin, Ritambhara Singh, Beilun Wang, Yanjun Qi<br />
<em>arXiv</em> (2016-08-12) <a href="https://arxiv.org/abs/1608.03644v4">https://arxiv.org/abs/1608.03644v4</a></p>
</div>
<div id="ref-6Nw5JrLI">
<p>231. <strong>Convolutional Kitchen Sinks for Transcription Factor Binding Site Prediction</strong><br />
Alyssa Morrow, Vaishaal Shankar, Devin Petersohn, Anthony Joseph, Benjamin Recht, Nir Yosef<br />
<em>arXiv</em> (2017-05-31) <a href="https://arxiv.org/abs/1706.00125v1">https://arxiv.org/abs/1706.00125v1</a></p>
</div>
<div id="ref-1GOS0CRta">
<p>232. <strong>Biological Sequence Modeling with Convolutional Kernel Networks</strong><br />
Dexiong Chen, Laurent Jacob, Julien Mairal<br />
<em>Cold Spring Harbor Laboratory</em> (2017-11-10) <a href="https://doi.org/gcsmgj">https://doi.org/gcsmgj</a><br />
DOI: <a href="https://doi.org/10.1101/217257">10.1101/217257</a></p>
</div>
<div id="ref-iEmvzeT8">
<p>233. <strong>Reverse-complement parameter sharing improves deep learning models for genomics</strong><br />
Avanti Shrikumar, Peyton Greenside, Anshul Kundaje<br />
<em>Cold Spring Harbor Laboratory</em> (2017-01-27) <a href="https://doi.org/gcgk9t">https://doi.org/gcgk9t</a><br />
DOI: <a href="https://doi.org/10.1101/103663">10.1101/103663</a></p>
</div>
<div id="ref-Zo0D80FN">
<p>234. <strong>Separable Fully Connected Layers Improve Deep Learning Models For Genomics</strong><br />
Amr Mohamed Alexandari, Avanti Shrikumar, Anshul Kundaje<br />
<em>Cold Spring Harbor Laboratory</em> (2017-06-05) <a href="https://doi.org/gcsmgh">https://doi.org/gcsmgh</a><br />
DOI: <a href="https://doi.org/10.1101/146431">10.1101/146431</a></p>
</div>
<div id="ref-2UI1BZuD">
<p>235. <strong>Predicting effects of noncoding variants with deep learning–based sequence model</strong><br />
Jian Zhou, Olga G Troyanskaya<br />
<em>Nature Methods</em> (2015-08-24) <a href="https://doi.org/gcgk8g">https://doi.org/gcgk8g</a><br />
DOI: <a href="https://doi.org/10.1038/nmeth.3547">10.1038/nmeth.3547</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26301843">26301843</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4768299">PMC4768299</a></p>
</div>
<div id="ref-iBdkksok">
<p>236. <strong>DanQ: a hybrid convolutional and recurrent deep neural network for quantifying the function of DNA sequences</strong><br />
Daniel Quang, Xiaohui Xie<br />
<em>Nucleic Acids Research</em> (2016-04-15) <a href="https://doi.org/f8v4wj">https://doi.org/f8v4wj</a><br />
DOI: <a href="https://doi.org/10.1093/nar/gkw226">10.1093/nar/gkw226</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27084946">27084946</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4914104">PMC4914104</a></p>
</div>
<div id="ref-gsSLr9vf">
<p>237. <strong>Sequence and chromatin determinants of cell-type-specific transcription factor binding</strong><br />
A. Arvey, P. Agius, W. S. Noble, C. Leslie<br />
<em>Genome Research</em> (2012-09-01) <a href="https://doi.org/f37s7h">https://doi.org/f37s7h</a><br />
DOI: <a href="https://doi.org/10.1101/gr.127712.111">10.1101/gr.127712.111</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/22955984">22955984</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3431489">PMC3431489</a></p>
</div>
<div id="ref-rOo5pTPS">
<p>238. <strong>Analysis of computational footprinting methods for DNase sequencing experiments</strong><br />
Eduardo G Gusmao, Manuel Allhoff, Martin Zenke, Ivan G Costa<br />
<em>Nature Methods</em> (2016-02-22) <a href="https://doi.org/f8hz8z">https://doi.org/f8hz8z</a><br />
DOI: <a href="https://doi.org/10.1038/nmeth.3772">10.1038/nmeth.3772</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26901649">26901649</a></p>
</div>
<div id="ref-wW6QbBXz">
<p>239. <strong>ENCODE-DREAM in vivo Transcription Factor Binding Site Prediction Challenge</strong>(2017) <a href="https://www.synapse.org/#!Synapse:syn6131484/wiki/402026">https://www.synapse.org/#!Synapse:syn6131484/wiki/402026</a></p>
</div>
<div id="ref-BguHMHkG">
<p>240. <strong>FactorNet: a deep learning framework for predicting cell type specific transcription factor binding from nucleotide-resolution sequential data</strong><br />
Daniel Quang, Xiaohui Xie<br />
<em>Cold Spring Harbor Laboratory</em> (2017-06-18) <a href="https://doi.org/gcpvb3">https://doi.org/gcpvb3</a><br />
DOI: <a href="https://doi.org/10.1101/151274">10.1101/151274</a></p>
</div>
<div id="ref-pZqk9gDB">
<p>241. <strong>Learning from mistakes: Accurate prediction of cell type-specific transcription factor binding</strong><br />
Jens Keilwagen, Stefan Posch, Jan Grau<br />
<em>Cold Spring Harbor Laboratory</em> (2017-12-06) <a href="https://doi.org/gcsmgk">https://doi.org/gcsmgk</a><br />
DOI: <a href="https://doi.org/10.1101/230011">10.1101/230011</a></p>
</div>
<div id="ref-Nc8y8wkO">
<p>242. <strong>Transfer String Kernel for Cross-Context DNA-Protein Binding Prediction</strong><br />
Ritambhara Singh, Jack Lanchantin, Gabriel Robins, Yanjun Qi<br />
<em>IEEE/ACM Transactions on Computational Biology and Bioinformatics</em> (2018) <a href="https://doi.org/gcsmhp">https://doi.org/gcsmhp</a><br />
DOI: <a href="https://doi.org/10.1109/tcbb.2016.2609918">10.1109/tcbb.2016.2609918</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27654939">27654939</a></p>
</div>
<div id="ref-iWxvn0xF">
<p>243. <strong>Learning Transferable Features with Deep Adaptation Networks</strong><br />
Mingsheng Long, Yue Cao, Jianmin Wang, Michael I. Jordan<br />
<em>arXiv</em> (2015-02-10) <a href="https://arxiv.org/abs/1502.02791v2">https://arxiv.org/abs/1502.02791v2</a></p>
</div>
<div id="ref-nu0eLZr0">
<p>244. <strong>Domain-Adversarial Training of Neural Networks</strong><br />
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, Victor Lempitsky<br />
<em>arXiv</em> (2015-05-28) <a href="https://arxiv.org/abs/1505.07818v4">https://arxiv.org/abs/1505.07818v4</a></p>
</div>
<div id="ref-zhmq9ktJ">
<p>245. <strong>Learning Important Features Through Propagating Activation Differences</strong><br />
Avanti Shrikumar, Peyton Greenside, Anshul Kundaje<br />
<em>arXiv</em> (2017-04-10) <a href="https://arxiv.org/abs/1704.02685v1">https://arxiv.org/abs/1704.02685v1</a></p>
</div>
<div id="ref-19jjiGHWc">
<p>246. <strong>The state of the art of mammalian promoter recognition</strong><br />
T. Werner<br />
<em>Briefings in Bioinformatics</em> (2003-01-01) <a href="https://doi.org/cz8869">https://doi.org/cz8869</a><br />
DOI: <a href="https://doi.org/10.1093/bib/4.1.22">10.1093/bib/4.1.22</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/12715831">12715831</a></p>
</div>
<div id="ref-3Ew5V1iC">
<p>247. <strong>Detection of RNA polymerase II promoters and polyadenylation sites in human DNA sequence</strong><br />
Sherri Matis, Ying Xu, Manesh Shah, Xiaojun Guan, J.Ralph Einstein, Richard Mural, Edward Uberbacher<br />
<em>Computers &amp; Chemistry</em> (1996-03) <a href="https://doi.org/b3f4ch">https://doi.org/b3f4ch</a><br />
DOI: <a href="https://doi.org/10.1016/s0097-8485(96)80015-5">10.1016/s0097-8485(96)80015-5</a></p>
</div>
<div id="ref-as2HfoSh">
<p>248. <strong>Recognition of prokaryotic and eukaryotic promoters using convolutional deep learning neural networks</strong><br />
Ramzan Kh. Umarov, Victor V. Solovyev<br />
<em>PLOS ONE</em> (2017-02-03) <a href="https://doi.org/gcgmcr">https://doi.org/gcgmcr</a><br />
DOI: <a href="https://doi.org/10.1371/journal.pone.0171410">10.1371/journal.pone.0171410</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28158264">28158264</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5291440">PMC5291440</a></p>
</div>
<div id="ref-9XBPQ8b">
<p>249. <strong>Cap analysis gene expression for high-throughput analysis of transcriptional starting point and identification of promoter usage</strong><br />
T. Shiraki, S. Kondo, S. Katayama, K. Waki, T. Kasukawa, H. Kawaji, R. Kodzius, A. Watahiki, M. Nakamura, T. Arakawa, … Y. Hayashizaki<br />
<em>Proceedings of the National Academy of Sciences</em> (2003-12-08) <a href="https://doi.org/c8d26z">https://doi.org/c8d26z</a><br />
DOI: <a href="https://doi.org/10.1073/pnas.2136655100">10.1073/pnas.2136655100</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/14663149">14663149</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC307644">PMC307644</a></p>
</div>
<div id="ref-G0J9P3Ln">
<p>250. <strong>Genome-wide characterization of transcriptional start sites in humans by integrative transcriptome analysis</strong><br />
R. Yamashita, N. P. Sathira, A. Kanai, K. Tanimoto, T. Arauchi, Y. Tanaka, S.-i. Hashimoto, S. Sugano, K. Nakai, Y. Suzuki<br />
<em>Genome Research</em> (2011-03-03) <a href="https://doi.org/cf7nzg">https://doi.org/cf7nzg</a><br />
DOI: <a href="https://doi.org/10.1101/gr.110254.110">10.1101/gr.110254.110</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/21372179">21372179</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3083095">PMC3083095</a></p>
</div>
<div id="ref-8yA3foA6">
<p>251. <strong>Enhancers: five essential questions</strong><br />
Len A. Pennacchio, Wendy Bickmore, Ann Dean, Marcelo A. Nobrega, Gill Bejerano<br />
<em>Nature Reviews Genetics</em> (2013-03-18) <a href="https://doi.org/gcgk8n">https://doi.org/gcgk8n</a><br />
DOI: <a href="https://doi.org/10.1038/nrg3458">10.1038/nrg3458</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/23503198">23503198</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4445073">PMC4445073</a></p>
</div>
<div id="ref-J0PJHcHK">
<p>252. <strong>A unified architecture of transcriptional regulatory elements</strong><br />
Robin Andersson, Albin Sandelin, Charles G. Danko<br />
<em>Trends in Genetics</em> (2015-08) <a href="https://doi.org/f7nnvh">https://doi.org/f7nnvh</a><br />
DOI: <a href="https://doi.org/10.1016/j.tig.2015.05.007">10.1016/j.tig.2015.05.007</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26073855">26073855</a></p>
</div>
<div id="ref-2CbHXoFn">
<p>253. <strong>Basset: learning the regulatory code of the accessible genome with deep convolutional neural networks</strong><br />
David R. Kelley, Jasper Snoek, John L. Rinn<br />
<em>Genome Research</em> (2016-05-03) <a href="https://doi.org/f8sw35">https://doi.org/f8sw35</a><br />
DOI: <a href="https://doi.org/10.1101/gr.200535.115">10.1101/gr.200535.115</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27197224">27197224</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4937568">PMC4937568</a></p>
</div>
<div id="ref-jV2YerUS">
<p>254. <strong>DeepEnhancer: Predicting enhancers by convolutional neural networks</strong><br />
Xu Min, Ning Chen, Ting Chen, Rui Jiang<br />
<em>2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</em> (2016-12) <a href="https://doi.org/gcgk96">https://doi.org/gcgk96</a><br />
DOI: <a href="https://doi.org/10.1109/bibm.2016.7822593">10.1109/bibm.2016.7822593</a></p>
</div>
<div id="ref-1HbQQcY2q">
<p>255. <strong>Genome-Wide Prediction of cis-Regulatory Regions Using Supervised Deep Learning Methods</strong><br />
Yifeng Li, Wenqiang Shi, Wyeth W Wasserman<br />
<em>Cold Spring Harbor Laboratory</em> (2016-02-28) <a href="https://doi.org/gcgk86">https://doi.org/gcgk86</a><br />
DOI: <a href="https://doi.org/10.1101/041616">10.1101/041616</a></p>
</div>
<div id="ref-14TqLB9iZ">
<p>256. <strong>Predicting Enhancer-Promoter Interaction from Genomic Sequence with Deep Neural Networks</strong><br />
Shashank Singh, Yang Yang, Barnabas Poczos, Jian Ma<br />
<em>Cold Spring Harbor Laboratory</em> (2016-11-02) <a href="https://doi.org/gcgk9k">https://doi.org/gcgk9k</a><br />
DOI: <a href="https://doi.org/10.1101/085241">10.1101/085241</a></p>
</div>
<div id="ref-yVKIhIAf">
<p>257. <strong>A network-biology perspective of microRNA function and dysfunction in cancer</strong><br />
Cameron P. Bracken, Hamish S. Scott, Gregory J. Goodall<br />
<em>Nature Reviews Genetics</em> (2016-10-31) <a href="https://doi.org/f9bt2c">https://doi.org/f9bt2c</a><br />
DOI: <a href="https://doi.org/10.1038/nrg.2016.134">10.1038/nrg.2016.134</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27795564">27795564</a></p>
</div>
<div id="ref-8lpCCppx">
<p>258. <strong>Evolution of microRNA diversity and regulation in animals</strong><br />
Eugene Berezikov<br />
<em>Nature Reviews Genetics</em> (2011-11-18) <a href="https://doi.org/dk6596">https://doi.org/dk6596</a><br />
DOI: <a href="https://doi.org/10.1038/nrg3079">10.1038/nrg3079</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/22094948">22094948</a></p>
</div>
<div id="ref-12vPQi3gp">
<p>259. <strong>Predicting effective microRNA target sites in mammalian mRNAs</strong><br />
Vikram Agarwal, George W Bell, Jin-Wu Nam, David P Bartel<br />
<em>eLife</em> (2015-08-12) <a href="https://doi.org/gcgmc6">https://doi.org/gcgmc6</a><br />
DOI: <a href="https://doi.org/10.7554/elife.05005">10.7554/elife.05005</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26267216">26267216</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4532895">PMC4532895</a></p>
</div>
<div id="ref-1GwC1ll6h">
<p>260. <strong>deepTarget: End-to-end Learning Framework for microRNA Target Prediction using Deep Recurrent Neural Networks</strong><br />
Byunghan Lee, Junghwan Baek, Seunghyun Park, Sungroh Yoon<br />
<em>arXiv</em> (2016-03-30) <a href="https://arxiv.org/abs/1603.09123v2">https://arxiv.org/abs/1603.09123v2</a></p>
</div>
<div id="ref-1TeyWffV">
<p>261. <strong>deepMiRGene: Deep Neural Network based Precursor microRNA Prediction</strong><br />
Seunghyun Park, Seonwoo Min, Hyunsoo Choi, Sungroh Yoon<br />
<em>arXiv</em> (2016-04-29) <a href="https://arxiv.org/abs/1605.00017v1">https://arxiv.org/abs/1605.00017v1</a></p>
</div>
<div id="ref-pNoAbBEu">
<p>262. <strong>AUC-Maximized Deep Convolutional Neural Fields for Protein Sequence Labeling</strong><br />
Sheng Wang, Siqi Sun, Jinbo Xu<br />
<em>Machine Learning and Knowledge Discovery in Databases</em> (2016) <a href="https://doi.org/gcgk7n">https://doi.org/gcgk7n</a><br />
DOI: <a href="https://doi.org/10.1007/978-3-319-46227-1_1">10.1007/978-3-319-46227-1_1</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28884168">28884168</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5584645">PMC5584645</a></p>
</div>
<div id="ref-7atXz0r">
<p>263. <strong>MetaPSICOV: combining coevolution methods for accurate prediction of contacts and long range hydrogen bonding in proteins</strong><br />
David T. Jones, Tanya Singh, Tomasz Kosciolek, Stuart Tetchner<br />
<em>Bioinformatics</em> (2014-11-26) <a href="https://doi.org/f67gz4">https://doi.org/f67gz4</a><br />
DOI: <a href="https://doi.org/10.1093/bioinformatics/btu791">10.1093/bioinformatics/btu791</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/25431331">25431331</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4382908">PMC4382908</a></p>
</div>
<div id="ref-kboAopkh">
<p>264. <strong>Identification of direct residue contacts in protein-protein interaction by message passing</strong><br />
M. Weigt, R. A. White, H. Szurmant, J. A. Hoch, T. Hwa<br />
<em>Proceedings of the National Academy of Sciences</em> (2008-12-30) <a href="https://doi.org/dx4sww">https://doi.org/dx4sww</a><br />
DOI: <a href="https://doi.org/10.1073/pnas.0805923106">10.1073/pnas.0805923106</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/19116270">19116270</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2629192">PMC2629192</a></p>
</div>
<div id="ref-10dNuD89l">
<p>265. <strong>Protein 3D Structure Computed from Evolutionary Sequence Variation</strong><br />
Debora S. Marks, Lucy J. Colwell, Robert Sheridan, Thomas A. Hopf, Andrea Pagnani, Riccardo Zecchina, Chris Sander<br />
<em>PLoS ONE</em> (2011-12-07) <a href="https://doi.org/cmnkx6">https://doi.org/cmnkx6</a><br />
DOI: <a href="https://doi.org/10.1371/journal.pone.0028766">10.1371/journal.pone.0028766</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/22163331">22163331</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3233603">PMC3233603</a></p>
</div>
<div id="ref-1AlhRKQbe">
<p>266. <strong>A Unified Multitask Architecture for Predicting Local Protein Properties</strong><br />
Yanjun Qi, Merja Oja, Jason Weston, William Stafford Noble<br />
<em>PLoS ONE</em> (2012-03-26) <a href="https://doi.org/gcgmck">https://doi.org/gcgmck</a><br />
DOI: <a href="https://doi.org/10.1371/journal.pone.0032235">10.1371/journal.pone.0032235</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/22461885">22461885</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3312883">PMC3312883</a></p>
</div>
<div id="ref-UpFrhdJf">
<p>267. <strong>Improving prediction of secondary structure, local backbone angles and solvent accessible surface area of proteins by iterative deep learning</strong><br />
Rhys Heffernan, Kuldip Paliwal, James Lyons, Abdollah Dehzangi, Alok Sharma, Jihua Wang, Abdul Sattar, Yuedong Yang, Yaoqi Zhou<br />
<em>Scientific Reports</em> (2015-06-22) <a href="https://doi.org/gcgk8q">https://doi.org/gcgk8q</a><br />
DOI: <a href="https://doi.org/10.1038/srep11476">10.1038/srep11476</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26098304">26098304</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4476419">PMC4476419</a></p>
</div>
<div id="ref-Aic7UyXM">
<p>268. <strong>Protein secondary structure prediction based on position-specific scoring matrices 1 1Edited by G. Von Heijne</strong><br />
David T Jones<br />
<em>Journal of Molecular Biology</em> (1999-09) <a href="https://doi.org/d3fxv7">https://doi.org/d3fxv7</a><br />
DOI: <a href="https://doi.org/10.1006/jmbi.1999.3091">10.1006/jmbi.1999.3091</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/10493868">10493868</a></p>
</div>
<div id="ref-8t43CQ9m">
<p>269. <strong>Deep Supervised and Convolutional Generative Stochastic Network for Protein Secondary Structure Prediction</strong><br />
Jian Zhou, Olga G. Troyanskaya<br />
<em>arXiv</em> (2014-03-06) <a href="https://arxiv.org/abs/1403.1347v1">https://arxiv.org/abs/1403.1347v1</a></p>
</div>
<div id="ref-kqjqFesT">
<p>270. <strong>Protein contact prediction by integrating joint evolutionary coupling analysis and supervised learning</strong><br />
Jianzhu Ma, Sheng Wang, Zhiyong Wang, Jinbo Xu<br />
<em>Bioinformatics</em> (2015-08-14) <a href="https://doi.org/f7zggf">https://doi.org/f7zggf</a><br />
DOI: <a href="https://doi.org/10.1093/bioinformatics/btv472">10.1093/bioinformatics/btv472</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26275894">26275894</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4838177">PMC4838177</a></p>
</div>
<div id="ref-xdoT1yUx">
<p>271. <strong>Deep architectures for protein contact map prediction</strong><br />
Pietro Di Lena, Ken Nagata, Pierre Baldi<br />
<em>Bioinformatics</em> (2012-07-30) <a href="https://doi.org/f4bwr4">https://doi.org/f4bwr4</a><br />
DOI: <a href="https://doi.org/10.1093/bioinformatics/bts475">10.1093/bioinformatics/bts475</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/22847931">22847931</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3463120">PMC3463120</a></p>
</div>
<div id="ref-18bNbDNlc">
<p>272. <strong>Predicting protein residue–residue contacts using deep networks and boosting</strong><br />
Jesse Eickholt, Jianlin Cheng<br />
<em>Bioinformatics</em> (2012-10-09) <a href="https://doi.org/f4hqfh">https://doi.org/f4hqfh</a><br />
DOI: <a href="https://doi.org/10.1093/bioinformatics/bts598">10.1093/bioinformatics/bts598</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/23047561">23047561</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3509494">PMC3509494</a></p>
</div>
<div id="ref-F13xtRbV">
<p>273. <strong>Improved Contact Predictions Using the Recognition of Protein Like Contact Patterns</strong><br />
Marcin J. Skwark, Daniele Raimondi, Mirco Michel, Arne Elofsson<br />
<em>PLoS Computational Biology</em> (2014-11-06) <a href="https://doi.org/f25c62">https://doi.org/f25c62</a><br />
DOI: <a href="https://doi.org/10.1371/journal.pcbi.1003889">10.1371/journal.pcbi.1003889</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/25375897">25375897</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4222596">PMC4222596</a></p>
</div>
<div id="ref-zScWGveU">
<p>274. <strong>RR Results - CASP12</strong>(2017) <a href="http://www.predictioncenter.org/casp12/rrc_avrg_results.cgi">http://www.predictioncenter.org/casp12/rrc_avrg_results.cgi</a></p>
</div>
<div id="ref-u9uApoaB">
<p>275. <strong>CAMEO - Continuous Automated Model Evaluation</strong>(2017) <a href="http://www.cameo3d.org/">http://www.cameo3d.org/</a></p>
</div>
<div id="ref-39RPiE10">
<p>276. <strong>Predicting membrane protein contacts from non-membrane proteins by deep transfer learning</strong><br />
Zhen Li, Sheng Wang, Yizhou Yu, Jinbo Xu<br />
<em>arXiv</em> (2017-04-24) <a href="https://arxiv.org/abs/1704.07207v1">https://arxiv.org/abs/1704.07207v1</a></p>
</div>
<div id="ref-16s8GKCdZ">
<p>277. <strong>End-to-end differentiable learning of protein structure</strong><br />
Mohammed AlQuraishi<br />
<em>Cold Spring Harbor Laboratory</em> (2018-02-14) <a href="https://doi.org/gc3gsf">https://doi.org/gc3gsf</a><br />
DOI: <a href="https://doi.org/10.1101/265231">10.1101/265231</a></p>
</div>
<div id="ref-bSYQrdJA">
<p>278. <strong>Single-Particle Cryo-EM at Crystallographic Resolution</strong><br />
Yifan Cheng<br />
<em>Cell</em> (2015-04) <a href="https://doi.org/gcnz3b">https://doi.org/gcnz3b</a><br />
DOI: <a href="https://doi.org/10.1016/j.cell.2015.03.049">10.1016/j.cell.2015.03.049</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/25910205">25910205</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4409662">PMC4409662</a></p>
</div>
<div id="ref-Ud5iHvkw">
<p>279. <strong>A Primer to Single-Particle Cryo-Electron Microscopy</strong><br />
Yifan Cheng, Nikolaus Grigorieff, Pawel A. Penczek, Thomas Walz<br />
<em>Cell</em> (2015-04) <a href="https://doi.org/f692sc">https://doi.org/f692sc</a><br />
DOI: <a href="https://doi.org/10.1016/j.cell.2015.03.050">10.1016/j.cell.2015.03.050</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/25910204">25910204</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4409659">PMC4409659</a></p>
</div>
<div id="ref-6W1hknHI">
<p>280. <strong>SwarmPS: Rapid, semi-automated single particle selection software</strong><br />
David Woolford, Geoffery Ericksson, Rosalba Rothnagel, David Muller, Michael J. Landsberg, Radosav S. Pantelic, Alasdair McDowall, Bernard Pailthorpe, Paul R. Young, Ben Hankamer, Jasmine Banks<br />
<em>Journal of Structural Biology</em> (2007-01) <a href="https://doi.org/cqvpns">https://doi.org/cqvpns</a><br />
DOI: <a href="https://doi.org/10.1016/j.jsb.2006.04.006">10.1016/j.jsb.2006.04.006</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/16774837">16774837</a></p>
</div>
<div id="ref-HpxWaOv3">
<p>281. <strong>Semi-automated selection of cryo-EM particles in RELION-1.3</strong><br />
Sjors H. W. Scheres<br />
<em>Journal of Structural Biology</em> (2015-02) <a href="https://doi.org/f6xkc7">https://doi.org/f6xkc7</a><br />
DOI: <a href="https://doi.org/10.1016/j.jsb.2014.11.010">10.1016/j.jsb.2014.11.010</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/25486611">25486611</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4318617">PMC4318617</a></p>
</div>
<div id="ref-ku0xqQxt">
<p>282. <strong>DeepPicker: A deep learning approach for fully automated particle picking in cryo-EM</strong><br />
Feng Wang, Huichao Gong, Gaochao Liu, Meijing Li, Chuangye Yan, Tian Xia, Xueming Li, Jianyang Zeng<br />
<em>Journal of Structural Biology</em> (2016-09) <a href="https://doi.org/f8xr4n">https://doi.org/f8xr4n</a><br />
DOI: <a href="https://doi.org/10.1016/j.jsb.2016.07.006">10.1016/j.jsb.2016.07.006</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27424268">27424268</a></p>
</div>
<div id="ref-18QrMkpC5">
<p>283. <strong>A deep convolutional neural network approach to single-particle recognition in cryo-electron microscopy</strong><br />
Yanan Zhu, Qi Ouyang, Youdong Mao<br />
<em>BMC Bioinformatics</em> (2017-07-21) <a href="https://doi.org/gcnz3c">https://doi.org/gcnz3c</a><br />
DOI: <a href="https://doi.org/10.1186/s12859-017-1757-y">10.1186/s12859-017-1757-y</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28732461">28732461</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5521087">PMC5521087</a></p>
</div>
<div id="ref-RRR3YEJV">
<p>284. <strong>Massively parallel unsupervised single-particle cryo-EM data clustering via statistical manifold learning</strong><br />
Jiayi Wu, Yong-Bei Ma, Charles Congdon, Bevin Brett, Shuobing Chen, Yaofang Xu, Qi Ouyang, Youdong Mao<br />
<em>PLOS ONE</em> (2017-08-07) <a href="https://doi.org/gbqwkp">https://doi.org/gbqwkp</a><br />
DOI: <a href="https://doi.org/10.1371/journal.pone.0182130">10.1371/journal.pone.0182130</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28786986">28786986</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5546606">PMC5546606</a></p>
</div>
<div id="ref-imRWjslx">
<p>285. <strong>Protein–Protein Interactions Essentials: Key Concepts to Building and Analyzing Interactome Networks</strong><br />
Javier De Las Rivas, Celia Fontanillo<br />
<em>PLoS Computational Biology</em> (2010-06-24) <a href="https://doi.org/d8wc48">https://doi.org/d8wc48</a><br />
DOI: <a href="https://doi.org/10.1371/journal.pcbi.1000807">10.1371/journal.pcbi.1000807</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/20589078">20589078</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2891586">PMC2891586</a></p>
</div>
<div id="ref-xEQI6dXW">
<p>286. <strong>Extracting interactions between proteins from the literature</strong><br />
Deyu Zhou, Yulan He<br />
<em>Journal of Biomedical Informatics</em> (2008-04) <a href="https://doi.org/b9kh98">https://doi.org/b9kh98</a><br />
DOI: <a href="https://doi.org/10.1016/j.jbi.2007.11.008">10.1016/j.jbi.2007.11.008</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/18207462">18207462</a></p>
</div>
<div id="ref-TNHJioqT">
<p>287. <strong>Deep learning for extracting protein-protein interactions from biomedical literature</strong><br />
Yifan Peng, Zhiyong Lu<br />
<em>arXiv</em> (2017-06-05) <a href="https://arxiv.org/abs/1706.01556v2">https://arxiv.org/abs/1706.01556v2</a></p>
</div>
<div id="ref-rVgq22nD">
<p>288. <strong>DeepPPI: Boosting Prediction of Protein–Protein Interactions with Deep Neural Networks</strong><br />
Xiuquan Du, Shiwei Sun, Changlin Hu, Yu Yao, Yuanting Yan, Yanping Zhang<br />
<em>Journal of Chemical Information and Modeling</em> (2017-05-26) <a href="https://doi.org/gbmn43">https://doi.org/gbmn43</a><br />
DOI: <a href="https://doi.org/10.1021/acs.jcim.7b00028">10.1021/acs.jcim.7b00028</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28514151">28514151</a></p>
</div>
<div id="ref-3nZqSy6z">
<p>289. <strong>Sequence-based prediction of protein protein interaction using a deep-learning algorithm</strong><br />
Tanlin Sun, Bo Zhou, Luhua Lai, Jianfeng Pei<br />
<em>BMC Bioinformatics</em> (2017-05-25) <a href="https://doi.org/gcgnzs">https://doi.org/gcgnzs</a><br />
DOI: <a href="https://doi.org/10.1186/s12859-017-1700-2">10.1186/s12859-017-1700-2</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28545462">28545462</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5445391">PMC5445391</a></p>
</div>
<div id="ref-T2lbgFlY">
<p>290. <strong>Predicting protein–protein interactions from protein sequences by a stacked sparse autoencoder deep neural network</strong><br />
Yan-Bin Wang, Zhu-Hong You, Xiao Li, Tong-Hai Jiang, Xing Chen, Xi Zhou, Lei Wang<br />
<em>Molecular BioSystems</em> (2017) <a href="https://doi.org/gbg8zc">https://doi.org/gbg8zc</a><br />
DOI: <a href="https://doi.org/10.1039/c7mb00188f">10.1039/c7mb00188f</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28604872">28604872</a></p>
</div>
<div id="ref-2Ftmbvt4">
<p>291. <strong>Prediction of residue-residue contact matrix for protein-protein interaction with Fisher score features and deep learning</strong><br />
Tianchuan Du, Li Liao, Cathy H. Wu, Bilin Sun<br />
<em>Methods</em> (2016-11) <a href="https://doi.org/f9cjkk">https://doi.org/f9cjkk</a><br />
DOI: <a href="https://doi.org/10.1016/j.ymeth.2016.06.001">10.1016/j.ymeth.2016.06.001</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27282356">27282356</a></p>
</div>
<div id="ref-ul5VuLBZ">
<p>292. <strong>Reliable prediction of T-cell epitopes using neural networks with novel sequence representations</strong><br />
Morten Nielsen, Claus Lundegaard, Peder Worning, Sanne Lise Lauemøller, Kasper Lamberth, Søren Buus, Søren Brunak, Ole Lund<br />
<em>Protein Science</em> (2003-05) <a href="https://doi.org/bfzjwt">https://doi.org/bfzjwt</a><br />
DOI: <a href="https://doi.org/10.1110/ps.0239403">10.1110/ps.0239403</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/12717023">12717023</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2323871">PMC2323871</a></p>
</div>
<div id="ref-EKi3Bq3D">
<p>293. <strong>Gapped sequence alignment using artificial neural networks: application to the MHC class I system</strong><br />
Massimo Andreatta, Morten Nielsen<br />
<em>Bioinformatics</em> (2015-10-29) <a href="https://doi.org/f8dggc">https://doi.org/f8dggc</a><br />
DOI: <a href="https://doi.org/10.1093/bioinformatics/btv639">10.1093/bioinformatics/btv639</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26515819">26515819</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6402319">PMC6402319</a></p>
</div>
<div id="ref-QxZTL7xX">
<p>294. <strong>NetMHCpan, a method for MHC class I binding prediction beyond humans</strong><br />
Ilka Hoof, Bjoern Peters, John Sidney, Lasse Eggers Pedersen, Alessandro Sette, Ole Lund, Søren Buus, Morten Nielsen<br />
<em>Immunogenetics</em> (2008-11-12) <a href="https://doi.org/cn4g24">https://doi.org/cn4g24</a><br />
DOI: <a href="https://doi.org/10.1007/s00251-008-0341-z">10.1007/s00251-008-0341-z</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/19002680">19002680</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3319061">PMC3319061</a></p>
</div>
<div id="ref-KJjibUtO">
<p>295. <strong>NetMHCpan-3.0; improved prediction of binding to MHC class I molecules integrating information from multiple receptor and peptide length datasets</strong><br />
Morten Nielsen, Massimo Andreatta<br />
<em>Genome Medicine</em> (2016-03-30) <a href="https://doi.org/gcgnzt">https://doi.org/gcgnzt</a><br />
DOI: <a href="https://doi.org/10.1186/s13073-016-0288-x">10.1186/s13073-016-0288-x</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27029192">27029192</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4812631">PMC4812631</a></p>
</div>
<div id="ref-9ZNxZTxD">
<p>296. <strong>MHCflurry: open-source class I MHC binding affinity prediction</strong><br />
Timothy O’Donnell, Alex Rubinsteyn, Maria Bonsack, Angelika Riemer, Jeffrey Hammerbacher<br />
<em>Cold Spring Harbor Laboratory</em> (2017-08-09) <a href="https://doi.org/gcpzg6">https://doi.org/gcpzg6</a><br />
DOI: <a href="https://doi.org/10.1101/174243">10.1101/174243</a></p>
</div>
<div id="ref-1Hk3NTSn2">
<p>297. <strong>Predicting Peptide-MHC Binding Affinities With Imputed Training Data</strong><br />
Alex Rubinsteyn, Timothy O’Donnell, Nandita Damaraju, Jeffrey Hammerbacher<br />
<em>Cold Spring Harbor Laboratory</em> (2016-05-22) <a href="https://doi.org/gcgk89">https://doi.org/gcgk89</a><br />
DOI: <a href="https://doi.org/10.1101/054775">10.1101/054775</a></p>
</div>
<div id="ref-FRl0MTLd">
<p>298. <strong>High-order neural networks and kernel methods for peptide-MHC binding prediction</strong><br />
Pavel P. Kuksa, Martin Renqiang Min, Rishabh Dugar, Mark Gerstein<br />
<em>Bioinformatics</em> (2015-07-23) <a href="https://doi.org/f7zdkw">https://doi.org/f7zdkw</a><br />
DOI: <a href="https://doi.org/10.1093/bioinformatics/btv371">10.1093/bioinformatics/btv371</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26206306">26206306</a></p>
</div>
<div id="ref-1aswoG70">
<p>299. <strong>Evaluation of machine learning methods to predict peptide binding to MHC Class I proteins</strong><br />
Rohit Bhattacharya, Ashok Sivakumar, Collin Tokheim, Violeta Beleva Guthrie, Valsamo Anagnostou, Victor E. Velculescu, Rachel Karchin<br />
<em>Cold Spring Harbor Laboratory</em> (2017-06-23) <a href="https://doi.org/gcpzg5">https://doi.org/gcpzg5</a><br />
DOI: <a href="https://doi.org/10.1101/154757">10.1101/154757</a></p>
</div>
<div id="ref-12i8Apfdc">
<p>300. <strong>HLA class I binding prediction via convolutional neural networks</strong><br />
Yeeleng S Vang, Xiaohui Xie<br />
<em>Bioinformatics</em> (2017-04-21) <a href="https://doi.org/f94p2v">https://doi.org/f94p2v</a><br />
DOI: <a href="https://doi.org/10.1093/bioinformatics/btx264">10.1093/bioinformatics/btx264</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28444127">28444127</a></p>
</div>
<div id="ref-8CiDACi3">
<p>301. <strong>Network-based prediction of protein function</strong><br />
Roded Sharan, Igor Ulitsky, Ron Shamir<br />
<em>Molecular Systems Biology</em> (2007-03-13) <a href="https://doi.org/cc936j">https://doi.org/cc936j</a><br />
DOI: <a href="https://doi.org/10.1038/msb4100129">10.1038/msb4100129</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/17353930">17353930</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1847944">PMC1847944</a></p>
</div>
<div id="ref-WMwUw1o4">
<p>302. <strong>Learning the Structural Vocabulary of a Network</strong><br />
Saket Navlakha<br />
<em>Neural Computation</em> (2017-02) <a href="https://doi.org/f9vx8v">https://doi.org/f9vx8v</a><br />
DOI: <a href="https://doi.org/10.1162/neco_a_00924">10.1162/neco_a_00924</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28030777">28030777</a></p>
</div>
<div id="ref-dkPu3iv1">
<p>303. <strong>deepNF: Deep network fusion for protein function prediction</strong><br />
Vladimir Gligorijević, Meet Barot, Richard Bonneau<br />
<em>Cold Spring Harbor Laboratory</em> (2017-11-22) <a href="https://doi.org/gcpzg7">https://doi.org/gcpzg7</a><br />
DOI: <a href="https://doi.org/10.1101/223339">10.1101/223339</a></p>
</div>
<div id="ref-MwUPw4CD">
<p>304. <strong>Inductive Representation Learning on Large Graphs</strong><br />
William L. Hamilton, Rex Ying, Jure Leskovec<br />
<em>arXiv</em> (2017-06-07) <a href="https://arxiv.org/abs/1706.02216v2">https://arxiv.org/abs/1706.02216v2</a></p>
</div>
<div id="ref-FMsqNIQ4">
<p>305. <strong>Stochastic Training of Graph Convolutional Networks</strong><br />
Jianfei Chen, Jun Zhu<br />
<em>arXiv</em> (2017-10-29) <a href="https://arxiv.org/abs/1710.10568v1">https://arxiv.org/abs/1710.10568v1</a></p>
</div>
<div id="ref-40EG4ZEU">
<p>306. <strong>Deep Learning Automates the Quantitative Analysis of Individual Cells in Live-Cell Imaging Experiments</strong><br />
David A. Van Valen, Takamasa Kudo, Keara M. Lane, Derek N. Macklin, Nicolas T. Quach, Mialy M. DeFelice, Inbal Maayan, Yu Tanouchi, Euan A. Ashley, Markus W. Covert<br />
<em>PLOS Computational Biology</em> (2016-11-04) <a href="https://doi.org/f9rfj3">https://doi.org/f9rfj3</a><br />
DOI: <a href="https://doi.org/10.1371/journal.pcbi.1005177">10.1371/journal.pcbi.1005177</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27814364">27814364</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5096676">PMC5096676</a></p>
</div>
<div id="ref-TutLhFSz">
<p>307. <strong>U-Net: Convolutional Networks for Biomedical Image Segmentation</strong><br />
Olaf Ronneberger, Philipp Fischer, Thomas Brox<br />
<em>Lecture Notes in Computer Science</em> (2015) <a href="https://doi.org/gcgk7j">https://doi.org/gcgk7j</a><br />
DOI: <a href="https://doi.org/10.1007/978-3-319-24574-4_28">10.1007/978-3-319-24574-4_28</a></p>
</div>
<div id="ref-On4vW5aU">
<p>308. <strong>Prospective identification of hematopoietic lineage choice by deep learning</strong><br />
Felix Buggenthin, Florian Buettner, Philipp S Hoppe, Max Endele, Manuel Kroiss, Michael Strasser, Michael Schwarzfischer, Dirk Loeffler, Konstantinos D Kokkaliaris, Oliver Hilsenbeck, … Carsten Marr<br />
<em>Nature Methods</em> (2017-02-20) <a href="https://doi.org/gcgk8j">https://doi.org/gcgk8j</a><br />
DOI: <a href="https://doi.org/10.1038/nmeth.4182">10.1038/nmeth.4182</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28218899">28218899</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5376497">PMC5376497</a></p>
</div>
<div id="ref-gllSeTW">
<p>309. <strong>Reconstructing cell cycle and disease progression using deep learning</strong><br />
Philipp Eulenberg, Niklas Koehler, Thomas Blasi, Andrew Filby, Anne E. Carpenter, Paul Rees, Fabian J. Theis, F. Alexander Wolf<br />
<em>Cold Spring Harbor Laboratory</em> (2016-10-17) <a href="https://doi.org/gcgk9f">https://doi.org/gcgk9f</a><br />
DOI: <a href="https://doi.org/10.1101/081364">10.1101/081364</a></p>
</div>
<div id="ref-BMg062hc">
<p>310. <strong>Automating Morphological Profiling with Generic Deep Convolutional Networks</strong><br />
Nick Pawlowski, Juan C Caicedo, Shantanu Singh, Anne E Carpenter, Amos Storkey<br />
<em>Cold Spring Harbor Laboratory</em> (2016-11-02) <a href="https://doi.org/gcgk9j">https://doi.org/gcgk9j</a><br />
DOI: <a href="https://doi.org/10.1101/085118">10.1101/085118</a></p>
</div>
<div id="ref-71c6rs2z">
<p>311. <strong>Generative Modeling with Conditional Autoencoders: Building an Integrated Cell</strong><br />
Gregory R. Johnson, Rory M. Donovan-Maiye, Mary M. Maleckar<br />
<em>arXiv</em> (2017-04-28) <a href="https://arxiv.org/abs/1705.00092v1">https://arxiv.org/abs/1705.00092v1</a></p>
</div>
<div id="ref-hkKO4QYl">
<p>312. <strong>Applications in image-based profiling of perturbations</strong><br />
Juan C Caicedo, Shantanu Singh, Anne E Carpenter<br />
<em>Current Opinion in Biotechnology</em> (2016-06) <a href="https://doi.org/f8s4hm">https://doi.org/f8s4hm</a><br />
DOI: <a href="https://doi.org/10.1016/j.copbio.2016.04.003">10.1016/j.copbio.2016.04.003</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27089218">27089218</a></p>
</div>
<div id="ref-m3Ij21U8">
<p>313. <strong>Large-scale image-based screening and profiling of cellular phenotypes</strong><br />
Nicola Bougen-Zhukov, Sheng Yang Loh, Hwee Kuan Lee, Lit-Hsin Loo<br />
<em>Cytometry Part A</em> (2016-07-19) <a href="https://doi.org/f9rp9b">https://doi.org/f9rp9b</a><br />
DOI: <a href="https://doi.org/10.1002/cyto.a.22909">10.1002/cyto.a.22909</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27434125">27434125</a></p>
</div>
<div id="ref-McjXFLLq">
<p>314. <strong>Machine learning and computer vision approaches for phenotypic profiling</strong><br />
Ben T. Grys, Dara S. Lo, Nil Sahin, Oren Z. Kraus, Quaid Morris, Charles Boone, Brenda J. Andrews<br />
<em>The Journal of Cell Biology</em> (2016-12-09) <a href="https://doi.org/gcgk8t">https://doi.org/gcgk8t</a><br />
DOI: <a href="https://doi.org/10.1083/jcb.201610026">10.1083/jcb.201610026</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27940887">27940887</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5223612">PMC5223612</a></p>
</div>
<div id="ref-1AWC7HsO0">
<p>315. <strong>Single-cell genome sequencing: current state of the science</strong><br />
Charles Gawad, Winston Koh, Stephen R. Quake<br />
<em>Nature Reviews Genetics</em> (2016-01-25) <a href="https://doi.org/f8ddqt">https://doi.org/f8ddqt</a><br />
DOI: <a href="https://doi.org/10.1038/nrg.2015.16">10.1038/nrg.2015.16</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26806412">26806412</a></p>
</div>
<div id="ref-1GvfSy48x">
<p>316. <strong>Somatic mutation in single human neurons tracks developmental and transcriptional history</strong><br />
Michael A. Lodato, Mollie B. Woodworth, Semin Lee, Gilad D. Evrony, Bhaven K. Mehta, Amir Karger, Soohyun Lee, Thomas W. Chittenden, Alissa M. D’Gama, Xuyu Cai, … Christopher A. Walsh<br />
<em>Science</em> (2015-10-01) <a href="https://doi.org/f7sxdv">https://doi.org/f7sxdv</a><br />
DOI: <a href="https://doi.org/10.1126/science.aab1785">10.1126/science.aab1785</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26430121">26430121</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4664477">PMC4664477</a></p>
</div>
<div id="ref-QafUwNKn">
<p>317. <strong>Single-cell transcriptome sequencing: recent advances and remaining challenges</strong><br />
Serena Liu, Cole Trapnell<br />
<em>F1000Research</em> (2016-02-17) <a href="https://doi.org/gcgmcd">https://doi.org/gcgmcd</a><br />
DOI: <a href="https://doi.org/10.12688/f1000research.7223.1">10.12688/f1000research.7223.1</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26949524">26949524</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4758375">PMC4758375</a></p>
</div>
<div id="ref-v97iPXDw">
<p>318. <strong>Single-Cell and Single-Molecule Analysis of Gene Expression Regulation</strong><br />
Maria Vera, Jeetayu Biswas, Adrien Senecal, Robert H. Singer, Hye Yoon Park<br />
<em>Annual Review of Genetics</em> (2016-11-23) <a href="https://doi.org/gcgmb5">https://doi.org/gcgmb5</a><br />
DOI: <a href="https://doi.org/10.1146/annurev-genet-120215-034854">10.1146/annurev-genet-120215-034854</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27893965">27893965</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5149423">PMC5149423</a></p>
</div>
<div id="ref-1CAw3FaPI">
<p>319. <strong>scNMT-seq enables joint profiling of chromatin accessibility DNA methylation and transcription in single cells</strong><br />
Stephen J. Clark, Ricard Argelaguet, Chantriolnt-Andreas Kapourani, Thomas M. Stubbs, Heather J. Lee, Celia Alda-Catalinas, Felix Krueger, Guido Sanguinetti, Gavin Kelsey, John C. Marioni, … Wolf Reik<br />
<em>Cold Spring Harbor Laboratory</em> (2017-05-17) <a href="https://doi.org/gcgk93">https://doi.org/gcgk93</a><br />
DOI: <a href="https://doi.org/10.1101/138685">10.1101/138685</a></p>
</div>
<div id="ref-XimuXZlz">
<p>320. <strong>Denoising genome-wide histone ChIP-seq with convolutional neural networks</strong><br />
Pang Wei Koh, Emma Pierson, Anshul Kundaje<br />
<em>Cold Spring Harbor Laboratory</em> (2016-05-07) <a href="https://doi.org/gcgk88">https://doi.org/gcgk88</a><br />
DOI: <a href="https://doi.org/10.1101/052118">10.1101/052118</a></p>
</div>
<div id="ref-1ERrBiqG7">
<p>321. <strong>Removal of batch effects using distribution-matching residual networks</strong><br />
Uri Shaham, Kelly P Stanton, Jun Zhao, Huamin Li, Khadir Raddassi, Ruth Montgomery, Yuval Kluger<br />
<em>Bioinformatics</em> (2017-04-13) <a href="https://doi.org/gbrshc">https://doi.org/gbrshc</a><br />
DOI: <a href="https://doi.org/10.1093/bioinformatics/btx196">10.1093/bioinformatics/btx196</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28419223">28419223</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5870543">PMC5870543</a></p>
</div>
<div id="ref-1HPu3R2B4">
<p>322. <strong>Single-Cell Genomics Unveils Critical Regulators of Th17 Cell Pathogenicity</strong><br />
Jellert T. Gaublomme, Nir Yosef, Youjin Lee, Rona S. Gertner, Li V. Yang, Chuan Wu, Pier Paolo Pandolfi, Tak Mak, Rahul Satija, Alex K. Shalek, … Aviv Regev<br />
<em>Cell</em> (2015-12) <a href="https://doi.org/f73pfp">https://doi.org/f73pfp</a><br />
DOI: <a href="https://doi.org/10.1016/j.cell.2015.11.009">10.1016/j.cell.2015.11.009</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26607794">26607794</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4671824">PMC4671824</a></p>
</div>
<div id="ref-r3Gbjksq">
<p>323. <strong>Sensitive detection of rare disease-associated cell subsets via representation learning.</strong><br />
Eirini Arvaniti, Manfred Claassen<br />
<em>Cold Spring Harbor Laboratory</em> (2016-03-31) <a href="https://doi.org/gcgk87">https://doi.org/gcgk87</a><br />
DOI: <a href="https://doi.org/10.1101/046508">10.1101/046508</a></p>
</div>
<div id="ref-yJxCo4h1">
<p>324. <strong>Interpretable dimensionality reduction of single cell transcriptome data with deep generative models</strong><br />
Jiarui Ding, Anne E. Condon, Sohrab P. Shah<br />
<em>Cold Spring Harbor Laboratory</em> (2017-09-01) <a href="https://doi.org/gcnzb8">https://doi.org/gcnzb8</a><br />
DOI: <a href="https://doi.org/10.1101/178624">10.1101/178624</a></p>
</div>
<div id="ref-RHqbJgpe">
<p>325. <strong>A deep generative model for gene expression profiles from single-cell RNA sequencing</strong><br />
Romain Lopez, Jeffrey Regier, Michael Cole, Michael Jordan, Nir Yosef<br />
<em>arXiv</em> (2017-09-07) <a href="https://arxiv.org/abs/1709.02082v4">https://arxiv.org/abs/1709.02082v4</a></p>
</div>
<div id="ref-TGyu2Woj">
<p>326. <strong>Visualizing Data using t-SNE</strong><br />
Laurens van der Maaten, Geoffrey Hinton<br />
<em>Journal of Machine Learning Research</em> (2008) <a href="http://www.jmlr.org/papers/v9/vandermaaten08a.html">http://www.jmlr.org/papers/v9/vandermaaten08a.html</a></p>
</div>
<div id="ref-owp8L957">
<p>327. <strong>Using neural networks for reducing the dimensions of single-cell RNA-Seq data</strong><br />
Chieh Lin, Siddhartha Jain, Hannah Kim, Ziv Bar-Joseph<br />
<em>Nucleic Acids Research</em> (2017-07-31) <a href="https://doi.org/gcnzb7">https://doi.org/gcnzb7</a><br />
DOI: <a href="https://doi.org/10.1093/nar/gkx681">10.1093/nar/gkx681</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28973464">28973464</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5737331">PMC5737331</a></p>
</div>
<div id="ref-vk9ZInF3">
<p>328. <strong>The Human Cell Atlas</strong><br />
Aviv Regev, Sarah A Teichmann, Eric S Lander, Ido Amit, Christophe Benoist, Ewan Birney, Bernd Bodenmiller, Peter Campbell, Piero Carninci, Menna Clatworthy, … <br />
<em>eLife</em> (2017-12-05) <a href="https://doi.org/gcnzcv">https://doi.org/gcnzcv</a><br />
DOI: <a href="https://doi.org/10.7554/elife.27041">10.7554/elife.27041</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/29206104">29206104</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5762154">PMC5762154</a></p>
</div>
<div id="ref-Oljj2W96">
<p>329. <strong>Reversed graph embedding resolves complex single-cell developmental trajectories</strong><br />
Xiaojie Qiu, Qi Mao, Ying Tang, Li Wang, Raghav Chawla, Hannah Pliner, Cole Trapnell<br />
<em>Cold Spring Harbor Laboratory</em> (2017-02-21) <a href="https://doi.org/gcgk9x">https://doi.org/gcgk9x</a><br />
DOI: <a href="https://doi.org/10.1101/110668">10.1101/110668</a></p>
</div>
<div id="ref-2gn6PKkv">
<p>330. <strong>Mastering the game of Go with deep neural networks and tree search</strong><br />
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, … Demis Hassabis<br />
<em>Nature</em> (2016-01) <a href="https://doi.org/f77tw6">https://doi.org/f77tw6</a><br />
DOI: <a href="https://doi.org/10.1038/nature16961">10.1038/nature16961</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26819042">26819042</a></p>
</div>
<div id="ref-N9NzkOjA">
<p>331. <strong>Compositional biases of bacterial genomes and evolutionary implications.</strong><br />
S Karlin, J Mrázek, AM Campbell<br />
<em>Journal of Bacteriology</em> (1997-06) <a href="https://doi.org/gcgmbp">https://doi.org/gcgmbp</a><br />
DOI: <a href="https://doi.org/10.1128/jb.179.12.3899-3913.1997">10.1128/jb.179.12.3899-3913.1997</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/9190805">9190805</a></p>
</div>
<div id="ref-QV551Nlx">
<p>332. <strong>Accurate phylogenetic classification of variable-length DNA fragments</strong><br />
Alice Carolyn McHardy, Héctor García Martín, Aristotelis Tsirigos, Philip Hugenholtz, Isidore Rigoutsos<br />
<em>Nature Methods</em> (2006-12-10) <a href="https://doi.org/fwrtm4">https://doi.org/fwrtm4</a><br />
DOI: <a href="https://doi.org/10.1038/nmeth976">10.1038/nmeth976</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/17179938">17179938</a></p>
</div>
<div id="ref-1HtJuEkb2">
<p>333. <strong>NBC: the Naive Bayes Classification tool webserver for taxonomic classification of metagenomic reads</strong><br />
G. L. Rosen, E. R. Reichenberger, A. M. Rosenfeld<br />
<em>Bioinformatics</em> (2010-11-08) <a href="https://doi.org/dtqrnt">https://doi.org/dtqrnt</a><br />
DOI: <a href="https://doi.org/10.1093/bioinformatics/btq619">10.1093/bioinformatics/btq619</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/21062764">21062764</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3008645">PMC3008645</a></p>
</div>
<div id="ref-1HhqhBwrM">
<p>334. <strong>Informatics for Unveiling Hidden Genome Signatures</strong><br />
T. Abe<br />
<em>Genome Research</em> (2003-04-01) <a href="https://doi.org/c475s4">https://doi.org/c475s4</a><br />
DOI: <a href="https://doi.org/10.1101/gr.634603">10.1101/gr.634603</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/12671005">12671005</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC430167">PMC430167</a></p>
</div>
<div id="ref-56wEWVIl">
<p>335. <strong>Metagenomic microbial community profiling using unique clade-specific marker genes</strong><br />
Nicola Segata, Levi Waldron, Annalisa Ballarini, Vagheesh Narasimhan, Olivier Jousson, Curtis Huttenhower<br />
<em>Nature Methods</em> (2012-06-10) <a href="https://doi.org/gcgk8f">https://doi.org/gcgk8f</a><br />
DOI: <a href="https://doi.org/10.1038/nmeth.2066">10.1038/nmeth.2066</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/22688413">22688413</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3443552">PMC3443552</a></p>
</div>
<div id="ref-RqhGD9c7">
<p>336. <strong>WGSQuikr: Fast Whole-Genome Shotgun Metagenomic Classification</strong><br />
David Koslicki, Simon Foucart, Gail Rosen<br />
<em>PLoS ONE</em> (2014-03-13) <a href="https://doi.org/gcgmcm">https://doi.org/gcgmcm</a><br />
DOI: <a href="https://doi.org/10.1371/journal.pone.0091784">10.1371/journal.pone.0091784</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/24626336">24626336</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3953531">PMC3953531</a></p>
</div>
<div id="ref-189TQrQA9">
<p>337. <strong>Scalable metagenomic taxonomy classification using a reference genome database</strong><br />
Sasha K. Ames, David A. Hysom, Shea N. Gardner, G. Scott Lloyd, Maya B. Gokhale, Jonathan E. Allen<br />
<em>Bioinformatics</em> (2013-07-04) <a href="https://doi.org/f48vw6">https://doi.org/f48vw6</a><br />
DOI: <a href="https://doi.org/10.1093/bioinformatics/btt389">10.1093/bioinformatics/btt389</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/23828782">23828782</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3753567">PMC3753567</a></p>
</div>
<div id="ref-8DLzxOEt">
<p>338. <strong>Large-scale machine learning for metagenomics sequence classification</strong><br />
Kévin Vervier, Pierre Mahé, Maud Tournoud, Jean-Baptiste Veyrieras, Jean-Philippe Vert<br />
<em>Bioinformatics</em> (2015-11-20) <a href="https://doi.org/f8h92j">https://doi.org/f8h92j</a><br />
DOI: <a href="https://doi.org/10.1093/bioinformatics/btv683">10.1093/bioinformatics/btv683</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26589281">26589281</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4896366">PMC4896366</a></p>
</div>
<div id="ref-qUGH5CX8">
<p>339. <strong>Combining gene prediction methods to improve metagenomic gene annotation</strong><br />
Non G Yok, Gail L Rosen<br />
<em>BMC Bioinformatics</em> (2011-01-13) <a href="https://doi.org/dcnc32">https://doi.org/dcnc32</a><br />
DOI: <a href="https://doi.org/10.1186/1471-2105-12-20">10.1186/1471-2105-12-20</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/21232129">21232129</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3042383">PMC3042383</a></p>
</div>
<div id="ref-yFOAeemA">
<p>340. <strong>Machine learning for metagenomics: methods and tools</strong><br />
Hayssam Soueidan, Macha Nikolski<br />
<em>Metagenomics</em> (2017-01-01) <a href="https://doi.org/gcgmct">https://doi.org/gcgmct</a><br />
DOI: <a href="https://doi.org/10.1515/metgen-2016-0001">10.1515/metgen-2016-0001</a></p>
</div>
<div id="ref-W0cYSf89">
<p>341. <strong>Utilizing Machine Learning Approaches to Understand the Interrelationship of Diet, the Human Gastrointestinal Microbiome, and Health</strong><br />
Heather Guetterman, Loretta Auvil, Nate Russell, Michael Welge, Matt Berry, Lisa Gatzke, Colleen Bushell, Hannah Holscher<br />
<em>The FASEB Journal</em> (2016-04-01) <a href="https://www.fasebj.org/doi/abs/10.1096/fasebj.30.1_supplement.406.3">https://www.fasebj.org/doi/abs/10.1096/fasebj.30.1_supplement.406.3</a><br />
DOI: <a href="https://doi.org/10.1096/fasebj.30.1_supplement.406.3">10.1096/fasebj.30.1_supplement.406.3</a></p>
</div>
<div id="ref-aI9g2UOc">
<p>342. <strong>Supervised classification of human microbiota</strong><br />
Dan Knights, Elizabeth K. Costello, Rob Knight<br />
<em>FEMS Microbiology Reviews</em> (2011-03) <a href="https://doi.org/c4bwxs">https://doi.org/c4bwxs</a><br />
DOI: <a href="https://doi.org/10.1111/j.1574-6976.2010.00251.x">10.1111/j.1574-6976.2010.00251.x</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/21039646">21039646</a></p>
</div>
<div id="ref-c5P9jHCg">
<p>343. <strong>A comprehensive evaluation of multicategory classification methods for microbiomic data</strong><br />
Alexander Statnikov, Mikael Henaff, Varun Narendra, Kranti Konganti, Zhiguo Li, Liying Yang, Zhiheng Pei, Martin J Blaser, Constantin F Aliferis, Alexander V Alekseyenko<br />
<em>Microbiome</em> (2013-04-05) <a href="https://doi.org/gcgmb6">https://doi.org/gcgmb6</a><br />
DOI: <a href="https://doi.org/10.1186/2049-2618-1-11">10.1186/2049-2618-1-11</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/24456583">24456583</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3960509">PMC3960509</a></p>
</div>
<div id="ref-y9s5irW">
<p>344. <strong>Machine Learning Meta-analysis of Large Metagenomic Datasets: Tools and Biological Insights</strong><br />
Edoardo Pasolli, Duy Tin Truong, Faizan Malik, Levi Waldron, Nicola Segata<br />
<em>PLOS Computational Biology</em> (2016-07-11) <a href="https://doi.org/gcgmch">https://doi.org/gcgmch</a><br />
DOI: <a href="https://doi.org/10.1371/journal.pcbi.1004977">10.1371/journal.pcbi.1004977</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27400279">27400279</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4939962">PMC4939962</a></p>
</div>
<div id="ref-5W4KMSdT">
<p>345. <strong>DectICO: an alignment-free supervised metagenomic classification method based on feature extraction and dynamic selection</strong><br />
Xiao Ding, Fudong Cheng, Changchang Cao, Xiao Sun<br />
<em>BMC Bioinformatics</em> (2015-10-07) <a href="https://doi.org/f743hh">https://doi.org/f743hh</a><br />
DOI: <a href="https://doi.org/10.1186/s12859-015-0753-3">10.1186/s12859-015-0753-3</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26446672">26446672</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4596415">PMC4596415</a></p>
</div>
<div id="ref-Vb3Fwx7d">
<p>346. <strong>Class Prediction and Feature Selection with Linear Optimization for Metagenomic Count Data</strong><br />
Zhenqiu Liu, Dechang Chen, Li Sheng, Amy Y. Liu<br />
<em>PLoS ONE</em> (2013-03-26) <a href="https://doi.org/f4w5df">https://doi.org/f4w5df</a><br />
DOI: <a href="https://doi.org/10.1371/journal.pone.0053253">10.1371/journal.pone.0053253</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/23555553">23555553</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3608598">PMC3608598</a></p>
</div>
<div id="ref-1AN5UPfb1">
<p>347. <strong>Fizzy: feature subset selection for metagenomics</strong><br />
Gregory Ditzler, J. Calvin Morrison, Yemin Lan, Gail L. Rosen<br />
<em>BMC Bioinformatics</em> (2015-11-04) <a href="https://doi.org/gb8w9s">https://doi.org/gb8w9s</a><br />
DOI: <a href="https://doi.org/10.1186/s12859-015-0793-8">10.1186/s12859-015-0793-8</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26538306">26538306</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4634798">PMC4634798</a></p>
</div>
<div id="ref-O9D66oYa">
<p>348. <strong>A Bootstrap Based Neyman-Pearson Test for Identifying Variable Importance</strong><br />
Gregory Ditzler, Robi Polikar, Gail Rosen<br />
<em>IEEE Transactions on Neural Networks and Learning Systems</em> (2015-04) <a href="https://doi.org/f66w3q">https://doi.org/f66w3q</a><br />
DOI: <a href="https://doi.org/10.1109/tnnls.2014.2320415">10.1109/tnnls.2014.2320415</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/25794384">25794384</a></p>
</div>
<div id="ref-q1A2AEtO">
<p>349. <strong>Orphelia: predicting genes in metagenomic sequencing reads</strong><br />
Katharina J. Hoff, Thomas Lingner, Peter Meinicke, Maike Tech<br />
<em>Nucleic Acids Research</em> (2009-05-08) <a href="https://doi.org/c6mst6">https://doi.org/c6mst6</a><br />
DOI: <a href="https://doi.org/10.1093/nar/gkp327">10.1093/nar/gkp327</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/19429689">19429689</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2703946">PMC2703946</a></p>
</div>
<div id="ref-QlbXLqH">
<p>350. <strong>FragGeneScan: predicting genes in short and error-prone reads</strong><br />
Mina Rho, Haixu Tang, Yuzhen Ye<br />
<em>Nucleic Acids Research</em> (2010-08-28) <a href="https://doi.org/fjg4kc">https://doi.org/fjg4kc</a><br />
DOI: <a href="https://doi.org/10.1093/nar/gkq747">10.1093/nar/gkq747</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/20805240">20805240</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2978382">PMC2978382</a></p>
</div>
<div id="ref-1E1PWjqTm">
<p>351. <strong>Continuous Distributed Representation of Biological Sequences for Deep Proteomics and Genomics</strong><br />
Ehsaneddin Asgari, Mohammad R. K. Mofrad<br />
<em>PLOS ONE</em> (2015-11-10) <a href="https://doi.org/gcgmcq">https://doi.org/gcgmcq</a><br />
DOI: <a href="https://doi.org/10.1371/journal.pone.0141287">10.1371/journal.pone.0141287</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26555596">26555596</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4640716">PMC4640716</a></p>
</div>
<div id="ref-G8RKF6sz">
<p>352. <strong>Fast model-based protein homology detection without alignment</strong><br />
S. Hochreiter, M. Heusel, K. Obermayer<br />
<em>Bioinformatics</em> (2007-05-08) <a href="https://doi.org/b38n9w">https://doi.org/b38n9w</a><br />
DOI: <a href="https://doi.org/10.1093/bioinformatics/btm247">10.1093/bioinformatics/btm247</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/17488755">17488755</a></p>
</div>
<div id="ref-zYUI7tc1">
<p>353. <strong>Convolutional LSTM Networks for Subcellular Localization of Proteins</strong><br />
Søren Kaae Sønderby, Casper Kaae Sønderby, Henrik Nielsen, Ole Winther<br />
<em>Algorithms for Computational Biology</em> (2015) <a href="https://doi.org/gcrnp2">https://doi.org/gcrnp2</a><br />
DOI: <a href="https://doi.org/10.1007/978-3-319-21233-3_6">10.1007/978-3-319-21233-3_6</a></p>
</div>
<div id="ref-11wVLI2Hn">
<p>354. <strong>Neural network-based taxonomic clustering for metagenomics</strong><br />
Steven D. Essinger, Robi Polikar, Gail L. Rosen<br />
<em>The 2010 International Joint Conference on Neural Networks (IJCNN)</em> (2010-07) <a href="https://doi.org/bnsxts">https://doi.org/bnsxts</a><br />
DOI: <a href="https://doi.org/10.1109/ijcnn.2010.5596644">10.1109/ijcnn.2010.5596644</a></p>
</div>
<div id="ref-c4rnN1wo">
<p>355. <strong>Clustering metagenomic sequences with interpolated Markov models</strong><br />
David R Kelley, Steven L Salzberg<br />
<em>BMC Bioinformatics</em> (2010-11-02) <a href="https://doi.org/fgjq98">https://doi.org/fgjq98</a><br />
DOI: <a href="https://doi.org/10.1186/1471-2105-11-544">10.1186/1471-2105-11-544</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/21044341">21044341</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3098094">PMC3098094</a></p>
</div>
<div id="ref-Wz7VUS03">
<p>356. <strong>METAGENOMIC TAXONOMIC CLASSIFICATION USING EXTREME LEARNING MACHINES</strong><br />
ZEEHASHAM RASHEED, HUZEFA RANGWALA<br />
<em>Journal of Bioinformatics and Computational Biology</em> (2012-10) <a href="https://doi.org/gcgmbt">https://doi.org/gcgmbt</a><br />
DOI: <a href="https://doi.org/10.1142/s0219720012500151">10.1142/s0219720012500151</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/22849369">22849369</a></p>
</div>
<div id="ref-iPIJrVVs">
<p>357. <strong>Globoko ucenje na genomskih in filogenetskih podatkih</strong><br />
Nina Mrzelj<br />
<em>Univerza v Ljubljani, Fakulteta za racunalništvo in informatiko</em> (2016) <a href="https://repozitorij.uni-lj.si/IzpisGradiva.php?id=85515">https://repozitorij.uni-lj.si/IzpisGradiva.php?id=85515</a></p>
</div>
<div id="ref-oas5tbC7">
<p>358. <strong>Influence of microbiome species in hard-to-heal wounds on disease severity and treatment duration</strong><br />
Dagmar Chudobova, Kristyna Cihalova, Roman Guran, Simona Dostalova, Kristyna Smerkova, Radek Vesely, Jaromir Gumulec, Michal Masarik, Zbynek Heger, Vojtech Adam, Rene Kizek<br />
<em>The Brazilian Journal of Infectious Diseases</em> (2015-11) <a href="https://doi.org/gcgk7z">https://doi.org/gcgk7z</a><br />
DOI: <a href="https://doi.org/10.1016/j.bjid.2015.08.013">10.1016/j.bjid.2015.08.013</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26518264">26518264</a></p>
</div>
<div id="ref-i38A0beL">
<p>359. <strong>Multi-Layer and Recursive Neural Networks for Metagenomic Classification</strong><br />
Gregory Ditzler, Robi Polikar, Gail Rosen<br />
<em>IEEE Transactions on NanoBioscience</em> (2015-09) <a href="https://doi.org/gcgmbj">https://doi.org/gcgmbj</a><br />
DOI: <a href="https://doi.org/10.1109/tnb.2015.2461219">10.1109/tnb.2015.2461219</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26316190">26316190</a></p>
</div>
<div id="ref-NQ5jiN7B">
<p>360. <strong>TensorFlow vs. scikit-learn : The Microbiome Challenge</strong><br />
Ali A. Faruqi<br />
<em>Ali A. Faruqi</em> <a href="http://alifar76.github.io/sklearn-metrics/">http://alifar76.github.io/sklearn-metrics/</a></p>
</div>
<div id="ref-g2vvbB91">
<p>361. <strong>Advances in Optimizing Recurrent Networks</strong><br />
Yoshua Bengio, Nicolas Boulanger-Lewandowski, Razvan Pascanu<br />
<em>arXiv</em> (2012-12-04) <a href="https://arxiv.org/abs/1212.0901v2">https://arxiv.org/abs/1212.0901v2</a></p>
</div>
<div id="ref-Jw2asgH1">
<p>362. <strong>DeepNano: Deep recurrent neural networks for base calling in MinION nanopore reads</strong><br />
Vladimír Boža, Broňa Brejová, Tomáš Vinař<br />
<em>PLOS ONE</em> (2017-06-05) <a href="https://doi.org/gcrnp4">https://doi.org/gcrnp4</a><br />
DOI: <a href="https://doi.org/10.1371/journal.pone.0178751">10.1371/journal.pone.0178751</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28582401">28582401</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5459436">PMC5459436</a></p>
</div>
<div id="ref-2cMhMv5A">
<p>363. <strong>Sequence to Sequence Learning with Neural Networks</strong><br />
Ilya Sutskever, Oriol Vinyals, Quoc V. Le<br />
<em>arXiv</em> (2014-09-10) <a href="https://arxiv.org/abs/1409.3215v3">https://arxiv.org/abs/1409.3215v3</a></p>
</div>
<div id="ref-FVfZESYP">
<p>364. <strong>Creating a universal SNP and small indel variant caller with deep neural networks</strong><br />
Ryan Poplin, Pi-Chuan Chang, David Alexander, Scott Schwartz, Thomas Colthurst, Alexander Ku, Dan Newburger, Jojo Dijamco, Nam Nguyen, Pegah T. Afshar, … Mark A. DePristo<br />
<em>Cold Spring Harbor Laboratory</em> (2016-12-14) <a href="https://doi.org/gcgk9m">https://doi.org/gcgk9m</a><br />
DOI: <a href="https://doi.org/10.1101/092890">10.1101/092890</a></p>
</div>
<div id="ref-NCr4QkOg">
<p>365. <strong>A framework for variation discovery and genotyping using next-generation DNA sequencing data</strong><br />
Mark A DePristo, Eric Banks, Ryan Poplin, Kiran V Garimella, Jared R Maguire, Christopher Hartl, Anthony A Philippakis, Guillermo del Angel, Manuel A Rivas, Matt Hanna, … Mark J Daly<br />
<em>Nature Genetics</em> (2011-04-10) <a href="https://doi.org/d9k453">https://doi.org/d9k453</a><br />
DOI: <a href="https://doi.org/10.1038/ng.806">10.1038/ng.806</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/21478889">21478889</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3083463">PMC3083463</a></p>
</div>
<div id="ref-GSLRw2L5">
<p>366. <strong>Training Genotype Callers with Neural Networks</strong><br />
Rémi Torracinta, Fabien Campagne<br />
<em>Cold Spring Harbor Laboratory</em> (2016-12-30) <a href="https://doi.org/gcgk9s">https://doi.org/gcgk9s</a><br />
DOI: <a href="https://doi.org/10.1101/097469">10.1101/097469</a></p>
</div>
<div id="ref-VMkPJjVk">
<p>367. <strong>Xception: Deep Learning with Depthwise Separable Convolutions</strong><br />
François Chollet<br />
<em>arXiv</em> (2016-10-07) <a href="https://arxiv.org/abs/1610.02357v3">https://arxiv.org/abs/1610.02357v3</a></p>
</div>
<div id="ref-ECTm1SuA">
<p>368. <strong>Adaptive Somatic Mutations Calls with Deep Learning and Semi-Simulated Data</strong><br />
Remi Torracinta, Laurent Mesnard, Susan Levine, Rita Shaknovich, Maureen Hanson, Fabien Campagne<br />
<em>Cold Spring Harbor Laboratory</em> (2016-10-04) <a href="https://doi.org/gcgk9d">https://doi.org/gcgk9d</a><br />
DOI: <a href="https://doi.org/10.1101/079087">10.1101/079087</a></p>
</div>
<div id="ref-WNE8N7Cp">
<p>369. <strong>Toward an Integration of Deep Learning and Neuroscience</strong><br />
Adam H. Marblestone, Greg Wayne, Konrad P. Kording<br />
<em>Frontiers in Computational Neuroscience</em> (2016-09-14) <a href="https://doi.org/gcsgr2">https://doi.org/gcsgr2</a><br />
DOI: <a href="https://doi.org/10.3389/fncom.2016.00094">10.3389/fncom.2016.00094</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27683554">27683554</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5021692">PMC5021692</a></p>
</div>
<div id="ref-Exe9wdYF">
<p>370. <strong>Deep Neural Networks In Computational Neuroscience</strong><br />
Tim Christian Kietzmann, Patrick McClure, Nikolaus Kriegeskorte<br />
<em>Cold Spring Harbor Laboratory</em> (2017-05-04) <a href="https://doi.org/gcsgrx">https://doi.org/gcsgrx</a><br />
DOI: <a href="https://doi.org/10.1101/133504">10.1101/133504</a></p>
</div>
<div id="ref-Oc2wJ1JO">
<p>371. <strong>Neuroscience-Inspired Artificial Intelligence</strong><br />
Demis Hassabis, Dharshan Kumaran, Christopher Summerfield, Matthew Botvinick<br />
<em>Neuron</em> (2017-07) <a href="https://doi.org/gbp987">https://doi.org/gbp987</a><br />
DOI: <a href="https://doi.org/10.1016/j.neuron.2017.06.011">10.1016/j.neuron.2017.06.011</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28728020">28728020</a></p>
</div>
<div id="ref-EGwetrwp">
<p>372. <strong>Using goal-driven deep learning models to understand sensory cortex</strong><br />
Daniel LK Yamins, James J DiCarlo<br />
<em>Nature Neuroscience</em> (2016-02-23) <a href="https://doi.org/gcsgrw">https://doi.org/gcsgrw</a><br />
DOI: <a href="https://doi.org/10.1038/nn.4244">10.1038/nn.4244</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26906502">26906502</a></p>
</div>
<div id="ref-1E5vZzUF2">
<p>373. <strong>Inferring single-trial neural population dynamics using sequential auto-encoders</strong><br />
Chethan Pandarinath, Daniel J. O’Shea, Jasmine Collins, Rafal Jozefowicz, Sergey D. Stavisky, Jonathan C. Kao, Eric M. Trautmann, Matthew T. Kaufman, Stephen I. Ryu, Leigh R. Hochberg, … David Sussillo<br />
<em>Cold Spring Harbor Laboratory</em> (2017-06-20) <a href="https://doi.org/gcsgrz">https://doi.org/gcsgrz</a><br />
DOI: <a href="https://doi.org/10.1101/152884">10.1101/152884</a></p>
</div>
<div id="ref-1bJ0G7FE">
<p>374. <strong>Machines that learn to segment images: a crucial technology for connectomics</strong><br />
Viren Jain, H Sebastian Seung, Srinivas C Turaga<br />
<em>Current Opinion in Neurobiology</em> (2010-10) <a href="https://doi.org/b23662">https://doi.org/b23662</a><br />
DOI: <a href="https://doi.org/10.1016/j.conb.2010.07.004">10.1016/j.conb.2010.07.004</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/20801638">20801638</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2975605">PMC2975605</a></p>
</div>
<div id="ref-JaDImylU">
<p>375. <strong>Model-based Bayesian inference of neural activity and connectivity from all-optical interrogation of a neural circuit</strong><br />
Laurence Aitchison, Lloyd Russell, Adam M Packer, Jinyao Yan, Philippe Castonguay, Michael Hausser, Srinivas C Turaga<br />
<em>Advances in Neural Information Processing Systems 30</em> (2017) <a href="http://papers.nips.cc/paper/6940-model-based-bayesian-inference-of-neural-activity-and-connectivity-from-all-optical-interrogation-of-a-neural-circuit.pdf">http://papers.nips.cc/paper/6940-model-based-bayesian-inference-of-neural-activity-and-connectivity-from-all-optical-interrogation-of-a-neural-circuit.pdf</a></p>
</div>
<div id="ref-VOQtVhWs">
<p>376. <strong>The Path to Personalized Medicine</strong><br />
Margaret A. Hamburg, Francis S. Collins<br />
<em>New England Journal of Medicine</em> (2010-07-22) <a href="https://doi.org/bp78cs">https://doi.org/bp78cs</a><br />
DOI: <a href="https://doi.org/10.1056/nejmp1006304">10.1056/nejmp1006304</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/20551152">20551152</a></p>
</div>
<div id="ref-3JyJ3DTh">
<p>377. <strong>Biomedical Informatics for Computer-Aided Decision Support Systems: A Survey</strong><br />
Ashwin Belle, Mark A. Kon, Kayvan Najarian<br />
<em>The Scientific World Journal</em> (2013) <a href="https://doi.org/gb7x73">https://doi.org/gb7x73</a><br />
DOI: <a href="https://doi.org/10.1155/2013/769639">10.1155/2013/769639</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/23431259">23431259</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3575619">PMC3575619</a></p>
</div>
<div id="ref-kL0B4m9d">
<p>378. <strong>Advantages and disadvantages of using artificial neural networks versus logistic regression for predicting medical outcomes</strong><br />
Jack V. Tu<br />
<em>Journal of Clinical Epidemiology</em> (1996-11) <a href="https://doi.org/fg794g">https://doi.org/fg794g</a><br />
DOI: <a href="https://doi.org/10.1016/s0895-4356(96)00002-9">10.1016/s0895-4356(96)00002-9</a></p>
</div>
<div id="ref-jdg2u7bX">
<p>379. <strong>Use of an Artificial Neural Network for the Diagnosis of Myocardial Infarction</strong><br />
William G. Baxt<br />
<em>Annals of Internal Medicine</em> (1991-12-01) <a href="https://doi.org/gcgmc5">https://doi.org/gcgmc5</a><br />
DOI: <a href="https://doi.org/10.7326/0003-4819-115-11-843">10.7326/0003-4819-115-11-843</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/1952470">1952470</a></p>
</div>
<div id="ref-xX68eyvs">
<p>380. <strong>Clinical Prediction Rules</strong><br />
John H. Wasson, Harold C. Sox, Raymond K. Neff, Lee Goldman<br />
<em>New England Journal of Medicine</em> (1985-09-26) <a href="https://doi.org/d7mchc">https://doi.org/d7mchc</a><br />
DOI: <a href="https://doi.org/10.1056/nejm198509263131306">10.1056/nejm198509263131306</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/3897864">3897864</a></p>
</div>
<div id="ref-qxxwkSAT">
<p>381. <strong>The use of artificial neural networks in decision support in cancer: A systematic review</strong><br />
Paulo J. Lisboa, Azzam F. G. Taktak<br />
<em>Neural Networks</em> (2006-05) <a href="https://doi.org/cnvgmv">https://doi.org/cnvgmv</a><br />
DOI: <a href="https://doi.org/10.1016/j.neunet.2005.10.007">10.1016/j.neunet.2005.10.007</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/16483741">16483741</a></p>
</div>
<div id="ref-cpNVdlL7">
<p>382. <strong>Estimating causal effects of treatments in randomized and nonrandomized studies.</strong><br />
Donald B. Rubin<br />
<em>Journal of Educational Psychology</em> (1974) <a href="https://doi.org/cqq7sc">https://doi.org/cqq7sc</a><br />
DOI: <a href="https://doi.org/10.1037/h0037350">10.1037/h0037350</a></p>
</div>
<div id="ref-173ftiSzF">
<p>383. <strong>Learning Representations for Counterfactual Inference</strong><br />
Fredrik D. Johansson, Uri Shalit, David Sontag<br />
<em>arXiv</em> (2016-05-12) <a href="https://arxiv.org/abs/1605.03661v3">https://arxiv.org/abs/1605.03661v3</a></p>
</div>
<div id="ref-1GRT18Tt2">
<p>384. <strong>Causal Phenotype Discovery via Deep Networks</strong><br />
David C Kale, Zhengping Che, Mohammad Taha Bahadori, Wenzhe Li, Yan Liu, Randall Wetzel<br />
<em>AMIA … Annual Symposium proceedings. AMIA Symposium</em> (2015-11-05) <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4765623/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4765623/</a><br />
PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26958203">26958203</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4765623">PMC4765623</a></p>
</div>
<div id="ref-4zpZxjHR">
<p>385. <strong>Modeling Missing Data in Clinical Time Series with RNNs</strong><br />
Zachary C. Lipton, David C. Kale, Randall Wetzel<br />
<em>arXiv</em> (2016-06-13) <a href="https://arxiv.org/abs/1606.04130v5">https://arxiv.org/abs/1606.04130v5</a></p>
</div>
<div id="ref-O7Vbecm2">
<p>386. <strong>Recurrent Neural Networks for Multivariate Time Series with Missing Values</strong><br />
Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, Yan Liu<br />
<em>arXiv</em> (2016-06-06) <a href="https://arxiv.org/abs/1606.01865v2">https://arxiv.org/abs/1606.01865v2</a></p>
</div>
<div id="ref-fOaBA9Vc">
<p>387. <strong>Predicting Complications in Critical Care Using Heterogeneous Clinical Data</strong><br />
Vijay Huddar, Bapu Koundinya Desiraju, Vaibhav Rajan, Sakyajit Bhattacharya, Shourya Roy, Chandan K. Reddy<br />
<em>IEEE Access</em> (2016) <a href="https://doi.org/gcgk94">https://doi.org/gcgk94</a><br />
DOI: <a href="https://doi.org/10.1109/access.2016.2618775">10.1109/access.2016.2618775</a></p>
</div>
<div id="ref-glyI7H6F">
<p>388. <strong>Phenotyping of Clinical Time Series with LSTM Recurrent Neural Networks</strong><br />
Zachary C. Lipton, David C. Kale, Randall C. Wetzel<br />
<em>arXiv</em> (2015-10-26) <a href="https://arxiv.org/abs/1510.07641v2">https://arxiv.org/abs/1510.07641v2</a></p>
</div>
<div id="ref-16OQvsRqJ">
<p>389. <strong>Optimal medication dosing from suboptimal clinical examples: A deep reinforcement learning approach</strong><br />
Shamim Nemati, Mohammad M. Ghassemi, Gari D. Clifford<br />
<em>2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</em> (2016-08) <a href="https://doi.org/gcgk98">https://doi.org/gcgk98</a><br />
DOI: <a href="https://doi.org/10.1109/embc.2016.7591355">10.1109/embc.2016.7591355</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28268938">28268938</a></p>
</div>
<div id="ref-eCrLGgiX">
<p>390. <strong>From vital signs to clinical outcomes for patients with sepsis: a machine learning basis for a clinical decision support system</strong><br />
Eren Gultepe, Jeffrey P Green, Hien Nguyen, Jason Adams, Timothy Albertson, Ilias Tagkopoulos<br />
<em>Journal of the American Medical Informatics Association</em> (2014-03) <a href="https://doi.org/f5trc2">https://doi.org/f5trc2</a><br />
DOI: <a href="https://doi.org/10.1136/amiajnl-2013-001815">10.1136/amiajnl-2013-001815</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/23959843">23959843</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3932455">PMC3932455</a></p>
</div>
<div id="ref-eehGXQlY">
<p>391. <strong>Imaging-based enrichment criteria using deep learning algorithms for efficient clinical trials in mild cognitive impairment</strong><br />
Vamsi K. Ithapu, Vikas Singh, Ozioma C. Okonkwo, Richard J. Chappell, N. Maritza Dowling, Sterling C. Johnson<br />
<em>Alzheimer’s &amp; Dementia</em> (2015-12) <a href="https://doi.org/f73fvf">https://doi.org/f73fvf</a><br />
DOI: <a href="https://doi.org/10.1016/j.jalz.2015.01.010">10.1016/j.jalz.2015.01.010</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26093156">26093156</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4684492">PMC4684492</a></p>
</div>
<div id="ref-mo3GQwJj">
<p>392. <strong>Integrated deep learned transcriptomic and structure-based predictor of clinical trials outcomes</strong><br />
Artem V Artemov, Evgeny Putin, Quentin Vanhaelen, Alexander Aliper, Ivan V Ozerov, Alex Zhavoronkov<br />
<em>Cold Spring Harbor Laboratory</em> (2016-12-20) <a href="https://doi.org/gcgk9p">https://doi.org/gcgk9p</a><br />
DOI: <a href="https://doi.org/10.1101/095653">10.1101/095653</a></p>
</div>
<div id="ref-13c9OPizf">
<p>393. <strong>Innovation in the pharmaceutical industry: New estimates of R&amp;D costs</strong><br />
Joseph A. DiMasi, Henry G. Grabowski, Ronald W. Hansen<br />
<em>Journal of Health Economics</em> (2016-05) <a href="https://doi.org/f3mn5k">https://doi.org/f3mn5k</a><br />
DOI: <a href="https://doi.org/10.1016/j.jhealeco.2016.01.012">10.1016/j.jhealeco.2016.01.012</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26928437">26928437</a></p>
</div>
<div id="ref-79Ktl2">
<p>394. <strong>An analysis of the attrition of drug candidates from four major pharmaceutical companies</strong><br />
Michael J. Waring, John Arrowsmith, Andrew R. Leach, Paul D. Leeson, Sam Mandrell, Robert M. Owen, Garry Pairaudeau, William D. Pennie, Stephen D. Pickett, Jibo Wang, … Alex Weir<br />
<em>Nature Reviews Drug Discovery</em> (2015-06-19) <a href="https://doi.org/f7hqmh">https://doi.org/f7hqmh</a><br />
DOI: <a href="https://doi.org/10.1038/nrd4609">10.1038/nrd4609</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26091267">26091267</a></p>
</div>
<div id="ref-Ot5bUkmI">
<p>395. <strong>The Connectivity Map: Using Gene-Expression Signatures to Connect Small Molecules, Genes, and Disease</strong><br />
J. Lamb<br />
<em>Science</em> (2006-09-29) <a href="https://doi.org/c92ptt">https://doi.org/c92ptt</a><br />
DOI: <a href="https://doi.org/10.1126/science.1132939">10.1126/science.1132939</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/17008526">17008526</a></p>
</div>
<div id="ref-gTwjIQqB">
<p>396. <strong>A survey of current trends in computational drug repositioning</strong><br />
Jiao Li, Si Zheng, Bin Chen, Atul J. Butte, S. Joshua Swamidass, Zhiyong Lu<br />
<em>Briefings in Bioinformatics</em> (2015-03-31) <a href="https://doi.org/f78wph">https://doi.org/f78wph</a><br />
DOI: <a href="https://doi.org/10.1093/bib/bbv020">10.1093/bib/bbv020</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/25832646">25832646</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4719067">PMC4719067</a></p>
</div>
<div id="ref-1BkEtNVsj">
<p>397. <strong>A review of connectivity map and computational approaches in pharmacogenomics</strong><br />
Aliyu Musa, Laleh Soltan Ghoraie, Shu-Dong Zhang, Galina Galzko, Olli Yli-Harja, Matthias Dehmer, Benjamin Haibe-Kains, Frank Emmert-Streib<br />
<em>Briefings in Bioinformatics</em> (2017-01-09) <a href="https://doi.org/gcgk8w">https://doi.org/gcgk8w</a><br />
DOI: <a href="https://doi.org/10.1093/bib/bbw112">10.1093/bib/bbw112</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28069634">28069634</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5952941">PMC5952941</a></p>
</div>
<div id="ref-ir7ElHha">
<p>398. <strong>A review of validation strategies for computational drug repositioning</strong><br />
Adam S. Brown, Chirag J. Patel<br />
<em>Briefings in Bioinformatics</em> (2016-11-22) <a href="https://academic.oup.com/bib/article/doi/10.1093/bib/bbw110/2562646/A-review-of-validation-strategies-for">https://academic.oup.com/bib/article/doi/10.1093/bib/bbw110/2562646/A-review-of-validation-strategies-for</a><br />
DOI: <a href="https://doi.org/10.1093/bib/bbw110">10.1093/bib/bbw110</a></p>
</div>
<div id="ref-M1EW8Rfl">
<p>399. <strong>Drug repositioning: a machine-learning approach through data integration</strong><br />
Francesco Napolitano, Yan Zhao, Vânia M Moreira, Roberto Tagliaferri, Juha Kere, Mauro D’Amato, Dario Greco<br />
<em>Journal of Cheminformatics</em> (2013-06-22) <a href="https://doi.org/f243g5">https://doi.org/f243g5</a><br />
DOI: <a href="https://doi.org/10.1186/1758-2946-5-30">10.1186/1758-2946-5-30</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/23800010">23800010</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3704944">PMC3704944</a></p>
</div>
<div id="ref-16FEYidu2">
<p>400. <strong>Drug–Disease Association and Drug-Repositioning Predictions in Complex Diseases Using Causal Inference–Probabilistic Matrix Factorization</strong><br />
Jihong Yang, Zheng Li, Xiaohui Fan, Yiyu Cheng<br />
<em>Journal of Chemical Information and Modeling</em> (2014-08-22) <a href="https://doi.org/f6hpb4">https://doi.org/f6hpb4</a><br />
DOI: <a href="https://doi.org/10.1021/ci500340n">10.1021/ci500340n</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/25116798">25116798</a></p>
</div>
<div id="ref-18lqFDKRR">
<p>401. <strong>Drug repositioning for non-small cell lung cancer by using machine learning algorithms and topological graph theory</strong><br />
Chien-Hung Huang, Peter Mu-Hsin Chang, Chia-Wei Hsu, Chi-Ying F. Huang, Ka-Lok Ng<br />
<em>BMC Bioinformatics</em> (2016-01-11) <a href="https://doi.org/gcgmb7">https://doi.org/gcgmb7</a><br />
DOI: <a href="https://doi.org/10.1186/s12859-015-0845-0">10.1186/s12859-015-0845-0</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26817825">26817825</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4895785">PMC4895785</a></p>
</div>
<div id="ref-QcwZC8wG">
<p>402. <strong>Machine Learning Prediction of Cancer Cell Sensitivity to Drugs Based on Genomic and Chemical Properties</strong><br />
Michael P. Menden, Francesco Iorio, Mathew Garnett, Ultan McDermott, Cyril H. Benes, Pedro J. Ballester, Julio Saez-Rodriguez<br />
<em>PLoS ONE</em> (2013-04-30) <a href="https://doi.org/f4vfwd">https://doi.org/f4vfwd</a><br />
DOI: <a href="https://doi.org/10.1371/journal.pone.0061318">10.1371/journal.pone.0061318</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/23646105">23646105</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3640019">PMC3640019</a></p>
</div>
<div id="ref-ppGS5h4v">
<p>403. <strong>Large-scale integration of small-molecule induced genome-wide transcriptional responses, Kinome-wide binding affinities and cell-growth inhibition profiles reveal global trends characterizing systems-level drug action</strong><br />
Dušica Vidovic, Amar Koleti, Stephan C Schürer<br />
<em>Frontiers in Genetics</em> <a href="https://doi.org/10.3389/fgene.2014.00342">https://doi.org/10.3389/fgene.2014.00342</a><br />
DOI: <a href="https://doi.org/10.3389/fgene.2014.00342">10.3389/fgene.2014.00342</a></p>
</div>
<div id="ref-tOpadZQw">
<p>404. <strong>Computational Discovery of Putative Leads for Drug Repositioning through Drug-Target Interaction Prediction</strong><br />
Edgar D. Coelho, Joel P. Arrais, José Luís Oliveira<br />
<em>PLOS Computational Biology</em> (2016-11-28) <a href="https://doi.org/gcgmcj">https://doi.org/gcgmcj</a><br />
DOI: <a href="https://doi.org/10.1371/journal.pcbi.1005219">10.1371/journal.pcbi.1005219</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27893735">27893735</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5125559">PMC5125559</a></p>
</div>
<div id="ref-1SIuofeg">
<p>405. <strong>Large-Scale Off-Target Identification Using Fast and Accurate Dual Regularized One-Class Collaborative Filtering and Its Application to Drug Repurposing</strong><br />
Hansaim Lim, Aleksandar Poleksic, Yuan Yao, Hanghang Tong, Di He, Luke Zhuang, Patrick Meng, Lei Xie<br />
<em>PLOS Computational Biology</em> (2016-10-07) <a href="https://doi.org/f9ktgd">https://doi.org/f9ktgd</a><br />
DOI: <a href="https://doi.org/10.1371/journal.pcbi.1005135">10.1371/journal.pcbi.1005135</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27716836">27716836</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5055357">PMC5055357</a></p>
</div>
<div id="ref-TeIxEjqm">
<p>406. <strong>Pairwise input neural network for target-ligand interaction prediction</strong><br />
Caihua Wang, Juan Liu, Fei Luo, Yafang Tan, Zixin Deng, Qian-Nan Hu<br />
<em>2014 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</em> (2014-11) <a href="https://doi.org/gcgk95">https://doi.org/gcgk95</a><br />
DOI: <a href="https://doi.org/10.1109/bibm.2014.6999129">10.1109/bibm.2014.6999129</a></p>
</div>
<div id="ref-cQAldRdg">
<p>407. <strong>L1000CDS2: LINCS L1000 characteristic direction signatures search engine</strong><br />
Qiaonan Duan, St Patrick Reid, Neil R Clark, Zichen Wang, Nicolas F Fernandez, Andrew D Rouillard, Ben Readhead, Sarah R Tritsch, Rachel Hodos, Marc Hafner, … Avi Ma’ayan<br />
<em>npj Systems Biology and Applications</em> (2016-08-04) <a href="https://doi.org/gcgk8k">https://doi.org/gcgk8k</a><br />
DOI: <a href="https://doi.org/10.1038/npjsba.2016.15">10.1038/npjsba.2016.15</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28413689">28413689</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5389891">PMC5389891</a></p>
</div>
<div id="ref-RAadmvJN">
<p>408. <strong>Hit and lead generation: beyond high-throughput screening</strong><br />
Konrad H. Bleicher, Hans-Joachim Böhm, Klaus Müller, Alexander I. Alanine<br />
<em>Nature Reviews Drug Discovery</em> (2003-05) <a href="https://doi.org/c8crxp">https://doi.org/c8crxp</a><br />
DOI: <a href="https://doi.org/10.1038/nrd1086">10.1038/nrd1086</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/12750740">12750740</a></p>
</div>
<div id="ref-1D6emOV6q">
<p>409. <strong>Hit discovery and hit-to-lead approaches</strong><br />
György M. Keserű, Gergely M. Makara<br />
<em>Drug Discovery Today</em> (2006-08) <a href="https://doi.org/ffz3hv">https://doi.org/ffz3hv</a><br />
DOI: <a href="https://doi.org/10.1016/j.drudis.2006.06.016">10.1016/j.drudis.2006.06.016</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/16846802">16846802</a></p>
</div>
<div id="ref-cjj5vT3H">
<p>410. <strong>Influence Relevance Voting: An Accurate And Interpretable Virtual High Throughput Screening Method</strong><br />
S. Joshua Swamidass, Chloé-Agathe Azencott, Ting-Wan Lin, Hugo Gramajo, Shiou-Chuan Tsai, Pierre Baldi<br />
<em>Journal of Chemical Information and Modeling</em> (2009-03-26) <a href="https://doi.org/cw5jfr">https://doi.org/cw5jfr</a><br />
DOI: <a href="https://doi.org/10.1021/ci8004379">10.1021/ci8004379</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/19391629">19391629</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2750043">PMC2750043</a></p>
</div>
<div id="ref-uP7SgBVd">
<p>411. <strong>Modeling Industrial ADMET Data with Multitask Networks</strong><br />
Steven Kearnes, Brian Goldman, Vijay Pande<br />
<em>arXiv</em> (2016-06-28) <a href="https://arxiv.org/abs/1606.08793v3">https://arxiv.org/abs/1606.08793v3</a></p>
</div>
<div id="ref-7QsMcDYy">
<p>412. <strong>XenoSite: Accurately Predicting CYP-Mediated Sites of Metabolism with Neural Networks</strong><br />
Jed Zaretzki, Matthew Matlock, S. Joshua Swamidass<br />
<em>Journal of Chemical Information and Modeling</em> (2013-11-23) <a href="https://doi.org/f5nfz3">https://doi.org/f5nfz3</a><br />
DOI: <a href="https://doi.org/10.1021/ci400518g">10.1021/ci400518g</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/24224933">24224933</a></p>
</div>
<div id="ref-1Dzz0P0qr">
<p>413. <strong>Multi-task Neural Networks for QSAR Predictions</strong><br />
George E. Dahl, Navdeep Jaitly, Ruslan Salakhutdinov<br />
<em>arXiv</em> (2014-06-04) <a href="https://arxiv.org/abs/1406.1231v1">https://arxiv.org/abs/1406.1231v1</a></p>
</div>
<div id="ref-xOaTIeBY">
<p>414. <strong>Deep Neural Nets as a Method for Quantitative Structure–Activity Relationships</strong><br />
Junshui Ma, Robert P. Sheridan, Andy Liaw, George E. Dahl, Vladimir Svetnik<br />
<em>Journal of Chemical Information and Modeling</em> (2015-02-17) <a href="https://doi.org/f6358c">https://doi.org/f6358c</a><br />
DOI: <a href="https://doi.org/10.1021/ci500747n">10.1021/ci500747n</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/25635324">25635324</a></p>
</div>
<div id="ref-KJCJKadA">
<p>415. <strong>Did Kaggle Predict Drug Candidate Activities? Or Not?</strong><br />
Derek Lowe 11 December, 2012<br />
<em>In the Pipeline</em> (2012-12-11) <a href="https://blogs.sciencemag.org/pipeline/archives/2012/12/11/did_kaggle_predict_drug_candidate_activities_or_not">https://blogs.sciencemag.org/pipeline/archives/2012/12/11/did_kaggle_predict_drug_candidate_activities_or_not</a></p>
</div>
<div id="ref-F8fP2vAg">
<p>416. <strong>Deep learning as an opportunity in virtual screening</strong><br />
Thomas Unterthiner, Andreas Mayr, Günter Klambauer, Marvin Steijaert, Jörg K. Wegner, Hugo Ceulemans, Sepp Hochreiter<br />
<em>Neural Information Processing Systems 2014: Deep Learning and Representation Learning Workshop</em> (2014) <a href="http://www.dlworkshop.org/23.pdf?attredirects=0">http://www.dlworkshop.org/23.pdf?attredirects=0</a></p>
</div>
<div id="ref-yAoN5gTU">
<p>417. <strong>Massively Multitask Networks for Drug Discovery</strong><br />
Bharath Ramsundar, Steven Kearnes, Patrick Riley, Dale Webster, David Konerding, Vijay Pande<br />
<em>arXiv</em> (2015-02-06) <a href="https://arxiv.org/abs/1502.02072v1">https://arxiv.org/abs/1502.02072v1</a></p>
</div>
<div id="ref-Y1D0SZrO">
<p>418. <strong>DeepTox: Toxicity Prediction using Deep Learning</strong><br />
Andreas Mayr, Günter Klambauer, Thomas Unterthiner, Sepp Hochreiter<br />
<em>Frontiers in Environmental Science</em> (2016-02-02) <a href="https://doi.org/gcgmc3">https://doi.org/gcgmc3</a><br />
DOI: <a href="https://doi.org/10.3389/fenvs.2015.00080">10.3389/fenvs.2015.00080</a></p>
</div>
<div id="ref-B4cL1o2P">
<p>419. <strong>Computational Modeling of β-Secretase 1 (BACE-1) Inhibitors Using Ligand Based Approaches</strong><br />
Govindan Subramanian, Bharath Ramsundar, Vijay Pande, Rajiah Aldrin Denny<br />
<em>Journal of Chemical Information and Modeling</em> (2016-10-10) <a href="https://doi.org/f88ngh">https://doi.org/f88ngh</a><br />
DOI: <a href="https://doi.org/10.1021/acs.jcim.6b00290">10.1021/acs.jcim.6b00290</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27689393">27689393</a></p>
</div>
<div id="ref-WeiyYhfy">
<p>420. <strong>The enumeration of chemical space</strong><br />
Jean-Louis Reymond, Lars Ruddigkeit, Lorenz Blum, Ruud van Deursen<br />
<em>Wiley Interdisciplinary Reviews: Computational Molecular Science</em> (2012-04-18) <a href="https://doi.org/gcgk7f">https://doi.org/gcgk7f</a><br />
DOI: <a href="https://doi.org/10.1002/wcms.1104">10.1002/wcms.1104</a></p>
</div>
<div id="ref-1E0x7QgLP">
<p>421. <strong>Accurate and efficient target prediction using a potency-sensitive influence-relevance voter</strong><br />
Alessandro Lusci, David Fooshee, Michael Browning, Joshua Swamidass, Pierre Baldi<br />
<em>Journal of Cheminformatics</em> (2015-12) <a href="https://doi.org/f76hzq">https://doi.org/f76hzq</a><br />
DOI: <a href="https://doi.org/10.1186/s13321-015-0110-6">10.1186/s13321-015-0110-6</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26719774">26719774</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4696267">PMC4696267</a></p>
</div>
<div id="ref-17eGl2pn9">
<p>422. <strong>Molecular Descriptors for Chemoinformatics</strong><br />
Roberto Todeschini, Viviana Consonni (editors)<br />
<em>Methods and Principles in Medicinal Chemistry</em> (2009-07-15) <a href="https://doi.org/c9xk4r">https://doi.org/c9xk4r</a><br />
DOI: <a href="https://doi.org/10.1002/9783527628766">10.1002/9783527628766</a></p>
</div>
<div id="ref-QnZ7V9Rd">
<p>423. <strong>Extended-Connectivity Fingerprints</strong><br />
David Rogers, Mathew Hahn<br />
<em>Journal of Chemical Information and Modeling</em> (2010-04-28) <a href="https://doi.org/fp3ctj">https://doi.org/fp3ctj</a><br />
DOI: <a href="https://doi.org/10.1021/ci100050t">10.1021/ci100050t</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/20426451">20426451</a></p>
</div>
<div id="ref-qpmV0H2p">
<p>424. <strong>Automatic chemical design using a data-driven continuous representation of molecules</strong><br />
Rafael Gómez-Bombarelli, David Duvenaud, José Miguel Hernández-Lobato, Jorge Aguilera-Iparraguirre, Timothy D. Hirzel, Ryan P. Adams, Alán Aspuru-Guzik<br />
<em>arXiv</em> (2016-10-07) <a href="https://arxiv.org/abs/1610.02415v1">https://arxiv.org/abs/1610.02415v1</a></p>
</div>
<div id="ref-PMuw2Jdj">
<p>425. <strong>Chemception: A Deep Neural Network with Minimal Chemistry Knowledge Matches the Performance of Expert-developed QSAR/QSPR Models</strong><br />
Garrett B. Goh, Charles Siegel, Abhinav Vishnu, Nathan O. Hodas, Nathan Baker<br />
<em>arXiv</em> (2017-06-20) <a href="https://arxiv.org/abs/1706.06689v1">https://arxiv.org/abs/1706.06689v1</a></p>
</div>
<div id="ref-Oe573FaL">
<p>426. <strong>Convolutional Networks on Graphs for Learning Molecular Fingerprints</strong><br />
David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alan Aspuru-Guzik, Ryan P Adams<br />
<em>Advances in Neural Information Processing Systems 28</em> (2015) <a href="http://papers.nips.cc/paper/5954-convolutional-networks-on-graphs-for-learning-molecular-fingerprints.pdf">http://papers.nips.cc/paper/5954-convolutional-networks-on-graphs-for-learning-molecular-fingerprints.pdf</a></p>
</div>
<div id="ref-17Wih4Hd5">
<p>427. <strong>Deep Architectures and Deep Learning in Chemoinformatics: The Prediction of Aqueous Solubility for Drug-Like Molecules</strong><br />
Alessandro Lusci, Gianluca Pollastri, Pierre Baldi<br />
<em>Journal of Chemical Information and Modeling</em> (2013-07-02) <a href="https://doi.org/f48nb7">https://doi.org/f48nb7</a><br />
DOI: <a href="https://doi.org/10.1021/ci400187y">10.1021/ci400187y</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/23795551">23795551</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3739985">PMC3739985</a></p>
</div>
<div id="ref-145os4Y6t">
<p>428. <strong>Molecular graph convolutions: moving beyond fingerprints</strong><br />
Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, Patrick Riley<br />
<em>Journal of Computer-Aided Molecular Design</em> (2016-08) <a href="https://doi.org/f86bbs">https://doi.org/f86bbs</a><br />
DOI: <a href="https://doi.org/10.1007/s10822-016-9938-8">10.1007/s10822-016-9938-8</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27558503">27558503</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5028207">PMC5028207</a></p>
</div>
<div id="ref-P4ixsM8i">
<p>429. <strong>Low Data Drug Discovery with One-Shot Learning</strong><br />
Han Altae-Tran, Bharath Ramsundar, Aneesh S. Pappu, Vijay Pande<br />
<em>ACS Central Science</em> (2017-04-03) <a href="https://doi.org/f95dnd">https://doi.org/f95dnd</a><br />
DOI: <a href="https://doi.org/10.1021/acscentsci.6b00367">10.1021/acscentsci.6b00367</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28470045">28470045</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5408335">PMC5408335</a></p>
</div>
<div id="ref-UKHPpRBn">
<p>430. <strong>Convolutional Embedding of Attributed Molecular Graphs for Physical Property Prediction</strong><br />
Connor W. Coley, Regina Barzilay, William H. Green, Tommi S. Jaakkola, Klavs F. Jensen<br />
<em>Journal of Chemical Information and Modeling</em> (2017-07-25) <a href="https://doi.org/gbpqrq">https://doi.org/gbpqrq</a><br />
DOI: <a href="https://doi.org/10.1021/acs.jcim.6b00601">10.1021/acs.jcim.6b00601</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28696688">28696688</a></p>
</div>
<div id="ref-dVi4xCKj">
<p>431. <strong>Learning a Local-Variable Model of Aromatic and Conjugated Systems</strong><br />
Matthew K. Matlock, Na Le Dang, S. Joshua Swamidass<br />
<em>ACS Central Science</em> (2018-01-03) <a href="https://doi.org/gcsgr4">https://doi.org/gcsgr4</a><br />
DOI: <a href="https://doi.org/10.1021/acscentsci.7b00405">10.1021/acscentsci.7b00405</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/29392176">29392176</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5785769">PMC5785769</a></p>
</div>
<div id="ref-nMXQBadV">
<p>432. <strong>Covariant Compositional Networks For Learning Graphs</strong><br />
Risi Kondor, Hy Truong Son, Horace Pan, Brandon Anderson, Shubhendu Trivedi<br />
<em>arXiv</em> (2018-01-07) <a href="https://arxiv.org/abs/1801.02144v1">https://arxiv.org/abs/1801.02144v1</a></p>
</div>
<div id="ref-11QhcW8tX">
<p>433. <strong>MoleculeNet: a benchmark for molecular machine learning</strong><br />
Zhenqin Wu, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S. Pappu, Karl Leswing, Vijay Pande<br />
<em>Chemical Science</em> (2018) <a href="https://doi.org/gcsgr5">https://doi.org/gcsgr5</a><br />
DOI: <a href="https://doi.org/10.1039/c7sc02664a">10.1039/c7sc02664a</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/29629118">29629118</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5868307">PMC5868307</a></p>
</div>
<div id="ref-bbJMoSfn">
<p>434. <strong>What do we know and when do we know it?</strong><br />
Anthony Nicholls<br />
<em>Journal of Computer-Aided Molecular Design</em> (2008-02-06) <a href="https://doi.org/cfp6g6">https://doi.org/cfp6g6</a><br />
DOI: <a href="https://doi.org/10.1007/s10822-008-9170-2">10.1007/s10822-008-9170-2</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/18253702">18253702</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2270923">PMC2270923</a></p>
</div>
<div id="ref-Ytvk62dX">
<p>435. <strong>deepchem/deepchem</strong><br />
GitHub<br />
(2017) <a href="https://github.com/deepchem/deepchem">https://github.com/deepchem/deepchem</a></p>
</div>
<div id="ref-5GEmrHBd">
<p>436. <strong>Mol2vec: Unsupervised Machine Learning Approach with Chemical Intuition</strong><br />
Sabrina Jaeger, Simone Fulle, Samo Turk<br />
<em>Journal of Chemical Information and Modeling</em> (2018-01-10) <a href="https://doi.org/gcsgwx">https://doi.org/gcsgwx</a><br />
DOI: <a href="https://doi.org/10.1021/acs.jcim.7b00616">10.1021/acs.jcim.7b00616</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/29268609">29268609</a></p>
</div>
<div id="ref-13iyYvEcB">
<p>437. <strong>Structure-Based Virtual Screening for Drug Discovery: a Problem-Centric Review</strong><br />
Tiejun Cheng, Qingliang Li, Zhigang Zhou, Yanli Wang, Stephen H. Bryant<br />
<em>The AAPS Journal</em> (2012-01-27) <a href="https://doi.org/fxjg96">https://doi.org/fxjg96</a><br />
DOI: <a href="https://doi.org/10.1208/s12248-012-9322-0">10.1208/s12248-012-9322-0</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/22281989">22281989</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3282008">PMC3282008</a></p>
</div>
<div id="ref-17YaKNLKk">
<p>438. <strong>Atomic Convolutional Networks for Predicting Protein-Ligand Binding Affinity</strong><br />
Joseph Gomes, Bharath Ramsundar, Evan N. Feinberg, Vijay S. Pande<br />
<em>arXiv</em> (2017-03-30) <a href="https://arxiv.org/abs/1703.10603v1">https://arxiv.org/abs/1703.10603v1</a></p>
</div>
<div id="ref-Oc6JOTS6">
<p>439. <strong>TopologyNet: Topology based deep convolutional and multi-task neural networks for biomolecular property predictions</strong><br />
Zixuan Cang, Guo-Wei Wei<br />
<em>PLOS Computational Biology</em> (2017-07-27) <a href="https://doi.org/gbp9ps">https://doi.org/gbp9ps</a><br />
DOI: <a href="https://doi.org/10.1371/journal.pcbi.1005690">10.1371/journal.pcbi.1005690</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28749969">28749969</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5549771">PMC5549771</a></p>
</div>
<div id="ref-YO41GAOP">
<p>440. <strong>The PDBbind Database:  Methodologies and Updates</strong><br />
Renxiao Wang, Xueliang Fang, Yipin Lu, Chao-Yie Yang, Shaomeng Wang<br />
<em>Journal of Medicinal Chemistry</em> (2005-06) <a href="https://doi.org/djbvfc">https://doi.org/djbvfc</a><br />
DOI: <a href="https://doi.org/10.1021/jm048957q">10.1021/jm048957q</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/15943484">15943484</a></p>
</div>
<div id="ref-Gue0c5Gb">
<p>441. <strong>Boosting Docking-Based Virtual Screening with Deep Learning</strong><br />
Janaina Cruz Pereira, Ernesto Raúl Caffarena, Cicero Nogueira dos Santos<br />
<em>Journal of Chemical Information and Modeling</em> (2016-11-29) <a href="https://doi.org/f9jhpn">https://doi.org/f9jhpn</a><br />
DOI: <a href="https://doi.org/10.1021/acs.jcim.6b00355">10.1021/acs.jcim.6b00355</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28024405">28024405</a></p>
</div>
<div id="ref-bNBiIiTt">
<p>442. <strong>Protein-Ligand Scoring with Convolutional Neural Networks</strong><br />
Matthew Ragoza, Joshua Hochuli, Elisa Idrobo, Jocelyn Sunseri, David Ryan Koes<br />
<em>arXiv</em> (2016-12-08) <a href="https://arxiv.org/abs/1612.02751v1">https://arxiv.org/abs/1612.02751v1</a></p>
</div>
<div id="ref-kJ4hy7E">
<p>443. <strong>Enabling future drug discovery by de novo design</strong><br />
Markus Hartenfeller, Gisbert Schneider<br />
<em>Wiley Interdisciplinary Reviews: Computational Molecular Science</em> (2011-04-25) <a href="https://doi.org/bv7hkf">https://doi.org/bv7hkf</a><br />
DOI: <a href="https://doi.org/10.1002/wcms.49">10.1002/wcms.49</a></p>
</div>
<div id="ref-omzv9ryI">
<p>444. <strong>De Novo Design at the Edge of Chaos</strong><br />
Petra Schneider, Gisbert Schneider<br />
<em>Journal of Medicinal Chemistry</em> (2016-02-16) <a href="https://doi.org/gcgk76">https://doi.org/gcgk76</a><br />
DOI: <a href="https://doi.org/10.1021/acs.jmedchem.5b01849">10.1021/acs.jmedchem.5b01849</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26881908">26881908</a></p>
</div>
<div id="ref-15y7iq6HF">
<p>445. <strong>Generating Sequences With Recurrent Neural Networks</strong><br />
Alex Graves<br />
<em>arXiv</em> (2013-08-04) <a href="https://arxiv.org/abs/1308.0850v5">https://arxiv.org/abs/1308.0850v5</a></p>
</div>
<div id="ref-8LWFFeYg">
<p>446. <strong>Generating Focussed Molecule Libraries for Drug Discovery with Recurrent Neural Networks</strong><br />
Marwin H. S. Segler, Thierry Kogej, Christian Tyrchan, Mark P. Waller<br />
<em>arXiv</em> (2017-01-05) <a href="https://arxiv.org/abs/1701.01329v1">https://arxiv.org/abs/1701.01329v1</a></p>
</div>
<div id="ref-AQ3N6Ayw">
<p>447. <strong>Grammar Variational Autoencoder</strong><br />
Matt J. Kusner, Brooks Paige, José Miguel Hernández-Lobato<br />
<em>arXiv</em> (2017-03-06) <a href="https://arxiv.org/abs/1703.01925v1">https://arxiv.org/abs/1703.01925v1</a></p>
</div>
<div id="ref-x1nE5icc">
<p>448. <strong>ChEMBL: a large-scale bioactivity database for drug discovery</strong><br />
A. Gaulton, L. J. Bellis, A. P. Bento, J. Chambers, M. Davies, A. Hersey, Y. Light, S. McGlinchey, D. Michalovich, B. Al-Lazikani, J. P. Overington<br />
<em>Nucleic Acids Research</em> (2011-09-23) <a href="https://doi.org/bs9shd">https://doi.org/bs9shd</a><br />
DOI: <a href="https://doi.org/10.1093/nar/gkr777">10.1093/nar/gkr777</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/21948594">21948594</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3245175">PMC3245175</a></p>
</div>
<div id="ref-1EayJRsI">
<p>449. <strong>Molecular De Novo Design through Deep Reinforcement Learning</strong><br />
Marcus Olivecrona, Thomas Blaschke, Ola Engkvist, Hongming Chen<br />
<em>arXiv</em> (2017-04-25) <a href="https://arxiv.org/abs/1704.07555v2">https://arxiv.org/abs/1704.07555v2</a></p>
</div>
<div id="ref-lERqKdZJ">
<p>450. <strong>Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control</strong><br />
Natasha Jaques, Shixiang Gu, Dzmitry Bahdanau, José Miguel Hernández-Lobato, Richard E. Turner, Douglas Eck<br />
<em>arXiv</em> (2016-11-09) <a href="https://arxiv.org/abs/1611.02796v9">https://arxiv.org/abs/1611.02796v9</a></p>
</div>
<div id="ref-D2B03NVK">
<p>451. <strong>Understanding deep learning requires rethinking generalization</strong><br />
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals<br />
<em>arXiv</em> (2016-11-10) <a href="https://arxiv.org/abs/1611.03530v2">https://arxiv.org/abs/1611.03530v2</a></p>
</div>
<div id="ref-bBG5t78u">
<p>452. <strong>Why does deep and cheap learning work so well?</strong><br />
Henry W. Lin, Max Tegmark, David Rolnick<br />
<em>arXiv</em> (2016-08-29) <a href="https://arxiv.org/abs/1608.08225v3">https://arxiv.org/abs/1608.08225v3</a></p>
</div>
<div id="ref-JNnkm5Zt">
<p>453. <strong>The relationship between Precision-Recall and ROC curves</strong><br />
Jesse Davis, Mark Goadrich<br />
<em>Proceedings of the 23rd international conference on Machine learning - ICML ’06</em> (2006) <a href="https://doi.org/fc8wzr">https://doi.org/fc8wzr</a><br />
DOI: <a href="https://doi.org/10.1145/1143844.1143874">10.1145/1143844.1143874</a></p>
</div>
<div id="ref-ZxQ49E8q">
<p>454. <strong>An open investigation of the reproducibility of cancer biology research</strong><br />
Timothy M Errington, Elizabeth Iorns, William Gunn, Fraser Elisabeth Tan, Joelle Lomax, Brian A Nosek<br />
<em>eLife</em> (2014-12-10) <a href="https://doi.org/gcnzrq">https://doi.org/gcnzrq</a><br />
DOI: <a href="https://doi.org/10.7554/elife.04333">10.7554/elife.04333</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/25490932">25490932</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4270077">PMC4270077</a></p>
</div>
<div id="ref-P9vKrhYz">
<p>455. <strong>Adversarial Examples, Uncertainty, and Transfer Testing Robustness in Gaussian Process Hybrid Deep Networks</strong><br />
John Bradshaw, Alexander G. de G. Matthews, Zoubin Ghahramani<br />
<em>arXiv</em> (2017-07-08) <a href="https://arxiv.org/abs/1707.02476v1">https://arxiv.org/abs/1707.02476v1</a></p>
</div>
<div id="ref-1GDnudDWl">
<p>456. <strong>What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?</strong><br />
Alex Kendall, Yarin Gal<br />
<em>arXiv</em> (2017-03-15) <a href="https://arxiv.org/abs/1703.04977v2">https://arxiv.org/abs/1703.04977v2</a></p>
</div>
<div id="ref-WZjlu7tr">
<p>457. <strong>Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics</strong><br />
Alex Kendall, Yarin Gal, Roberto Cipolla<br />
<em>arXiv</em> (2017-05-19) <a href="https://arxiv.org/abs/1705.07115v3">https://arxiv.org/abs/1705.07115v3</a></p>
</div>
<div id="ref-QJ6hYH8N">
<p>458. <strong>On Calibration of Modern Neural Networks</strong><br />
Chuan Guo, Geoff Pleiss, Yu Sun, Kilian Q. Weinberger<br />
<em>arXiv</em> (2017-06-14) <a href="https://arxiv.org/abs/1706.04599v2">https://arxiv.org/abs/1706.04599v2</a></p>
</div>
<div id="ref-ZhuDaoNw">
<p>459. <strong>Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods</strong><br />
John C. Platt<br />
<em>Advances in Large Margin Classifiers</em> (1999)</p>
</div>
<div id="ref-9SnNyc8Y">
<p>460. <strong>Confidence interval prediction for neural network models</strong><br />
G. Chryssolouris, M. Lee, A. Ramsey<br />
<em>IEEE Transactions on Neural Networks</em> (1996) <a href="https://doi.org/cfc8m8">https://doi.org/cfc8m8</a><br />
DOI: <a href="https://doi.org/10.1109/72.478409">10.1109/72.478409</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/18255575">18255575</a></p>
</div>
<div id="ref-Tkobp7Qj">
<p>461. <strong>A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks</strong><br />
Dan Hendrycks, Kevin Gimpel<br />
<em>arXiv</em> (2016-10-07) <a href="https://arxiv.org/abs/1610.02136v3">https://arxiv.org/abs/1610.02136v3</a></p>
</div>
<div id="ref-vJxaHm0b">
<p>462. <strong>Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks</strong><br />
Shiyu Liang, Yixuan Li, R. Srikant<br />
<em>arXiv</em> (2017-06-08) <a href="https://arxiv.org/abs/1706.02690v4">https://arxiv.org/abs/1706.02690v4</a></p>
</div>
<div id="ref-InL72p4N">
<p>463. <strong>Concrete Problems in AI Safety</strong><br />
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, Dan Mané<br />
<em>arXiv</em> (2016-06-21) <a href="https://arxiv.org/abs/1606.06565v2">https://arxiv.org/abs/1606.06565v2</a></p>
</div>
<div id="ref-b5zIzCEO">
<p>464. <strong>Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods</strong><br />
Nicholas Carlini, David Wagner<br />
<em>arXiv</em> (2017-05-20) <a href="https://arxiv.org/abs/1705.07263v2">https://arxiv.org/abs/1705.07263v2</a></p>
</div>
<div id="ref-1FDihfnM">
<p>465. <strong>Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning</strong><br />
Yarin Gal, Zoubin Ghahramani<br />
<em>arXiv</em> (2015-06-06) <a href="https://arxiv.org/abs/1506.02142v6">https://arxiv.org/abs/1506.02142v6</a></p>
</div>
<div id="ref-12PvceitW">
<p>466. <strong>Leveraging uncertainty information from deep neural networks for disease detection</strong><br />
Christian Leibig, Vaneeda Allken, Murat Seçkin Ayhan, Philipp Berens, Siegfried Wahl<br />
<em>Scientific Reports</em> (2017-12) <a href="https://doi.org/gcqgdc">https://doi.org/gcqgdc</a><br />
DOI: <a href="https://doi.org/10.1038/s41598-017-17876-z">10.1038/s41598-017-17876-z</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/29259224">29259224</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5736701">PMC5736701</a></p>
</div>
<div id="ref-ZhgOMnnD">
<p>467. <strong>Robustly representing uncertainty in deep neural networks through sampling</strong><br />
Patrick McClure, Nikolaus Kriegeskorte<br />
<em>arXiv</em> (2016-11-05) <a href="https://arxiv.org/abs/1611.01639v7">https://arxiv.org/abs/1611.01639v7</a></p>
</div>
<div id="ref-emWW7yUp">
<p>468. <strong>Bayesian Hypernetworks</strong><br />
David Krueger, Chin-Wei Huang, Riashat Islam, Ryan Turner, Alexandre Lacoste, Aaron Courville<br />
<em>arXiv</em> (2017-10-13) <a href="https://arxiv.org/abs/1710.04759v2">https://arxiv.org/abs/1710.04759v2</a></p>
</div>
<div id="ref-Jy2oz6xk">
<p>469. <strong>Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles</strong><br />
Balaji Lakshminarayanan, Alexander Pritzel, Charles Blundell<br />
<em>arXiv</em> (2016-12-05) <a href="https://arxiv.org/abs/1612.01474v3">https://arxiv.org/abs/1612.01474v3</a></p>
</div>
<div id="ref-LxptBw9l">
<p>470. <strong>Uncertainty in deep learning</strong><br />
Yarin Gal<br />
<em>PhD thesis, University of Cambridge</em> (2016) <a href="http://www.cs.ox.ac.uk/people/yarin.gal/website/thesis/thesis.pdf">http://www.cs.ox.ac.uk/people/yarin.gal/website/thesis/thesis.pdf</a></p>
</div>
<div id="ref-1AhGoHZP9">
<p>471. <strong>Do Deep Nets Really Need to be Deep?</strong><br />
Lei Jimmy Ba, Rich Caruana<br />
<em>arXiv</em> (2013-12-21) <a href="https://arxiv.org/abs/1312.6184v7">https://arxiv.org/abs/1312.6184v7</a></p>
</div>
<div id="ref-1AkF8Wsv7">
<p>472. <strong>Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images</strong><br />
Anh Nguyen, Jason Yosinski, Jeff Clune<br />
<em>arXiv</em> (2014-12-05) <a href="https://arxiv.org/abs/1412.1897v4">https://arxiv.org/abs/1412.1897v4</a></p>
</div>
<div id="ref-QwXSJhr0">
<p>473. <strong>“Why Should I Trust You?”: Explaining the Predictions of Any Classifier</strong><br />
Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin<br />
<em>arXiv</em> (2016-02-16) <a href="https://arxiv.org/abs/1602.04938v3">https://arxiv.org/abs/1602.04938v3</a></p>
</div>
<div id="ref-voh0OiT2">
<p>474. <strong>Visualizing and Understanding Convolutional Networks</strong><br />
Matthew D Zeiler, Rob Fergus<br />
<em>arXiv</em> (2013-11-12) <a href="https://arxiv.org/abs/1311.2901v3">https://arxiv.org/abs/1311.2901v3</a></p>
</div>
<div id="ref-Kk20paR7">
<p>475. <strong>Visualizing Deep Neural Network Decisions: Prediction Difference Analysis</strong><br />
Luisa M Zintgraf, Taco S Cohen, Tameem Adel, Max Welling<br />
<em>arXiv</em> (2017-02-15) <a href="https://arxiv.org/abs/1702.04595v1">https://arxiv.org/abs/1702.04595v1</a></p>
</div>
<div id="ref-niTcKl83">
<p>476. <strong>Interpretable Explanations of Black Boxes by Meaningful Perturbation</strong><br />
Ruth C. Fong, Andrea Vedaldi<br />
<em>2017 IEEE International Conference on Computer Vision (ICCV)</em> (2017-10) <a href="https://doi.org/gcsk62">https://doi.org/gcsk62</a><br />
DOI: <a href="https://doi.org/10.1109/iccv.2017.371">10.1109/iccv.2017.371</a></p>
</div>
<div id="ref-1YcKYTvO">
<p>477. <strong>Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps</strong><br />
Karen Simonyan, Andrea Vedaldi, Andrew Zisserman<br />
<em>arXiv</em> (2013-12-20) <a href="https://arxiv.org/abs/1312.6034v2">https://arxiv.org/abs/1312.6034v2</a></p>
</div>
<div id="ref-au5CLIOH">
<p>478. <strong>On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation</strong><br />
Sebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Müller, Wojciech Samek<br />
<em>PLOS ONE</em> (2015-07-10) <a href="https://doi.org/gcgmcp">https://doi.org/gcgmcp</a><br />
DOI: <a href="https://doi.org/10.1371/journal.pone.0130140">10.1371/journal.pone.0130140</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26161953">26161953</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4498753">PMC4498753</a></p>
</div>
<div id="ref-b1sc0cgP">
<p>479. <strong>Investigating the influence of noise and distractors on the interpretation of neural networks</strong><br />
Pieter-Jan Kindermans, Kristof Schütt, Klaus-Robert Müller, Sven Dähne<br />
<em>arXiv</em> (2016-11-22) <a href="https://arxiv.org/abs/1611.07270v1">https://arxiv.org/abs/1611.07270v1</a></p>
</div>
<div id="ref-f2L6isRj">
<p>480. <strong>Striving for Simplicity: The All Convolutional Net</strong><br />
Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, Martin Riedmiller<br />
<em>arXiv</em> (2014-12-21) <a href="https://arxiv.org/abs/1412.6806v3">https://arxiv.org/abs/1412.6806v3</a></p>
</div>
<div id="ref-vjXoJqO3">
<p>481. <strong>Salient Deconvolutional Networks</strong><br />
Aravindh Mahendran, Andrea Vedaldi<br />
<em>Computer Vision – ECCV 2016</em> (2016) <a href="https://doi.org/gcgk7p">https://doi.org/gcgk7p</a><br />
DOI: <a href="https://doi.org/10.1007/978-3-319-46466-4_8">10.1007/978-3-319-46466-4_8</a></p>
</div>
<div id="ref-RZsNSRDS">
<p>482. <strong>Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization</strong><br />
Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra<br />
<em>arXiv</em> (2016-10-07) <a href="https://arxiv.org/abs/1610.02391v3">https://arxiv.org/abs/1610.02391v3</a></p>
</div>
<div id="ref-WzFOJBiA">
<p>483. <strong>Axiomatic Attribution for Deep Networks</strong><br />
Mukund Sundararajan, Ankur Taly, Qiqi Yan<br />
<em>arXiv</em> (2017-03-04) <a href="https://arxiv.org/abs/1703.01365v2">https://arxiv.org/abs/1703.01365v2</a></p>
</div>
<div id="ref-DeOI1oGf">
<p>484. <strong>An unexpected unity among methods for interpreting model predictions</strong><br />
Scott Lundberg, Su-In Lee<br />
<em>arXiv</em> (2016-11-22) <a href="https://arxiv.org/abs/1611.07478v3">https://arxiv.org/abs/1611.07478v3</a></p>
</div>
<div id="ref-YBJdA6LJ">
<p>485. <strong>17. A Value for n-Person Games</strong><br />
L. S. Shapley<br />
<em>Contributions to the Theory of Games (AM-28), Volume II</em> (1953) <a href="https://doi.org/10.1515/9781400881970-018">https://doi.org/10.1515/9781400881970-018</a><br />
DOI: <a href="https://doi.org/10.1515/9781400881970-018">10.1515/9781400881970-018</a></p>
</div>
<div id="ref-19mGl6pfy">
<p>486. <strong>Understanding Deep Image Representations by Inverting Them</strong><br />
Aravindh Mahendran, Andrea Vedaldi<br />
<em>arXiv</em> (2014-11-26) <a href="https://arxiv.org/abs/1412.0035v1">https://arxiv.org/abs/1412.0035v1</a></p>
</div>
<div id="ref-VjsZbMSz">
<p>487. <strong>Maximum entropy methods for extracting the learned features of deep neural networks</strong><br />
Alex Finnegan, Jun S. Song<br />
<em>Cold Spring Harbor Laboratory</em> (2017-02-03) <a href="https://doi.org/gcgk9w">https://doi.org/gcgk9w</a><br />
DOI: <a href="https://doi.org/10.1101/105957">10.1101/105957</a></p>
</div>
<div id="ref-1FkT6C6oa">
<p>488. <strong>Visualizing Deep Convolutional Neural Networks Using Natural Pre-images</strong><br />
Aravindh Mahendran, Andrea Vedaldi<br />
<em>International Journal of Computer Vision</em> (2016-05-18) <a href="https://doi.org/gcgk7x">https://doi.org/gcgk7x</a><br />
DOI: <a href="https://doi.org/10.1007/s11263-016-0911-8">10.1007/s11263-016-0911-8</a></p>
</div>
<div id="ref-XLHInhc1">
<p>489. <strong>Inceptionism: Going Deeper into Neural Networks</strong><br />
Alexander Mordvintsev, Christopher Olah, Mike Tyka<br />
<em>Google Research Blog</em> (2015-06) <a href="http://googleresearch.blogspot.co.uk/2015/06/inceptionism-going-deeper-into-neural.html">http://googleresearch.blogspot.co.uk/2015/06/inceptionism-going-deeper-into-neural.html</a></p>
</div>
<div id="ref-UAAd9Uez">
<p>490. <strong>Visualizing Higher-Layer Features of a Deep Network</strong><br />
Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pascal Vincent<br />
<em>University of Montreal</em> (2009-06) <a href="http://www.iro.umontreal.ca/~lisa/publications2/index.php/publications/show/247">http://www.iro.umontreal.ca/~lisa/publications2/index.php/publications/show/247</a></p>
</div>
<div id="ref-17i18PMkR">
<p>491. <strong>Understanding Neural Networks Through Deep Visualization</strong><br />
Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, Hod Lipson<br />
<em>arXiv</em> (2015-06-22) <a href="https://arxiv.org/abs/1506.06579v1">https://arxiv.org/abs/1506.06579v1</a></p>
</div>
<div id="ref-haHzVaaz">
<p>492. <strong>Neural Machine Translation by Jointly Learning to Align and Translate</strong><br />
Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio<br />
<em>arXiv</em> (2014-09-01) <a href="https://arxiv.org/abs/1409.0473v7">https://arxiv.org/abs/1409.0473v7</a></p>
</div>
<div id="ref-yHn4SDRI">
<p>493. <strong>Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</strong><br />
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, Yoshua Bengio<br />
<em>arXiv</em> (2015-02-10) <a href="https://arxiv.org/abs/1502.03044v3">https://arxiv.org/abs/1502.03044v3</a></p>
</div>
<div id="ref-SAvEOARL">
<p>494. <strong>Genetic Architect: Discovering Genomic Structure with Learned Neural Architectures</strong><br />
Laura Deming, Sasha Targ, Nate Sauder, Diogo Almeida, Chun Jimmie Ye<br />
<em>arXiv</em> (2016-05-23) <a href="https://arxiv.org/abs/1605.07156v1">https://arxiv.org/abs/1605.07156v1</a></p>
</div>
<div id="ref-UcRbawKo">
<p>495. <strong>RETAIN: An Interpretable Predictive Model for Healthcare using Reverse Time Attention Mechanism</strong><br />
Edward Choi, Mohammad Taha Bahadori, Joshua A. Kulas, Andy Schuetz, Walter F. Stewart, Jimeng Sun<br />
<em>arXiv</em> (2016-08-19) <a href="https://arxiv.org/abs/1608.05745v4">https://arxiv.org/abs/1608.05745v4</a></p>
</div>
<div id="ref-10nDTiETi">
<p>496. <strong>GRAM: Graph-based Attention Model for Healthcare Representation Learning</strong><br />
Edward Choi, Mohammad Taha Bahadori, Le Song, Walter F. Stewart, Jimeng Sun<br />
<em>arXiv</em> (2016-11-21) <a href="https://arxiv.org/abs/1611.07012v3">https://arxiv.org/abs/1611.07012v3</a></p>
</div>
<div id="ref-oc44yBj0">
<p>497. <strong>Sequence learning with recurrent networks: analysis of internal representations</strong><br />
Joydeep Ghosh, Vijay Karamcheti<br />
<em>Science of Artificial Neural Networks</em> (1992-07-01) <a href="https://doi.org/fbrtf4">https://doi.org/fbrtf4</a><br />
DOI: <a href="https://doi.org/10.1117/12.140112">10.1117/12.140112</a></p>
</div>
<div id="ref-2cpYveR4">
<p>498. <strong>Visualizing and Understanding Recurrent Networks</strong><br />
Andrej Karpathy, Justin Johnson, Li Fei-Fei<br />
<em>arXiv</em> (2015-06-05) <a href="https://arxiv.org/abs/1506.02078v2">https://arxiv.org/abs/1506.02078v2</a></p>
</div>
<div id="ref-1Ad3UOefc">
<p>499. <strong>LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks</strong><br />
Hendrik Strobelt, Sebastian Gehrmann, Hanspeter Pfister, Alexander M. Rush<br />
<em>arXiv</em> (2016-06-23) <a href="https://arxiv.org/abs/1606.07461v2">https://arxiv.org/abs/1606.07461v2</a></p>
</div>
<div id="ref-10ViHstXn">
<p>500. <strong>Automatic Rule Extraction from Long Short Term Memory Networks</strong><br />
W. James Murdoch, Arthur Szlam<br />
<em>arXiv</em> (2017-02-08) <a href="https://arxiv.org/abs/1702.02540v2">https://arxiv.org/abs/1702.02540v2</a></p>
</div>
<div id="ref-136QC0Zul">
<p>501. <strong>Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</strong><br />
Alec Radford, Luke Metz, Soumith Chintala<br />
<em>arXiv</em> (2015-11-19) <a href="https://arxiv.org/abs/1511.06434v2">https://arxiv.org/abs/1511.06434v2</a></p>
</div>
<div id="ref-AK17eOgD">
<p>502. <strong>The Cancer Genome Atlas Pan-Cancer analysis project</strong><br />
John N WeinsteinEric A Collisson, Gordon B Mills, Kenna R Mills Shaw, Brad A Ozenberger, Kyle Ellrott, Ilya Shmulevich, Chris Sander, Joshua M Stuart<br />
<em>Nature Genetics</em> (2013-09-26) <a href="https://doi.org/f3nt5c">https://doi.org/f3nt5c</a><br />
DOI: <a href="https://doi.org/10.1038/ng.2764">10.1038/ng.2764</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/24071849">24071849</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3919969">PMC3919969</a></p>
</div>
<div id="ref-aWn0df1m">
<p>503. <strong>Extracting a Biologically Relevant Latent Space from Cancer Transcriptomes with Variational Autoencoders</strong><br />
Gregory P. Way, Casey S. Greene<br />
<em>Cold Spring Harbor Laboratory</em> (2017-08-11) <a href="https://doi.org/gcm44d">https://doi.org/gcm44d</a><br />
DOI: <a href="https://doi.org/10.1101/174474">10.1101/174474</a></p>
</div>
<div id="ref-rMz1OFc6">
<p>504. <strong>Evaluating deep variational autoencoders trained on pan-cancer gene expression</strong><br />
Gregory P. Way, Casey S. Greene<br />
<em>arXiv</em> (2017-11-13) <a href="https://arxiv.org/abs/1711.04828v1">https://arxiv.org/abs/1711.04828v1</a></p>
</div>
<div id="ref-zBCcUQOM">
<p>505. <strong>GANs for Biological Image Synthesis</strong><br />
Anton Osokin, Anatole Chessel, Rafael E. Carazo Salas, Federico Vaggi<br />
<em>arXiv</em> (2017-08-15) <a href="https://arxiv.org/abs/1708.04692v2">https://arxiv.org/abs/1708.04692v2</a></p>
</div>
<div id="ref-1EdCkau6d">
<p>506. <strong>CytoGAN: Generative Modeling of Cell Images</strong><br />
Peter Goldsborough, Nick Pawlowski, Juan C Caicedo, Shantanu Singh, Anne Carpenter<br />
<em>Cold Spring Harbor Laboratory</em> (2017-12-02) <a href="https://doi.org/gcm44f">https://doi.org/gcm44f</a><br />
DOI: <a href="https://doi.org/10.1101/227645">10.1101/227645</a></p>
</div>
<div id="ref-69wxD9y">
<p>507. <strong>Understanding Black-box Predictions via Influence Functions</strong><br />
Pang Wei Koh, Percy Liang<br />
<em>arXiv</em> (2017-03-14) <a href="https://arxiv.org/abs/1703.04730v2">https://arxiv.org/abs/1703.04730v2</a></p>
</div>
<div id="ref-QphVo2P2">
<p>508. <strong>ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models</strong><br />
Minsuk Kahng, Pierre Y. Andrews, Aditya Kalro, Duen Horng Chau<br />
<em>arXiv</em> (2017-04-06) <a href="https://arxiv.org/abs/1704.01942v2">https://arxiv.org/abs/1704.01942v2</a></p>
</div>
<div id="ref-AEc66xxR">
<p>509. <strong>Towards Better Analysis of Deep Convolutional Neural Networks</strong><br />
Mengchen Liu, Jiaxin Shi, Zhen Li, Chongxuan Li, Jun Zhu, Shixia Liu<br />
<em>arXiv</em> (2016-04-24) <a href="https://arxiv.org/abs/1604.07043v3">https://arxiv.org/abs/1604.07043v3</a></p>
</div>
<div id="ref-14DAmZTDg">
<p>510. <strong>Distilling Knowledge from Deep Networks with Applications to Healthcare Domain</strong><br />
Zhengping Che, Sanjay Purushotham, Robinder Khemani, Yan Liu<br />
<em>arXiv</em> (2015-12-11) <a href="https://arxiv.org/abs/1512.03542v1">https://arxiv.org/abs/1512.03542v1</a></p>
</div>
<div id="ref-ZUCVI5eU">
<p>511. <strong>Rationalizing Neural Predictions</strong><br />
Tao Lei, Regina Barzilay, Tommi Jaakkola<br />
<em>arXiv</em> (2016-06-13) <a href="https://arxiv.org/abs/1606.04155v2">https://arxiv.org/abs/1606.04155v2</a></p>
</div>
<div id="ref-IcdRxiDv">
<p>512. <strong>Learning multiple layers of features from tiny images</strong><br />
Alex Krizhevsky<br />
(2009) <a href="https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf">https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf</a></p>
</div>
<div id="ref-5tvnB4uW">
<p>513. <strong>Functional Knowledge Transfer for High-accuracy Prediction of Under-studied Biological Processes</strong><br />
Christopher Y. Park, Aaron K. Wong, Casey S. Greene, Jessica Rowland, Yuanfang Guan, Lars A. Bongo, Rebecca D. Burdine, Olga G. Troyanskaya<br />
<em>PLoS Computational Biology</em> (2013-03-14) <a href="https://doi.org/f4qtp9">https://doi.org/f4qtp9</a><br />
DOI: <a href="https://doi.org/10.1371/journal.pcbi.1002957">10.1371/journal.pcbi.1002957</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/23516347">23516347</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3597527">PMC3597527</a></p>
</div>
<div id="ref-11NHbWB1V">
<p>514. <strong>DeepAD: Alzheimer′s Disease Classification via Deep Convolutional Neural Networks using MRI and fMRI</strong><br />
Saman Sarraf, Danielle D. DeSouza, John Anderson, Ghassem Tofighi, <br />
<em>Cold Spring Harbor Laboratory</em> (2016-08-21) <a href="https://doi.org/gcgk9b">https://doi.org/gcgk9b</a><br />
DOI: <a href="https://doi.org/10.1101/070441">10.1101/070441</a></p>
</div>
<div id="ref-2M3zXijc">
<p>515. <strong>DeepBound: Accurate Identification of Transcript Boundaries via Deep Convolutional Neural Fields</strong><br />
Mingfu Shao, Jianzhu Ma, Sheng Wang<br />
<em>Cold Spring Harbor Laboratory</em> (2017-04-07) <a href="https://doi.org/gcgk92">https://doi.org/gcgk92</a><br />
DOI: <a href="https://doi.org/10.1101/125229">10.1101/125229</a></p>
</div>
<div id="ref-KxEzGxJ6">
<p>516. <strong>A general framework for estimating the relative pathogenicity of human genetic variants</strong><br />
Martin Kircher, Daniela M Witten, Preti Jain, Brian J O’Roak, Gregory M Cooper, Jay Shendure<br />
<em>Nature Genetics</em> (2014-02-02) <a href="https://doi.org/f5s57j">https://doi.org/f5s57j</a><br />
DOI: <a href="https://doi.org/10.1038/ng.2892">10.1038/ng.2892</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/24487276">24487276</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3992975">PMC3992975</a></p>
</div>
<div id="ref-15JUKBg9y">
<p>517. <strong>Diet Networks: Thin Parameters for Fat Genomics</strong><br />
Adriana Romero, Pierre Luc Carrier, Akram Erraqabi, Tristan Sylvain, Alex Auvolat, Etienne Dejoie, Marc-André Legault, Marie-Pierre Dubé, Julie G. Hussin, Yoshua Bengio<br />
<em>International Conference on Learning Representations 2017</em> (2016-11-04) <a href="https://openreview.net/forum?id=Sk-oDY9ge&amp;noteId=Sk-oDY9ge">https://openreview.net/forum?id=Sk-oDY9ge&amp;noteId=Sk-oDY9ge</a></p>
</div>
<div id="ref-BQS8ClV0">
<p>518. <strong>Deep learning in neural networks: An overview</strong><br />
Jürgen Schmidhuber<br />
<em>Neural Networks</em> (2015-01) <a href="https://doi.org/f6v78n">https://doi.org/f6v78n</a><br />
DOI: <a href="https://doi.org/10.1016/j.neunet.2014.09.003">10.1016/j.neunet.2014.09.003</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/25462637">25462637</a></p>
</div>
<div id="ref-CKcJuj03">
<p>519. <strong>Deep Learning with Limited Numerical Precision</strong><br />
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, Pritish Narayanan<br />
<em>arXiv</em> (2015-02-09) <a href="https://arxiv.org/abs/1502.02551v1">https://arxiv.org/abs/1502.02551v1</a></p>
</div>
<div id="ref-1G3owNNps">
<p>520. <strong>Training deep neural networks with low precision multiplications</strong><br />
Matthieu Courbariaux, Yoshua Bengio, Jean-Pierre David<br />
<em>arXiv</em> (2014-12-22) <a href="https://arxiv.org/abs/1412.7024v5">https://arxiv.org/abs/1412.7024v5</a></p>
</div>
<div id="ref-ybP8QCqL">
<p>521. <strong>Taming the Wild: A Unified Analysis of Hogwild!-Style Algorithms</strong><br />
Christopher De Sa, Ce Zhang, Kunle Olukotun, Christopher Ré<br />
<em>Advances in neural information processing systems</em> (2015-12) <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4907892/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4907892/</a><br />
PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27330264">27330264</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4907892">PMC4907892</a></p>
</div>
<div id="ref-1GUizyE8e">
<p>522. <strong>Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations</strong><br />
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, Yoshua Bengio<br />
<em>arXiv</em> (2016-09-22) <a href="https://arxiv.org/abs/1609.07061v1">https://arxiv.org/abs/1609.07061v1</a></p>
</div>
<div id="ref-1CRF3gAV">
<p>523. <strong>Distilling the Knowledge in a Neural Network</strong><br />
Geoffrey Hinton, Oriol Vinyals, Jeff Dean<br />
<em>arXiv</em> (2015-03-09) <a href="https://arxiv.org/abs/1503.02531v1">https://arxiv.org/abs/1503.02531v1</a></p>
</div>
<div id="ref-F3e4wfzQ">
<p>524. <strong>Large-scale deep unsupervised learning using graphics processors</strong><br />
Rajat Raina, Anand Madhavan, Andrew Y. Ng<br />
<em>Proceedings of the 26th Annual International Conference on Machine Learning - ICML ’09</em> (2009) <a href="https://doi.org/dfh65x">https://doi.org/dfh65x</a><br />
DOI: <a href="https://doi.org/10.1145/1553374.1553486">10.1145/1553374.1553486</a></p>
</div>
<div id="ref-NSgduYNT">
<p>525. <strong>Improving the speed of neural networks on CPUs</strong><br />
Vincent Vanhoucke, Andrew Senior, Mark Z. Mao<br />
<em>Deep Learning and Unsupervised Feature Learning Workshop, NIPS 2011</em> (2011)</p>
</div>
<div id="ref-IULiPa6L">
<p>526. <strong>On parallelizability of stochastic gradient descent for speech DNNS</strong><br />
Frank Seide, Hao Fu, Jasha Droppo, Gang Li, Dong Yu<br />
<em>2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em> (2014-05) <a href="https://doi.org/gcgk99">https://doi.org/gcgk99</a><br />
DOI: <a href="https://doi.org/10.1109/icassp.2014.6853593">10.1109/icassp.2014.6853593</a></p>
</div>
<div id="ref-13KjSCKB2">
<p>527. <strong>Caffe con Troll: Shallow Ideas to Speed Up Deep Learning</strong><br />
Stefan Hadjis, Firas Abuzaid, Ce Zhang, Christopher Ré<br />
<em>arXiv</em> (2015-04-16) <a href="https://arxiv.org/abs/1504.04343v2">https://arxiv.org/abs/1504.04343v2</a></p>
</div>
<div id="ref-1FocAi7N0">
<p>528. <strong>Growing pains for deep learning</strong><br />
Chris Edwards<br />
<em>Communications of the ACM</em> (2015-06-25) <a href="https://doi.org/gcgmbz">https://doi.org/gcgmbz</a><br />
DOI: <a href="https://doi.org/10.1145/2771283">10.1145/2771283</a></p>
</div>
<div id="ref-aClNvbyM">
<p>529. <strong>Experiments on Parallel Training of Deep Neural Network using Model Averaging</strong><br />
Hang Su, Haoyu Chen<br />
<em>arXiv</em> (2015-07-05) <a href="https://arxiv.org/abs/1507.01239v3">https://arxiv.org/abs/1507.01239v3</a></p>
</div>
<div id="ref-fNkl8HFz">
<p>530. <strong>Efficient mini-batch training for stochastic optimization</strong><br />
Mu Li, Tong Zhang, Yuqiang Chen, Alexander J. Smola<br />
<em>Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD ’14</em> (2014) <a href="https://doi.org/gcgmbw">https://doi.org/gcgmbw</a><br />
DOI: <a href="https://doi.org/10.1145/2623330.2623612">10.1145/2623330.2623612</a></p>
</div>
<div id="ref-x0M6vals">
<p>531. <strong>CGBVS-DNN: Prediction of Compound-protein Interactions Based on Deep Learning</strong><br />
Masatoshi Hamanaka, Kei Taneishi, Hiroaki Iwata, Jun Ye, Jianguo Pei, Jinlong Hou, Yasushi Okuno<br />
<em>Molecular Informatics</em> (2016-08-12) <a href="https://doi.org/f3q59v">https://doi.org/f3q59v</a><br />
DOI: <a href="https://doi.org/10.1002/minf.201600045">10.1002/minf.201600045</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27515489">27515489</a></p>
</div>
<div id="ref-YwdqeYZi">
<p>532. <strong>cuDNN: Efficient Primitives for Deep Learning</strong><br />
Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen, John Tran, Bryan Catanzaro, Evan Shelhamer<br />
<em>arXiv</em> (2014-10-03) <a href="https://arxiv.org/abs/1410.0759v3">https://arxiv.org/abs/1410.0759v3</a></p>
</div>
<div id="ref-15lYGmZpY">
<p>533. <strong>Compressing Neural Networks with the Hashing Trick</strong><br />
Wenlin Chen, James T. Wilson, Stephen Tyree, Kilian Q. Weinberger, Yixin Chen<br />
<em>arXiv</em> (2015-04-19) <a href="https://arxiv.org/abs/1504.04788v1">https://arxiv.org/abs/1504.04788v1</a></p>
</div>
<div id="ref-9NKsJjSw">
<p>534. <strong>Deep Learning on FPGAs: Past, Present, and Future</strong><br />
Griffin Lacey, Graham W. Taylor, Shawki Areibi<br />
<em>arXiv</em> (2016-02-13) <a href="https://arxiv.org/abs/1602.04283v1">https://arxiv.org/abs/1602.04283v1</a></p>
</div>
<div id="ref-ULagTifF">
<p>535. <strong>In-Datacenter Performance Analysis of a Tensor Processing Unit</strong><br />
Norman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, … Doe Hyun Yoon<br />
<em>arXiv</em> (2017-04-16) <a href="https://arxiv.org/abs/1704.04760v1">https://arxiv.org/abs/1704.04760v1</a></p>
</div>
<div id="ref-xE3EYmck">
<p>536. <strong>MapReduce</strong><br />
Jeffrey Dean, Sanjay Ghemawat<br />
<em>Communications of the ACM</em> (2008-01-01) <a href="https://doi.org/br3wxw">https://doi.org/br3wxw</a><br />
DOI: <a href="https://doi.org/10.1145/1327452.1327492">10.1145/1327452.1327492</a></p>
</div>
<div id="ref-1XcexUAV">
<p>537. <strong>Distributed GraphLab</strong><br />
Yucheng Low, Danny Bickson, Joseph Gonzalez, Carlos Guestrin, Aapo Kyrola, Joseph M. Hellerstein<br />
<em>Proceedings of the VLDB Endowment</em> (2012-04-01) <a href="https://doi.org/gcgmcs">https://doi.org/gcgmcs</a><br />
DOI: <a href="https://doi.org/10.14778/2212351.2212354">10.14778/2212351.2212354</a></p>
</div>
<div id="ref-17cBimWgp">
<p>538. <strong>Large Scale Distributed Deep Networks</strong><br />
Jeffrey Dean, Greg S Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V Le, Mark Z Mao, Marc’Aurelio Ranzato, Andrew Senior, Paul Tucker, … Andrew Y Ng<br />
<em>Neural Information Processing Systems 2012</em> (2012-12) <a href="http://research.google.com/archive/large_deep_networks_nips2012.html">http://research.google.com/archive/large_deep_networks_nips2012.html</a></p>
</div>
<div id="ref-rmJZ2Aui">
<p>539. <strong>SparkNet: Training Deep Networks in Spark</strong><br />
Philipp Moritz, Robert Nishihara, Ion Stoica, Michael I. Jordan<br />
<em>arXiv</em> (2015-11-19) <a href="https://arxiv.org/abs/1511.06051v4">https://arxiv.org/abs/1511.06051v4</a></p>
</div>
<div id="ref-rZnxDitd">
<p>540. <strong>MLlib: Machine Learning in Apache Spark</strong><br />
Xiangrui Meng, Joseph Bradley, Burak Yavuz, Evan Sparks, Shivaram Venkataraman, Davies Liu, Jeremy Freeman, DB Tsai, Manish Amde, Sean Owen, … Ameet Talwalkar<br />
<em>arXiv</em> (2015-05-26) <a href="https://arxiv.org/abs/1505.06807v1">https://arxiv.org/abs/1505.06807v1</a></p>
</div>
<div id="ref-hOeUlCvS">
<p>541. <strong>TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems</strong><br />
Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, … Xiaoqiang Zheng<br />
<em>arXiv</em> (2016-03-14) <a href="https://arxiv.org/abs/1603.04467v2">https://arxiv.org/abs/1603.04467v2</a></p>
</div>
<div id="ref-FwEK0msb">
<p>542. <strong>fchollet/keras</strong><br />
GitHub<br />
(2017) <a href="https://github.com/fchollet/keras">https://github.com/fchollet/keras</a></p>
</div>
<div id="ref-y9IoEy4r">
<p>543. <strong>maxpumperla/elephas</strong><br />
GitHub<br />
(2017) <a href="https://github.com/maxpumperla/elephas">https://github.com/maxpumperla/elephas</a></p>
</div>
<div id="ref-4MZ2tmZ8">
<p>544. <strong>Deep learning with COTS HPC systems</strong><br />
Adam Coates, Brody Huval, Tao Wang, David Wu, Bryan Catanzaro, Ng Andrew<br />
<em>International Conference on Machine Learning</em> (2013-02-13) <a href="http://proceedings.mlr.press/v28/coates13.html">http://proceedings.mlr.press/v28/coates13.html</a></p>
</div>
<div id="ref-JUF9VoRD">
<p>545. <strong>Ensemble-Compression: A New Method for Parallel Training of Deep Neural Networks</strong><br />
Shizhao Sun, Wei Chen, Jiang Bian, Xiaoguang Liu, Tie-Yan Liu<br />
<em>arXiv</em> (2016-06-02) <a href="https://arxiv.org/abs/1606.00575v2">https://arxiv.org/abs/1606.00575v2</a></p>
</div>
<div id="ref-wz83yfHF">
<p>546. <strong>Algorithms for Hyper-parameter Optimization</strong><br />
James Bergstra, Rémi Bardenet, Yoshua Bengio, Balázs Kégl<br />
<em>Proceedings of the 24th International Conference on Neural Information Processing Systems</em> (2011) <a href="http://dl.acm.org/citation.cfm?id=2986459.2986743">http://dl.acm.org/citation.cfm?id=2986459.2986743</a><br />
ISBN: <a href="https://worldcat.org/isbn/978-1-61839-599-3">978-1-61839-599-3</a></p>
</div>
<div id="ref-1FSwIjR9s">
<p>547. <strong>Random Search for Hyper-Parameter Optimization</strong><br />
James Bergstra, Yoshua Bengio<br />
<em>Journal of Machine Learning Research</em> (2012) <a href="http://www.jmlr.org/papers/v13/bergstra12a.html">http://www.jmlr.org/papers/v13/bergstra12a.html</a></p>
</div>
<div id="ref-B6g0qKf4">
<p>548. <strong>Cloud computing and the DNA data race</strong><br />
Michael C Schatz, Ben Langmead, Steven L Salzberg<br />
<em>Nature Biotechnology</em> (2010-07) <a href="https://doi.org/cskgd3">https://doi.org/cskgd3</a><br />
DOI: <a href="https://doi.org/10.1038/nbt0710-691">10.1038/nbt0710-691</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/20622843">20622843</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2904649">PMC2904649</a></p>
</div>
<div id="ref-1E7bFCRV4">
<p>549. <strong>The real cost of sequencing: scaling computation to keep pace with data generation</strong><br />
Paul Muir, Shantao Li, Shaoke Lou, Daifeng Wang, Daniel J Spakowicz, Leonidas Salichos, Jing Zhang, George M. Weinstock, Farren Isaacs, Joel Rozowsky, Mark Gerstein<br />
<em>Genome Biology</em> (2016-03-23) <a href="https://doi.org/gcgmcb">https://doi.org/gcgmcb</a><br />
DOI: <a href="https://doi.org/10.1186/s13059-016-0917-0">10.1186/s13059-016-0917-0</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27009100">27009100</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4806511">PMC4806511</a></p>
</div>
<div id="ref-q0SsFrZd">
<p>550. <strong>The case for cloud computing in genome informatics</strong><br />
Lincoln D Stein<br />
<em>Genome Biology</em> (2010) <a href="https://doi.org/ach">https://doi.org/ach</a><br />
DOI: <a href="https://doi.org/10.1186/gb-2010-11-5-207">10.1186/gb-2010-11-5-207</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/20441614">20441614</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2898083">PMC2898083</a></p>
</div>
<div id="ref-ZSVsnPVO">
<p>551. <strong>One weird trick for parallelizing convolutional neural networks</strong><br />
Alex Krizhevsky<br />
<em>arXiv</em> (2014-04-23) <a href="https://arxiv.org/abs/1404.5997v2">https://arxiv.org/abs/1404.5997v2</a></p>
</div>
<div id="ref-ObFN78yp">
<p>552. <strong>A view of cloud computing</strong><br />
Michael Armbrust, Ion Stoica, Matei Zaharia, Armando Fox, Rean Griffith, Anthony D. Joseph, Randy Katz, Andy Konwinski, Gunho Lee, David Patterson, Ariel Rabkin<br />
<em>Communications of the ACM</em> (2010-04-01) <a href="https://doi.org/c4svd8">https://doi.org/c4svd8</a><br />
DOI: <a href="https://doi.org/10.1145/1721654.1721672">10.1145/1721654.1721672</a></p>
</div>
<div id="ref-o0F1MXBC">
<p>553. <strong>Data Sharing</strong><br />
Dan L. Longo, Jeffrey M. Drazen<br />
<em>New England Journal of Medicine</em> (2016-01-21) <a href="https://doi.org/gcgk8r">https://doi.org/gcgk8r</a><br />
DOI: <a href="https://doi.org/10.1056/nejme1516564">10.1056/nejme1516564</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26789876">26789876</a></p>
</div>
<div id="ref-194IoYUs3">
<p>554. <strong>Celebrating parasites</strong><br />
Casey S Greene, Lana X Garmire, Jack A Gilbert, Marylyn D Ritchie, Lawrence E Hunter<br />
<em>Nature Genetics</em> (2017-04) <a href="https://doi.org/gcgk8d">https://doi.org/gcgk8d</a><br />
DOI: <a href="https://doi.org/10.1038/ng.3830">10.1038/ng.3830</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28358134">28358134</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5710834">PMC5710834</a></p>
</div>
<div id="ref-wzHEnMZe">
<p>555. <strong>Is Multitask Deep Learning Practical for Pharma?</strong><br />
Bharath Ramsundar, Bowen Liu, Zhenqin Wu, Andreas Verras, Matthew Tudor, Robert P. Sheridan, Vijay Pande<br />
<em>Journal of Chemical Information and Modeling</em> (2017-08) <a href="https://doi.org/gcnzrm">https://doi.org/gcnzrm</a><br />
DOI: <a href="https://doi.org/10.1021/acs.jcim.7b00146">10.1021/acs.jcim.7b00146</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28692267">28692267</a></p>
</div>
<div id="ref-gvyja7v1">
<p>556. <strong>Enhancing reproducibility for computational methods</strong><br />
V. Stodden, M. McNutt, D. H. Bailey, E. Deelman, Y. Gil, B. Hanson, M. A. Heroux, J. P. A. Ioannidis, M. Taufer<br />
<em>Science</em> (2016-12-08) <a href="https://doi.org/gbr42b">https://doi.org/gbr42b</a><br />
DOI: <a href="https://doi.org/10.1126/science.aah6168">10.1126/science.aah6168</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27940837">27940837</a></p>
</div>
<div id="ref-117PEpTMe">
<p>557. <strong>DragoNN</strong><a href="http://kundajelab.github.io/dragonn/">http://kundajelab.github.io/dragonn/</a></p>
</div>
<div id="ref-enhj7VT6">
<p>558. <strong>How transferable are features in deep neural networks?</strong><br />
Jason Yosinski, Jeff Clune, Yoshua Bengio, Hod Lipson<br />
<em>Advances in Neural Information Processing Systems 27</em> (2014) <a href="http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf">http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf</a></p>
</div>
<div id="ref-HlDY7trA">
<p>559. <strong>Deep Model Based Transfer and Multi-Task Learning for Biological Image Analysis</strong><br />
Wenlu Zhang, Rongjian Li, Tao Zeng, Qian Sun, Sudhir Kumar, Jieping Ye, Shuiwang Ji<br />
<em>Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD ’15</em> (2015) <a href="https://doi.org/gcgmb2">https://doi.org/gcgmb2</a><br />
DOI: <a href="https://doi.org/10.1145/2783258.2783304">10.1145/2783258.2783304</a></p>
</div>
<div id="ref-z3I2IudI">
<p>560. <strong>Deep convolutional neural networks for annotating gene expression patterns in the mouse brain</strong><br />
Tao Zeng, Rongjian Li, Ravi Mukkamala, Jieping Ye, Shuiwang Ji<br />
<em>BMC Bioinformatics</em> (2015-05-07) <a href="https://doi.org/gb8w84">https://doi.org/gb8w84</a><br />
DOI: <a href="https://doi.org/10.1186/s12859-015-0553-9">10.1186/s12859-015-0553-9</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/25948335">25948335</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4432953">PMC4432953</a></p>
</div>
<div id="ref-2a7MHtAx">
<p>561. <strong>Accurate Classification of Protein Subcellular Localization from High-Throughput Microscopy Images Using Deep Learning</strong><br />
Tanel Pärnamaa, Leopold Parts<br />
<em>G3: Genes|Genomes|Genetics</em> (2017-04-08) <a href="https://doi.org/10.1534/g3.116.033654">https://doi.org/10.1534/g3.116.033654</a><br />
DOI: <a href="https://doi.org/10.1534/g3.116.033654">10.1534/g3.116.033654</a></p>
</div>
<div id="ref-DcnNfASG">
<p>562. <strong>Automated analysis of high‐content microscopy data with deep learning</strong><br />
Oren Z Kraus, Ben T Grys, Jimmy Ba, Yolanda Chong, Brendan J Frey, Charles Boone, Brenda J Andrews<br />
<em>Molecular Systems Biology</em> (2017-04) <a href="https://doi.org/f93cpr">https://doi.org/f93cpr</a><br />
DOI: <a href="https://doi.org/10.15252/msb.20177551">10.15252/msb.20177551</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28420678">28420678</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5408780">PMC5408780</a></p>
</div>
<div id="ref-1eN66lwn">
<p>563. <strong>Multimodal Deep Learning</strong><br />
Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, Andrew Y. Ng<br />
<em>Proceedings of the 28th International Conference on Machine Learning</em> (2011) <a href="https://ccrma.stanford.edu/~juhan/pubs/NgiamKhoslaKimNamLeeNg2011.pdf">https://ccrma.stanford.edu/~juhan/pubs/NgiamKhoslaKimNamLeeNg2011.pdf</a></p>
</div>
<div id="ref-obeRVckH">
<p>564. <strong>Deep Learning based multi-omics integration robustly predicts survival in liver cancer</strong><br />
Kumardeep Chaudhary, Olivier B. Poirion, Liangqun Lu, Lana X. Garmire<br />
<em>Cold Spring Harbor Laboratory</em> (2017-03-08) <a href="https://doi.org/gcgk9z">https://doi.org/gcgk9z</a><br />
DOI: <a href="https://doi.org/10.1101/114892">10.1101/114892</a></p>
</div>
<div id="ref-yOz8Ybj2">
<p>565. <strong>FIDDLE: An integrative deep learning framework for functional genomic data inference</strong><br />
Umut Eser, L. Stirling Churchman<br />
<em>Cold Spring Harbor Laboratory</em> (2016-10-17) <a href="https://doi.org/gcgk9g">https://doi.org/gcgk9g</a><br />
DOI: <a href="https://doi.org/10.1101/081380">10.1101/081380</a></p>
</div>
<div id="ref-1BARarxfz">
<p>566. <strong>Modeling Reactivity to Biological Macromolecules with a Deep Multitask Network</strong><br />
Tyler B. Hughes, Na Le Dang, Grover P. Miller, S. Joshua Swamidass<br />
<em>ACS Central Science</em> (2016-07-29) <a href="https://doi.org/gcgk78">https://doi.org/gcgk78</a><br />
DOI: <a href="https://doi.org/10.1021/acscentsci.6b00162">10.1021/acscentsci.6b00162</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27610414">27610414</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4999971">PMC4999971</a></p>
</div>
<div id="ref-nyjAIan4">
<p>567. <strong>IBM edges closer to human speech recognition</strong><br />
BI Intelligence<br />
<em>Business Insider</em> (2017-03-14) <a href="http://www.businessinsider.com/ibm-edges-closer-to-human-speech-recognition-2017-3">http://www.businessinsider.com/ibm-edges-closer-to-human-speech-recognition-2017-3</a></p>
</div>
<div id="ref-M2OLWojE">
<p>568. <strong>Achieving Human Parity in Conversational Speech Recognition</strong><br />
W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, G. Zweig<br />
<em>arXiv</em> (2016-10-17) <a href="https://arxiv.org/abs/1610.05256v2">https://arxiv.org/abs/1610.05256v2</a></p>
</div>
<div id="ref-wKioubsT">
<p>569. <strong>English Conversational Telephone Speech Recognition by Humans and Machines</strong><br />
George Saon, Gakuto Kurata, Tom Sercu, Kartik Audhkhasi, Samuel Thomas, Dimitrios Dimitriadis, Xiaodong Cui, Bhuvana Ramabhadran, Michael Picheny, Lynn-Li Lim, … Phil Hall<br />
<em>arXiv</em> (2017-03-06) <a href="https://arxiv.org/abs/1703.02136v1">https://arxiv.org/abs/1703.02136v1</a></p>
</div>
<div id="ref-1Fel6Bdb8">
<p>570. <strong>Intriguing properties of neural networks</strong><br />
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus<br />
<em>arXiv</em> (2013-12-21) <a href="https://arxiv.org/abs/1312.6199v4">https://arxiv.org/abs/1312.6199v4</a></p>
</div>
<div id="ref-UtcyntjF">
<p>571. <strong>Explaining and Harnessing Adversarial Examples</strong><br />
Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy<br />
<em>arXiv</em> (2014-12-20) <a href="https://arxiv.org/abs/1412.6572v3">https://arxiv.org/abs/1412.6572v3</a></p>
</div>
<div id="ref-AsLAb71x">
<p>572. <strong>Towards the Science of Security and Privacy in Machine Learning</strong><br />
Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, Michael Wellman<br />
<em>arXiv</em> (2016-11-11) <a href="https://arxiv.org/abs/1611.03814v1">https://arxiv.org/abs/1611.03814v1</a></p>
</div>
<div id="ref-18lZK7fxH">
<p>573. <strong>Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks</strong><br />
Weilin Xu, David Evans, Yanjun Qi<br />
<em>arXiv</em> (2017-04-04) <a href="https://arxiv.org/abs/1704.01155v2">https://arxiv.org/abs/1704.01155v2</a><br />
DOI: <a href="https://doi.org/10.14722/ndss.2018.23198">10.14722/ndss.2018.23198</a></p>
</div>
<div id="ref-6MR50hyY">
<p>574. <strong>Proof of prespecified endpoints in medical research with the bitcoin blockchain – The Grey Literature</strong><a href="https://blog.bgcarlisle.com/2014/08/25/proof-of-prespecified-endpoints-in-medical-research-with-the-bitcoin-blockchain/">https://blog.bgcarlisle.com/2014/08/25/proof-of-prespecified-endpoints-in-medical-research-with-the-bitcoin-blockchain/</a></p>
</div>
<div id="ref-QBWMEuxW">
<p>575. <strong>The most interesting case of scientific irreproducibility?</strong><br />
Daniel Himmelstein<br />
<em>Satoshi Village</em> (2017-03-08) <a href="https://blog.dhimmel.com/irreproducible-timestamps/">https://blog.dhimmel.com/irreproducible-timestamps/</a></p>
</div>
<div id="ref-igA8jklc">
<p>576. <strong>OpenTimestamps: a timestamping proof standard</strong><a href="https://opentimestamps.org/">https://opentimestamps.org/</a></p>
</div>
<div id="ref-yLr4pV4G">
<p>577. <strong>greenelab/deep-review</strong><br />
GitHub<br />
(2017) <a href="https://github.com/greenelab/deep-review">https://github.com/greenelab/deep-review</a></p>
</div>
</div>
<!-- default theme -->

<style>
    /* import google fonts */
    @import url("https://fonts.googleapis.com/css?family=Open+Sans:400,600,700");
    @import url("https://fonts.googleapis.com/css?family=Source+Code+Pro");

    /* -------------------------------------------------- */
    /* global */
    /* -------------------------------------------------- */

    /* all elements */
    * {
        /* force sans-serif font unless specified otherwise */
        font-family: "Open Sans", "Helvetica", sans-serif;

        /* prevent text inflation on some mobile browsers */
        -webkit-text-size-adjust: none !important;
        -moz-text-size-adjust: none !important;
        -o-text-size-adjust: none !important;
        text-size-adjust: none !important;
    }

    @media only screen {
        /* "page" element */
        body {
            position: relative;
            box-sizing: border-box;
            font-size: 12pt;
            line-height: 1.5;
            max-width: 8.5in;
            margin: 20px auto;
            padding: 40px;
            border-radius: 5px;
            border: solid 1px #bdbdbd;
            box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
            background: #ffffff;
        }
    }

    /* when on screen < 8.5in wide */
    @media only screen and (max-width: 8.5in) {
        /* "page" element */
        body {
            padding: 20px;
            margin: 0;
            border-radius: 0;
            border: none;
            box-shadow: 0 0 20px rgba(0, 0, 0, 0.05) inset;
            background: none;
        }
    }

    /* -------------------------------------------------- */
    /* headings */
    /* -------------------------------------------------- */

    /* all headings */
    h1,
    h2,
    h3,
    h4,
    h5,
    h6 {
        margin: 20px 0;
        padding: 0;
        font-weight: bold;
    }

    /* biggest heading */
    h1 {
        margin: 40px 0;
        text-align: center;
    }

    /* second biggest heading */
    h2 {
        margin-top: 30px;
        padding-bottom: 5px;
        border-bottom: solid 1px #bdbdbd;
    }

    /* -------------------------------------------------- */
    /* manuscript header */
    /* -------------------------------------------------- */

    /* manuscript title */
    header > h1 {
        margin: 0;
    }

    /* manuscript title caption text (ie "automatically generated on") */
    header + p {
        text-align: center;
        margin-top: 10px;
    }

    /* -------------------------------------------------- */
    /* text elements */
    /* -------------------------------------------------- */

    /* links */
    a {
        color: #2196f3;
    }

    /* normal links (not empty, not button link, not syntax highlighting link) */
    a:not(:empty):not(.button):not(.sourceLine) {
        padding-left: 1px;
        padding-right: 1px;
    }

    /* superscripts and subscripts */
    sub,
    sup {
        /* prevent from affecting line height */
        line-height: 0;
    }

    /* unordered and ordered lists*/
    ul,
    ol {
        padding-left: 20px;
    }

    /* class for styling text semibold */
    .semibold {
        font-weight: 600;
    }

    /* class for styling elements horizontally left aligned */
    .left {
        display: block;
        text-align: left;
        margin-left: auto;
        margin-right: 0;
        justify-content: left;
    }

    /* class for styling elements horizontally centered */
    .center {
        display: block;
        text-align: center;
        margin-left: auto;
        margin-right: auto;
        justify-content: center;
    }

    /* class for styling elements horizontally right aligned */
    .right {
        display: block;
        text-align: right;
        margin-left: 0;
        margin-right: auto;
        justify-content: right;
    }

    /* -------------------------------------------------- */
    /* section elements */
    /* -------------------------------------------------- */

    /* horizontal divider line */
    hr {
        border: none;
        height: 1px;
        background: #bdbdbd;
    }

    /* paragraphs, horizontal dividers, figures, tables, code */
    p,
    hr,
    figure,
    table,
    pre {
        /* treat all as "paragraphs", with consistent vertical margins */
        margin-top: 20px;
        margin-bottom: 20px;
    }

    /* -------------------------------------------------- */
    /* figures */
    /* -------------------------------------------------- */

    /* figure */
    figure {
        max-width: 100%;
        margin-left: auto;
        margin-right: auto;
    }

    /* figure caption */
    figcaption {
        padding: 0;
        padding-top: 10px;
    }

    /* figure image element */
    figure img {
        max-width: 100%;
        display: block;
        margin-left: auto;
        margin-right: auto;
    }

    /* figure auto-number */
    img + figcaption > span:first-of-type {
        font-weight: bold;
        margin-right: 5px;
    }

    /* -------------------------------------------------- */
    /* tables */
    /* -------------------------------------------------- */

    /* table */
    table {
        border-collapse: collapse;
        border-spacing: 0;
        width: 100%;
        margin-left: auto;
        margin-right: auto;
    }

    /* table cells */
    th,
    td {
        border: solid 1px #bdbdbd;
        padding: 10px;
        /* squash table if too wide for page by forcing line breaks */
        word-break: break-all;
        word-break: break-word;
    }

    /* header row and even rows */
    th,
    tr:nth-child(2n) {
        background-color: #fafafa;
    }

    /* odd rows */
    tr:nth-child(2n + 1) {
        background-color: #ffffff;
    }

    /* table caption */
    caption {
        text-align: left;
        padding: 0;
        padding-bottom: 10px;
    }

    /* table auto-number */
    table > caption > span:first-of-type,
    div.table_wrapper > table > caption > span:first-of-type {
        font-weight: bold;
        margin-right: 5px;
    }

    /* -------------------------------------------------- */
    /* code */
    /* -------------------------------------------------- */

    /* multi-line code block */
    pre {
        padding: 10px;
        background-color: #eeeeee;
        color: #000000;
        border-radius: 5px;
        break-inside: avoid;
        text-align: left;
    }

    /* inline code, ie code within normal text */
    :not(pre) > code {
        padding: 0 4px;
        background-color: #eeeeee;
        color: #000000;
        border-radius: 5px;
    }

    /* code text */
    /* apply all children, to reach syntax highlighting sub-elements */
    code,
    code * {
        /* force monospace font */
        font-family: "Source Code Pro", "Courier New", monospace;
    }

    /* -------------------------------------------------- */
    /* quotes */
    /* -------------------------------------------------- */

    /* quoted text */
    blockquote {
        margin: 0;
        padding: 0;
        border-left: 4px solid #bdbdbd;
        padding-left: 16px;
        break-inside: avoid;
    }

    /* -------------------------------------------------- */
    /* banners */
    /* -------------------------------------------------- */

    /* info banners */
    .banner {
        box-sizing: border-box;
        display: block;
        position: relative;
        width: 100%;
        margin-top: 20px;
        margin-bottom: 20px;
        padding: 20px;
        text-align: center;
    }

    /* paragraph in banner */
    .banner > p {
        margin: 0;
    }

    /* -------------------------------------------------- */
    /* highlight colors */
    /* -------------------------------------------------- */

    .white {
        background: #ffffff;
    }
    .lightgrey {
        background: #eeeeee;
    }
    .grey {
        background: #757575;
    }
    .darkgrey {
        background: #424242;
    }
    .black {
        background: #000000;
    }
    .lightred {
        background: #ffcdd2;
    }
    .lightyellow {
        background: #ffecb3;
    }
    .lightgreen {
        background: #dcedc8;
    }
    .lightblue {
        background: #e3f2fd;
    }
    .lightpurple {
        background: #f3e5f5;
    }
    .red {
        background: #f44336;
    }
    .orange {
        background: #ff9800;
    }
    .yellow {
        background: #ffeb3b;
    }
    .green {
        background: #4caf50;
    }
    .blue {
        background: #2196f3;
    }
    .purple {
        background: #9c27b0;
    }
    .white,
    .lightgrey,
    .lightred,
    .lightyellow,
    .lightgreen,
    .lightblue,
    .lightpurple,
    .orange,
    .yellow,
    .white a,
    .lightgrey a,
    .lightred a,
    .lightyellow a,
    .lightgreen a,
    .lightblue a,
    .lightpurple a,
    .orange a,
    .yellow a {
        color: #000000;
    }
    .grey,
    .darkgrey,
    .black,
    .red,
    .green,
    .blue,
    .purple,
    .grey a,
    .darkgrey a,
    .black a,
    .red a,
    .green a,
    .blue a,
    .purple a {
        color: #ffffff;
    }

    /* -------------------------------------------------- */
    /* buttons */
    /* -------------------------------------------------- */

    /* class for styling links like buttons */
    .button {
        display: inline-flex;
        justify-content: center;
        align-items: center;
        margin: 5px;
        padding: 10px 20px;
        font-size: 0.75em;
        font-weight: 600;
        text-transform: uppercase;
        text-decoration: none;
        letter-spacing: 1px;
        background: none;
        color: #2196f3;
        border: solid 1px #bdbdbd;
        border-radius: 5px;
    }

    /* buttons when hovered */
    .button:hover:not([disabled]),
    .icon_button:hover:not([disabled]) {
        cursor: pointer;
        background: #f5f5f5;
    }

    /* buttons when disabled */
    .button[disabled],
    .icon_button[disabled] {
        opacity: 0.35;
        pointer-events: none;
    }

    /* class for styling buttons containg only single icon */
    .icon_button {
        display: inline-flex;
        justify-content: center;
        align-items: center;
        text-decoration: none;
        margin: 0;
        padding: 0;
        background: none;
        border-radius: 5px;
        border: none;
        width: 20px;
        height: 20px;
        min-width: 20px;
        min-height: 20px;
    }

    /* icon button inner svg image */
    .icon_button > svg {
        height: 16px;
    }

    /* -------------------------------------------------- */
    /* icons */
    /* -------------------------------------------------- */

    /* class for styling icons inline with text */
    .inline_icon {
        height: 1em;
        position: relative;
        top: 0.125em;
    }

    /* -------------------------------------------------- */
    /* print control */
    /* -------------------------------------------------- */

    @media print {
        @page {
            /* suggested printing margin */
            margin: 0.5in;
        }

        /* document and "page" elements */
        html, body {
            margin: 0;
            padding: 0;
            width: 100%;
            height: 100%;
        }

        /* "page" element */
        body {
            font-size: 11pt !important;
            line-height: 1.35;
        }

        /* all headings */
        h1,
        h2,
        h3,
        h4,
        h5,
        h6 {
            margin: 15px 0;
        }

        /* heading 1 */
        h1 {
            font-size: 1.75em;
        }

        /* heading 2 */
        h2 {
            font-size: 1.25em;
            margin-top: 0;
        }

        /* heading 3 */
        h3 {
            font-size: 1.10em;
        }

        /* figures and tables */
        figure, table {
            font-size: 0.85em;
        }

        /* table cells */
        th,
        td {
            padding: 5px;
        }

        /* shrink font awesome icons */
        i.fas,
        i.fab,
        i.far,
        i.fal {
            transform: scale(0.85);
        }

        /* decrease banner margins */
        .banner {
            margin-top: 15px;
            margin-bottom: 15px;
            padding: 15px;
        }

        /* class for centering an element vertically on its own page */
        .page_center {
            margin: auto;
            width: 100%;
            height: 100%;
            display: flex;
            align-items: center;
            vertical-align: middle;
            break-before: page;
            break-after: page;
        }

        /* always insert a page break before the element */
        .page_break_before {
            break-before: page;
        }

        /* always insert a page break after the element */
        .page_break_after {
            break-after: page;
        }

        /* avoid page break before the element */
        .page_break_before_avoid {
            break-before: avoid;
        }

        /* avoid page break after the element */
        .page_break_after_avoid {
            break-after: avoid;
        }

        /* avoid page break inside the element */
        .page_break_inside_avoid {
            break-inside: avoid;
        }
    }

    /* -------------------------------------------------- */
    /* override pandoc css quirks */
    /* -------------------------------------------------- */

    .sourceCode {
        /* prevent unsightly overflow in wide code blocks */
        overflow: auto !important;
    }

    div.sourceCode {
        /* prevent background fill on top-most code block  container */
        background: none !important;
    }

    .sourceCode * {
        /* force consistent line spacing */
        line-height: 1.5 !important;
    }

    div.sourceCode {
        /* style code block margins same as <pre> element */
        margin-top: 20px;
        margin-bottom: 20px;
    }

    /* -------------------------------------------------- */
    /* mathjax */
    /* -------------------------------------------------- */

    /* mathjax containers */
    .math.display > span:not(.MathJax_Preview) {
        /* turn inline element (no dimensions) into block (allows fixed width and thus scrolling) */
        display: flex !important;
        overflow-x: auto !important;
        overflow-y: hidden !important;
        justify-content: center;
        align-items: center;
        margin: 0 !important;
    }

    /* right click menu */
    .MathJax_Menu {
        border-radius: 5px !important;
        border: solid 1px #bdbdbd !important;
        box-shadow: none !important;
    }

    /* equation auto-number */
    span[id^="eq:"] > span.math.display + span {
        font-weight: 600;
    }

    /* equation */
    span[id^="eq:"] > span.math.display > span {
        /* nudge to make room for equation auto-number */
        margin-right: 40px !important;
    }

    /* -------------------------------------------------- */
    /* table scroll plugin */
    /* -------------------------------------------------- */

    @media only screen {
        /* table wrapper */
        .table_wrapper {
            /* show scrollbar on tables if necessary to prevent overflow */
            overflow: auto;
            width: 100%;
            margin: 20px 0;
        }

        /* table within table wrapper */
        .table_wrapper table,
        .table_wrapper table * {
            /* don't break table words */
            word-break: normal !important;
        }

        .table_wrapper > table {
            /* move margins from table to table_wrapper to allow margin collapsing */
            margin: 0;
        }
    }

    /* -------------------------------------------------- */
    /* anchors plugin */
    /* -------------------------------------------------- */

    @media only screen {
        /* anchor button */
        .anchor {
            opacity: 0;
            margin-left: 5px;
        }

        /* anchor buttons within <h2>'s */
        h2 .anchor {
            margin-left: 10px;
        }

        /* anchor buttons when hovered/focused and anything containing an anchor button when hovered */
        *:hover > .anchor,
        .anchor:hover,
        .anchor:focus {
            opacity: 1;
        }

        /* anchor button when hovered */
        .anchor:hover {
            cursor: pointer;
        }
    }

    /* always show anchor button on devices with no mouse/hover ability */
    @media (hover: none) {
        .anchor {
            opacity: 1;
        }
    }

    /* always hide anchor button on print */
    @media only print {
        .anchor {
            display: none;
        }
    }

    /* -------------------------------------------------- */
    /* accordion plugin */
    /* -------------------------------------------------- */

    @media only screen {
        /* accordion arrow button */
        .accordion_arrow {
            margin-right: 10px;
        }

        /* arrow icon when <h2> data-collapsed attribute true */
        h2[data-collapsed="true"] > .accordion_arrow > svg {
            transform: rotate(-90deg);
        }

        /* all elements (except <h2>'s) when data-collapsed attribute true */
        *:not(h2)[data-collapsed="true"] {
            display: none;
        }

        /* accordion arrow button when hovered and <h2>'s when hovered */
        .accordion_arrow:hover,
        h2[data-collapsed="true"]:hover,
        h2[data-collapsed="false"]:hover {
            cursor: pointer;
        }
    }

    /* always hide accordion arrow button on print */
    @media only print {
        .accordion_arrow {
            display: none;
        }
    }

    /* -------------------------------------------------- */
    /* tooltips plugin */
    /* -------------------------------------------------- */

    @media only screen {
        /* tooltip container */
        #tooltip {
            position: absolute;
            width: 50%;
            min-width: 240px;
            max-width: 75%;
            z-index: 1;
        }

        /* tooltip content */
        #tooltip_content {
            margin-bottom: 5px;
            padding: 20px;
            border-radius: 5px;
            border: solid 1px #bdbdbd;
            box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
            background: #ffffff;
            word-break: break-word;
        }

        /* tooltip copy of paragraphs and figures */
        #tooltip_content > p,
        #tooltip_content > figure {
            margin: 0;
            max-height: 320px;
            overflow-y: auto;
        }

        /* tooltip copy of <img> */
        #tooltip_content > figure > img {
            max-height: 260px;
        }

        /* navigation bar */
        #tooltip_nav_bar {
            margin-top: 10px;
            text-align: center;
        }

        /* navigation bar previous/next buton */
        #tooltip_nav_bar > .icon_button {
            position: relative;
            top: 3px;
        }

        /* navigation bar previous button */
        #tooltip_nav_bar > .icon_button:first-of-type {
            margin-right: 5px;
        }

        /* navigation bar next button */
        #tooltip_nav_bar > .icon_button:last-of-type {
            margin-left: 5px;
        }
    }

    /* always hide tooltip on print */
    @media only print {
        #tooltip {
            display: none;
        }
    }

    /* -------------------------------------------------- */
    /* jump to first plugin */
    /* -------------------------------------------------- */

    @media only screen {
        /* jump button */
        .jump_arrow {
            position: relative;
            top: 0.125em;
            margin-right: 5px;
        }
    }

    /* always hide jump button on print */
    @media only print {
        .jump_arrow {
            display: none;
        }
    }

    /* -------------------------------------------------- */
    /* link highlight plugin */
    /* -------------------------------------------------- */

    @media only screen {
        /* anything with data-highlighted attribute true */
        [data-highlighted="true"] {
            background: #ffeb3b;
        }

        /* anything with data-selected attribute true */
        [data-selected="true"] {
            background: #ff8a65 !important;
        }

        /* animation definition for glow */
        @keyframes highlight_glow {
            0% {
                background: none;
            }
            10% {
                background: #bbdefb;
            }
            100% {
                background: none;
            }
        }

        /* anything with data-glow attribute true */
        [data-glow="true"] {
            animation: highlight_glow 2s;
        }
    }

    /* -------------------------------------------------- */
    /* table of contents plugin */
    /* -------------------------------------------------- */

    @media only screen {
        /* toc panel */
        #toc_panel {
            box-sizing: border-box;
            position: fixed;
            top: 0;
            left: 0;
            background: #ffffff;
            box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
            z-index: 2;
        }

        /* toc panel when closed */
        #toc_panel[data-open="false"] {
            min-width: 60px;
            width: 60px;
            height: 60px;
            border-right: solid 1px #bdbdbd;
            border-bottom: solid 1px #bdbdbd;
        }

        /* toc panel when open */
        #toc_panel[data-open="true"] {
            min-width: 260px;
            max-width: 480px;
            /* keep panel edge consistent distance away from "page" edge */
            width: calc(((100vw - 8.5in) / 2) - 30px - 40px);
            bottom: 0;
            border-right: solid 1px #bdbdbd;
        }

        /* toc panel header */
        #toc_header {
            box-sizing: border-box;
            display: flex;
            flex-direction: row;
            align-items: center;
            height: 60px;
            margin: 0;
            padding: 20px;
        }

        /* toc panel header when hovered */
        #toc_header:hover {
            cursor: pointer;
        }

        /* toc panel header when panel open */
        #toc_panel[data-open="true"] > #toc_header {
            border-bottom: solid 1px #bdbdbd;
        }

        /* toc open/close header button */
        #toc_button {
            margin-right: 20px;
        }

        /* hide toc list and header text when closed */
        #toc_panel[data-open="false"] > #toc_header > *:not(#toc_button),
        #toc_panel[data-open="false"] > #toc_list {
            display: none;
        }

        /* toc list of entries */
        #toc_list {
            box-sizing: border-box;
            width: 100%;
            padding: 20px;
            position: absolute;
            top: calc(60px + 1px);
            bottom: 0;
            overflow: auto;
        }

        /* toc entry, link to section in document */
        .toc_link {
            display: block;
            padding: 5px;
            position: relative;
            font-weight: 600;
            text-decoration: none;
        }

        /* toc entry when hovered or when "viewed" */
        .toc_link:hover,
        .toc_link[data-viewing="true"] {
            background: #f5f5f5;
        }

        /* toc entry, level 1 indentation */
        .toc_link[data-level="1"] {
            margin-left: 0;
        }

        /* toc entry, level 2 indentation */
        .toc_link[data-level="2"] {
            margin-left: 20px;
        }

        /* toc entry, level 3 indentation */
        .toc_link[data-level="3"] {
            margin-left: 40px;
        }

        /* toc entry, level 4 indentation */
        .toc_link[data-level="4"] {
            margin-left: 60px;
        }

        /* toc entry bullets */
        #toc_panel[data-bullets="true"] .toc_link[data-level]:before {
            position: absolute;
            left: -15px;
            top: -1px;
            font-size: 1.5em;
        }

        /* toc entry, level 2 bullet */
        #toc_panel[data-bullets="true"] .toc_link[data-level="2"]:before {
            content: "\2022";
        }

        /* toc entry, level 3 bullet */
        #toc_panel[data-bullets="true"] .toc_link[data-level="3"]:before {
            content: "\25AB";
        }

        /* toc entry, level 4 bullet */
        #toc_panel[data-bullets="true"] .toc_link[data-level="4"]:before {
            content: "-";
        }
    }

    /* when on screen < 8.5in wide */
    @media only screen and (max-width: 8.5in) {
        /* push <body> ("page") element down to make room for toc icon */
        .toc_body_nudge {
            padding-top: 60px;
        }

        /* toc icon when panel closed and not hovered */
        #toc_panel[data-open="false"]:not(:hover) {
            background: rgba(255, 255, 255, 0.75);
        }
    }

    /* always hide toc panel on print */
    @media only print {
        #toc_panel {
            display: none;
        }
    }

    /* -------------------------------------------------- */
    /* lightbox plugin */
    /* -------------------------------------------------- */

    @media only screen {
        /* regular <img> in document when hovered */
        .lightbox_document_img:hover {
            cursor: pointer;
        }

        .body_no_scroll {
            overflow: hidden !important;
        }

        /* screen overlay */
        #lightbox_overlay {
            display: flex;
            flex-direction: column;
            position: fixed;
            left: 0;
            top: 0;
            right: 0;
            bottom: 0;
            background: rgba(0, 0, 0, 0.75);
            z-index: 3;
        }

        /* middle area containing lightbox image */
        #lightbox_image_container {
            flex-grow: 1;
            display: flex;
            justify-content: center;
            align-items: center;
            overflow: hidden;
            position: relative;
            padding: 20px;
        }

        /* bottom area containing caption */
        #lightbox_bottom_container {
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100px;
            min-height: 100px;
            max-height: 100px;
            background: rgba(0, 0, 0, 0.5);
        }

        /* image number info text box */
        #lightbox_number_info {
            position: absolute;
            color: #ffffff;
            font-weight: 600;
            left: 2px;
            top: 0;
            z-index: 4;
        }

        /* zoom info text box */
        #lightbox_zoom_info {
            position: absolute;
            color: #ffffff;
            font-weight: 600;
            right: 2px;
            top: 0;
            z-index: 4;
        }

        /* copy of image caption */
        #lightbox_caption {
            box-sizing: border-box;
            display: inline-block;
            width: 100%;
            max-height: 100%;
            padding: 10px 0;
            text-align: center;
            overflow-y: auto;
            color: #ffffff;
        }

        /* navigation previous/next button */
        .lightbox_button {
            width: 100px;
            height: 100%;
            min-width: 100px;
            min-height: 100%;
            color: #ffffff;
        }

        /* navigation previous/next button when hovered */
        .lightbox_button:hover {
            background: none !important;
        }

        /* navigation button icon */
        .lightbox_button > svg {
            height: 25px;
        }

        /* figure auto-number */
        #lightbox_caption > span:first-of-type {
            font-weight: bold;
            margin-right: 5px;
        }

        /* lightbox image when hovered */
        #lightbox_img:hover {
            cursor: grab;
        }

        /* lightbox image when grabbed */
        #lightbox_img:active {
            cursor: grabbing;
        }
    }

    /* when on screen < 480px wide */
    @media only screen and (max-width: 480px) {
        /* make navigation buttons skinnier on small screens to make more room for caption text */
        .lightbox_button {
            width: 50px;
            min-width: 50px;
        }
    }

    /* always hide lightbox on print */
    @media only print {
        #lightbox_overlay {
            display: none;
        }
    }

    /* -------------------------------------------------- */
    /* hypothesis (annotations) plugin */
    /* -------------------------------------------------- */

    /* side panel */
    .annotator-frame {
        width: 280px !important;
        z-index: 0 !important;
    }

    /* match highlight color to rest of theme */
    .annotator-highlights-always-on .annotator-hl {
        background-color: #ffeb3b !important;
    }

    /* match focused color to rest of theme */
    .annotator-hl.annotator-hl-focused {
        background-color: #ff8a65 !important;
    }

    /* match bucket bar color to rest of theme */
    .annotator-bucket-bar {
        background: #f5f5f5 !important;
    }

    /* always hide toolbar and tooltip on print */
    @media only print {
        .annotator-frame {
            display: none !important;
        }

        hypothesis-adder {
            display: none !important;
        }
    }
</style>
<!-- table scroll plugin -->

<script>
    (function() {
        // /////////////////////////
        // DESCRIPTION
        // /////////////////////////

        // This Manubot plugin allows tables that are too wide to fit within
        // the page to have a scrollbar instead of being squashed to fit.

        // /////////////////////////
        // OPTIONS
        // /////////////////////////

        // plugin name prefix for url parameters
        const pluginName = 'tableScroll';

        // default plugin options
        const options = {
            // whether plugin is on or not
            enabled: 'true'
        };

        // change options above, or override with url parameter, eg:
        // 'manuscript.html?pluginName-enabled=false'

        // /////////////////////////
        // SCRIPT
        // /////////////////////////

        // start script
        function start() {
            // wrap each table in a container
            const tables = document.querySelectorAll('table');
            for (const table of tables)
                wrapElement(table).classList.add('table_wrapper');
            // table_wrapper CSS class in theme file provides scroll
            // functionality
        }

        // wrap element in div and return div
        function wrapElement(element) {
            const parent = element.parentNode;
            const wrapper = document.createElement('div');
            parent.replaceChild(wrapper, element);
            wrapper.appendChild(element);
            return wrapper;
        }

        // load options from url parameters
        function loadOptions() {
            const url = window.location.search;
            const params = new URLSearchParams(url);
            for (const optionName of Object.keys(options)) {
                const paramName = pluginName + '-' + optionName;
                const param = params.get(paramName);
                if (param !== '' && param !== null)
                    options[optionName] = param;
            }
        }
        loadOptions();

        // start script when document is finished loading
        if (options.enabled === 'true')
            window.addEventListener('load', start);
    })();
</script>
<!-- anchors plugin -->

<script>
    (function() {
        // /////////////////////////
        // DESCRIPTION
        // /////////////////////////

        // This Manubot plugin adds an anchor next to each of a certain type
        // of element that provides a human-readable url to that specific
        // item/position in the document (eg "manuscript.html#abstract"). It
        // also makes it such that scrolling out of view of a target removes
        // its identifier from the url.

        // /////////////////////////
        // OPTIONS
        // /////////////////////////

        // plugin name prefix for url parameters
        const pluginName = 'anchors';

        // default plugin options
        const options = {
            // which types of elements to add anchors next to, in
            // "document.querySelector" format
            typesQuery: 'h1, h2, h3, figure, table',
            // whether plugin is on or not
            enabled: 'true'
        };

        // change options above, or override with url parameter, eg:
        // 'manuscript.html?pluginName-enabled=false'

        // /////////////////////////
        // SCRIPT
        // /////////////////////////

        // start script
        function start() {
            // add anchor to each element of specified types
            const elements = document.querySelectorAll(options.typesQuery);
            for (const element of elements)
                addAnchor(element);

            // attach scroll listener to window
            window.addEventListener('scroll', onScroll);
        }

        // when window is scrolled
        function onScroll() {
            // if url has hash and user has scrolled out of view of hash
            // target, remove hash from url
            const target = getHashTarget();
            if (target) {
                if (
                    target.getBoundingClientRect().top > window.innerHeight ||
                    target.getBoundingClientRect().bottom < 0
                )
                    history.pushState(null, null, ' ');
            }
        }

        // add anchor to element
        function addAnchor(element) {
            let withId; // element with unique id
            let addTo; // element to add anchor button to

            // if figure or table, modify withId and addTo to get expected
            // elements
            if (element.tagName.toLowerCase() === 'figure') {
                withId = element.querySelector('img');
                addTo = element.querySelector('figcaption');
            } else if (element.tagName.toLowerCase() === 'table') {
                withId =
                    element.previousElementSibling ||
                    element.parentNode.previousElementSibling;
                addTo = element.querySelector('caption');
            }

            withId = withId || element;
            addTo = addTo || element;
            const id = withId.id || withId.name || null;

            // do not add anchor if element doesn't have assigned id.
            // id is generated by pandoc and is assumed to be unique and
            // human-readable
            if (!id)
                return;

            // create anchor button
            const anchor = document.createElement('a');
            anchor.innerHTML = document.querySelector('.icon_link').innerHTML;
            anchor.title = 'Link to this part of the document';
            anchor.classList.add('icon_button', 'anchor');
            anchor.dataset.ignore = 'true';
            anchor.href = '#' + id;
            addTo.appendChild(anchor);
        }

        // get element that is target of link or url hash
        function getHashTarget() {
            const hash = window.location.hash;
            const id = hash.slice(1);
            let target = document.querySelector(
                '[id="' + id + '"], [name="' + id + '"]'
            );
            if (!target)
                return;

            // if figure or table, modify target to get expected element
            if (hash.indexOf('#fig:') === 0)
                target = target.parentNode;
            if (hash.indexOf('#tbl:') === 0)
                target = target.nextElementSibling;

            return target;
        }

        // load options from url parameters
        function loadOptions() {
            const url = window.location.search;
            const params = new URLSearchParams(url);
            for (const optionName of Object.keys(options)) {
                const paramName = pluginName + '-' + optionName;
                const param = params.get(paramName);
                if (param !== '' && param !== null)
                    options[optionName] = param;
            }
        }
        loadOptions();

        // start script when document is finished loading
        if (options.enabled === 'true')
            window.addEventListener('load', start);
    })();
</script>

<!-- link icon -->

<template class="icon_link">
    <!-- modified from: https://fontawesome.com/icons/link -->
    <svg width="16" height="16" viewBox="0 0 512 512">
        <path
            fill="currentColor"
            d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"
        ></path>
    </svg>
</template>
<!-- accordion plugin -->

<script>
    (function() {
        // /////////////////////////
        // DESCRIPTION
        // /////////////////////////

        // This Manubot plugin allows sections of content under <h2> headings
        // to be collapsible.

        // /////////////////////////
        // OPTIONS
        // /////////////////////////

        // plugin name prefix for url parameters
        const pluginName = 'accordion';

        // default plugin options
        const options = {
            // whether to always start expanded ('false'), always start
            // collapsed ('true'), or start collapsed when screen small ('auto')
            startCollapsed: 'auto',
            // whether plugin is on or not
            enabled: 'true'
        };

        // change options above, or override with url parameter, eg:
        // 'manuscript.html?pluginName-enabled=false'

        // /////////////////////////
        // SCRIPT
        // /////////////////////////

        // start script
        function start() {
            // run through each <h2> heading
            const headings = document.querySelectorAll('h2');
            for (const heading of headings) {
                addArrow(heading);

                // start expanded/collapsed based on option
                if (
                    options.startCollapsed === 'true' ||
                    (options.startCollapsed === 'auto' && isSmallScreen())
                )
                    collapseHeading(heading);
                else
                    expandHeading(heading);
            }

            // attach hash change listener to window
            window.addEventListener('hashchange', onHashChange);
        }

        // when hash (eg manuscript.html#introduction) changes
        function onHashChange() {
            const target = getHashTarget();
            if (target)
                goToElement(target);
        }

        // add arrow to heading
        function addArrow(heading) {
            // add arrow button
            const arrow = document.createElement('button');
            arrow.innerHTML = document.querySelector(
                '.icon_angle_down'
            ).innerHTML;
            arrow.classList.add('icon_button', 'accordion_arrow');
            heading.insertBefore(arrow, heading.firstChild);

            // attach click listener to heading and button
            heading.addEventListener('click', onHeadingClick);
            arrow.addEventListener('click', onArrowClick);
        }

        // determine if on mobile-like device with small screen
        function isSmallScreen() {
            return Math.min(window.innerWidth, window.innerHeight) < 480;
        }

        // scroll to and focus element
        function goToElement(element, offset) {
            // expand accordion section if collapsed
            expandElement(element);
            const y =
                getRectInView(element).top -
                getRectInView(document.documentElement).top -
                (offset || 0);
            // trigger any function listening for "onscroll" event
            window.dispatchEvent(new Event('scroll'));
            window.scrollTo(0, y);
            document.activeElement.blur();
            element.focus();
        }

        // get element that is target of hash
        function getHashTarget(link) {
            const hash = link ? link.hash : window.location.hash;
            const id = hash.slice(1);
            let target = document.querySelector(
                '[id="' + id + '"], [name="' + id + '"]'
            );
            if (!target)
                return;

            // if figure or table, modify target to get expected element
            if (hash.indexOf('#fig:') === 0)
                target = target.parentNode;
            if (hash.indexOf('#tbl:') === 0)
                target = target.nextElementSibling;

            return target;
        }

        // when <h2> heading is clicked
        function onHeadingClick(event) {
            // only collapse if <h2> itself is target of click (eg, user did
            // not click on anchor within <h2>)
            if (event.target === this)
                toggleCollapse(this);
        }

        // when arrow button is clicked
        function onArrowClick() {
            toggleCollapse(this.parentNode);
        }

        // collapse section if expanded, expand if collapsed
        function toggleCollapse(heading) {
            if (heading.dataset.collapsed === 'false')
                collapseHeading(heading);
            else
                expandHeading(heading);
        }

        // elements to exclude from collapse, such as table of contents panel,
        // hypothesis panel, etc
        const exclude = '#toc_panel, div.annotator-frame, #lightbox_overlay';

        // collapse section
        function collapseHeading(heading) {
            heading.setAttribute('data-collapsed', 'true');
            const children = getChildren(heading);
            for (const child of children)
                child.setAttribute('data-collapsed', 'true');
        }

        // expand section
        function expandHeading(heading) {
            heading.setAttribute('data-collapsed', 'false');
            const children = getChildren(heading);
            for (const child of children)
                child.setAttribute('data-collapsed', 'false');
        }

        // get list of elements between this <h2> and next <h2> or <h1>
        // ("children" of the <h2> section)
        function getChildren(heading) {
            return nextUntil(heading, 'h2, h1', exclude);
        }

        // get position/dimensions of element or viewport
        function getRectInView(element) {
            let rect = {};
            rect.left = 0;
            rect.top = 0;
            rect.right = document.documentElement.clientWidth;
            rect.bottom = document.documentElement.clientHeight;
            let style = {};

            if (element instanceof HTMLElement) {
                rect = element.getBoundingClientRect();
                style = window.getComputedStyle(element);
            }

            const margin = {};
            margin.left = parseFloat(style.marginLeftWidth) || 0;
            margin.top = parseFloat(style.marginTopWidth) || 0;
            margin.right = parseFloat(style.marginRightWidth) || 0;
            margin.bottom = parseFloat(style.marginBottomWidth) || 0;

            const border = {};
            border.left = parseFloat(style.borderLeftWidth) || 0;
            border.top = parseFloat(style.borderTopWidth) || 0;
            border.right = parseFloat(style.borderRightWidth) || 0;
            border.bottom = parseFloat(style.borderBottomWidth) || 0;

            const newRect = {};
            newRect.left = rect.left + margin.left + border.left;
            newRect.top = rect.top + margin.top + border.top;
            newRect.right = rect.right + margin.right + border.right;
            newRect.bottom = rect.bottom + margin.bottom + border.bottom;
            newRect.width = newRect.right - newRect.left;
            newRect.height = newRect.bottom - newRect.top;

            return newRect;
        }

        // get list of elements after a start element up to element matching
        // query
        function nextUntil(element, query, exclude) {
            const elements = [];
            while (element = element.nextElementSibling, element) {
                if (element.matches(query))
                    break;
                if (!element.matches(exclude))
                    elements.push(element);
            }
            return elements;
        }

        // get closest element before specified element that matches query
        function firstBefore(element, query) {
            while (
                element &&
                element !== document.body &&
                !element.matches(query)
            )
                element = element.previousElementSibling || element.parentNode;

            return element;
        }

        // check if element is part of collapsed heading
        function isCollapsed(element) {
            while (element && element !== document.body) {
                if (element.dataset.collapsed === 'true')
                    return true;
                element = element.parentNode;
            }
            return false;
        }

        // expand heading containing element if necesary
        function expandElement(element) {
            if (isCollapsed(element)) {
                const heading = firstBefore(element, 'h2');
                if (heading)
                    heading.click();
            }
        }

        // load options from url parameters
        function loadOptions() {
            const url = window.location.search;
            const params = new URLSearchParams(url);
            for (const optionName of Object.keys(options)) {
                const paramName = pluginName + '-' + optionName;
                const param = params.get(paramName);
                if (param !== '' && param !== null)
                    options[optionName] = param;
            }
        }
        loadOptions();

        // start script when document is finished loading
        if (options.enabled === 'true')
            window.addEventListener('load', start);
    })();
</script>

<!-- angle down icon -->

<template class="icon_angle_down">
    <!-- modified from: https://fontawesome.com/icons/angle-down -->
    <svg width="16" height="16" viewBox="0 0 448 512">
        <path
            fill="currentColor"
            d="M207.029 381.476L12.686 187.132c-9.373-9.373-9.373-24.569 0-33.941l22.667-22.667c9.357-9.357 24.522-9.375 33.901-.04L224 284.505l154.745-154.021c9.379-9.335 24.544-9.317 33.901.04l22.667 22.667c9.373 9.373 9.373 24.569 0 33.941L240.971 381.476c-9.373 9.372-24.569 9.372-33.942 0z"
        ></path>
    </svg>
</template>
<!-- tooltips plugin -->

<script>
    (function() {
        // /////////////////////////
        // DESCRIPTION
        // /////////////////////////

        // This Manubot plugin makes it such that when the user hovers or
        // focuses a link to a citation or figure, a tooltip appears with a
        // preview of the reference content, along with arrows to navigate
        // between instances of the same reference in the document.

        // /////////////////////////
        // OPTIONS
        // /////////////////////////

        // plugin name prefix for url parameters
        const pluginName = 'tooltips';

        // default plugin options
        const options = {
            // whether user must click off to close tooltip instead of just
            // un-hovering
            clickClose: 'false',
            // delay (in ms) between opening and closing tooltip
            delay: '100',
            // whether plugin is on or not
            enabled: 'true'
        };

        // change options above, or override with url parameter, eg:
        // 'manuscript.html?pluginName-enabled=false'

        // /////////////////////////
        // SCRIPT
        // /////////////////////////

        // start script
        function start() {
            const links = getLinks();
            for (const link of links) {
                // attach hover and focus listeners to link
                link.addEventListener('mouseover', onLinkHover);
                link.addEventListener('mouseleave', onLinkUnhover);
                link.addEventListener('focus', onLinkFocus);
                link.addEventListener('touchend', onLinkTouch);
            }

            // attach mouse, key, and resize listeners to window
            window.addEventListener('mousedown', onClick);
            window.addEventListener('touchstart', onClick);
            window.addEventListener('keyup', onKeyUp);
            window.addEventListener('resize', onResize);
        }

        // when link is hovered
        function onLinkHover() {
            // function to open tooltip
            const delayOpenTooltip = function() {
                openTooltip(this);
            }.bind(this);

            // run open function after delay
            this.openTooltipTimer = window.setTimeout(
                delayOpenTooltip,
                options.delay
            );
        }

        // when mouse leaves link
        function onLinkUnhover() {
            // cancel opening tooltip
            window.clearTimeout(this.openTooltipTimer);

            // don't close on unhover if option specifies
            if (options.clickClose === 'true')
                return;

            // function to close tooltip
            const delayCloseTooltip = function() {
                // if tooltip open and if mouse isn't over tooltip, close
                const tooltip = document.getElementById('tooltip');
                if (tooltip && !tooltip.matches(':hover'))
                    closeTooltip();
            };

            // run close function after delay
            this.closeTooltipTimer = window.setTimeout(
                delayCloseTooltip,
                options.delay
            );
        }

        // when link is focused (tabbed to)
        function onLinkFocus(event) {
            openTooltip(this);
        }

        // when link is touched on touch screen
        function onLinkTouch(event) {
            // attempt to force hover state on first tap always, and trigger
            // regular link click (and navigation) on second tap
            if (event.target === document.activeElement)
                event.target.click();
            else {
                document.activeElement.blur();
                event.target.focus();
            }
            if (event.cancelable)
                event.preventDefault();
            event.stopPropagation();
            return false;
        }

        // when mouse is clicked anywhere in window
        function onClick(event) {
            closeTooltip();
        }

        // when key pressed
        function onKeyUp(event) {
            if (!event || !event.key)
                return;

            switch (event.key) {
                // trigger click of prev button
                case 'ArrowLeft':
                    const prevButton = document.getElementById(
                        'tooltip_prev_button'
                    );
                    if (prevButton)
                        prevButton.click();
                    break;
                // trigger click of next button
                case 'ArrowRight':
                    const nextButton = document.getElementById(
                        'tooltip_next_button'
                    );
                    if (nextButton)
                        nextButton.click();
                    break;
                // close on esc
                case 'Escape':
                    closeTooltip();
                    break;
            }
        }

        // when window is resized or zoomed
        function onResize() {
            closeTooltip();
        }

        // get all links of types we wish to handle
        function getLinks() {
            const queries = [];
            // exclude buttons, anchor links, toc links, etc
            const exclude =
                ':not(.button):not(.icon_button):not(.anchor):not(.toc_link)';
            queries.push('a[href^="#ref-"]' + exclude); // citation links
            queries.push('a[href^="#fig:"]' + exclude); // figure links
            const query = queries.join(', ');
            return document.querySelectorAll(query);
        }

        // get links with same target, get index of link in set, get total
        // same links
        function getSameLinks(link) {
            const sameLinks = [];
            const links = getLinks();
            for (const otherLink of links) {
                if (
                    otherLink.getAttribute('href') === link.getAttribute('href')
                )
                    sameLinks.push(otherLink);
            }

            return {
                elements: sameLinks,
                index: sameLinks.indexOf(link),
                total: sameLinks.length
            };
        }

        // open tooltip
        function openTooltip(link) {
            // delete tooltip if it exists, start fresh
            closeTooltip();

            // make tooltip element
            const tooltip = makeTooltip(link);

            // if source couldn't be found and tooltip not made, exit
            if (!tooltip)
                return;

            // make navbar elements
            const navBar = makeNavBar(link);
            if (navBar)
                tooltip.firstElementChild.appendChild(navBar);

            // attach tooltip to page
            document.body.appendChild(tooltip);

            // position tooltip
            const position = function() {
                positionTooltip(link);
            };
            position();

            // if tooltip contains images, position again after they've loaded
            const imgs = tooltip.querySelectorAll('img');
            for (const img of imgs)
                img.addEventListener('load', position);
        }

        // close (delete) tooltip
        function closeTooltip() {
            const tooltip = document.getElementById('tooltip');
            if (tooltip)
                tooltip.remove();
        }

        // make tooltip
        function makeTooltip(link) {
            // get target element that link points to
            const source = getSource(link);

            // if source can't be found, exit
            if (!source)
                return;

            // create new tooltip
            const tooltip = document.createElement('div');
            tooltip.id = 'tooltip';
            const tooltipContent = document.createElement('div');
            tooltipContent.id = 'tooltip_content';
            tooltip.appendChild(tooltipContent);

            // make copy of source node and put in tooltip
            const sourceCopy = makeCopy(source);
            tooltipContent.appendChild(sourceCopy);

            // attach mouse event listeners
            tooltip.addEventListener('click', onTooltipClick);
            tooltip.addEventListener('mousedown', onTooltipClick);
            tooltip.addEventListener('touchstart', onTooltipClick);
            tooltip.addEventListener('mouseleave', onTooltipUnhover);

            // (for interaction with lightbox plugin)
            // transfer click on tooltip copied img to original img
            const sourceImg = source.querySelector('img');
            const sourceCopyImg = sourceCopy.querySelector('img');
            if (sourceImg && sourceCopyImg) {
                const clickImg = function() {
                    sourceImg.click();
                    closeTooltip();
                };
                sourceCopyImg.addEventListener('click', clickImg);
            }

            return tooltip;
        }

        // make carbon copy of html dom element
        function makeCopy(source) {
            const sourceCopy = source.cloneNode(true);

            // delete elements marked with ignore (eg anchor and jump buttons)
            const deleteFromCopy = sourceCopy.querySelectorAll(
                '[data-ignore="true"]'
            );
            for (const element of deleteFromCopy)
                element.remove();

            // delete certain element attributes
            const attributes = [
                'id',
                'data-collapsed',
                'data-selected',
                'data-highlighted',
                'data-glow'
            ];
            for (const attribute of attributes) {
                sourceCopy.removeAttribute(attribute);
                const elements = sourceCopy.querySelectorAll(
                    '[' + attribute + ']'
                );
                for (const element of elements)
                    element.removeAttribute(attribute);
            }

            return sourceCopy;
        }

        // when tooltip is clicked
        function onTooltipClick(event) {
            // when user clicks on tooltip, stop click from transferring
            // outside of tooltip (eg, click off to close tooltip, or eg click
            // off to unhighlight same refs)
            event.stopPropagation();
        }

        // when tooltip is unhovered
        function onTooltipUnhover(event) {
            if (options.clickClose === 'true')
                return;

            // make sure new mouse/touch/focus no longer over tooltip or any
            // element within it
            const tooltip = document.getElementById('tooltip');
            if (!tooltip)
                return;
            if (this.contains(event.relatedTarget))
                return;

            closeTooltip();
        }

        // make nav bar to go betwen prev/next instances of same reference
        function makeNavBar(link) {
            // find other links to the same source
            const sameLinks = getSameLinks(link);

            // don't show nav bar when singular reference
            if (sameLinks.total <= 1)
                return;

            // find prev/next links with same target
            const prevLink = getPrevLink(link, sameLinks);
            const nextLink = getNextLink(link, sameLinks);

            // create nav bar
            const navBar = document.createElement('div');
            navBar.id = 'tooltip_nav_bar';
            const text = sameLinks.index + 1 + ' of ' + sameLinks.total;

            // create nav bar prev/next buttons
            const prevButton = document.createElement('button');
            const nextButton = document.createElement('button');
            prevButton.id = 'tooltip_prev_button';
            nextButton.id = 'tooltip_next_button';
            prevButton.title =
                'Jump to the previous occurence of this item in the document [←]';
            nextButton.title =
                'Jump to the next occurence of this item in the document [→]';
            prevButton.classList.add('icon_button');
            nextButton.classList.add('icon_button');
            prevButton.innerHTML = document.querySelector(
                '.icon_caret_left'
            ).innerHTML;
            nextButton.innerHTML = document.querySelector(
                '.icon_caret_right'
            ).innerHTML;
            navBar.appendChild(prevButton);
            navBar.appendChild(document.createTextNode(text));
            navBar.appendChild(nextButton);

            // attach click listeners to buttons
            prevButton.addEventListener('click', function() {
                onPrevNextClick(link, prevLink);
            });
            nextButton.addEventListener('click', function() {
                onPrevNextClick(link, nextLink);
            });

            return navBar;
        }

        // get previous link with same target
        function getPrevLink(link, sameLinks) {
            if (!sameLinks)
                sameLinks = getSameLinks(link);
            // wrap index to other side if < 1
            let index;
            if (sameLinks.index - 1 >= 0)
                index = sameLinks.index - 1;
            else
                index = sameLinks.total - 1;
            return sameLinks.elements[index];
        }

        // get next link with same target
        function getNextLink(link, sameLinks) {
            if (!sameLinks)
                sameLinks = getSameLinks(link);
            // wrap index to other side if > total
            let index;
            if (sameLinks.index + 1 <= sameLinks.total - 1)
                index = sameLinks.index + 1;
            else
                index = 0;
            return sameLinks.elements[index];
        }

        // get element that is target of link or url hash
        function getSource(link) {
            const hash = link ? link.hash : window.location.hash;
            const id = hash.slice(1);
            let target = document.querySelector(
                '[id="' + id + '"], [name="' + id + '"]'
            );
            if (!target)
                return;

            // if figure or table, modify target to get expected element
            if (hash.indexOf('#ref-') === 0)
                target = target.querySelector('p');
            else if (hash.indexOf('#fig:') === 0)
                target = target.parentNode;
            else if (hash.indexOf('#tbl:') === 0)
                return;

            return target;
        }

        // when prev/next arrow button is clicked
        function onPrevNextClick(link, prevNextLink) {
            if (link && prevNextLink)
                goToElement(prevNextLink, window.innerHeight * 0.5);
        }

        // scroll to and focus element
        function goToElement(element, offset) {
            // expand accordion section if collapsed
            expandElement(element);
            const y =
                getRectInView(element).top -
                getRectInView(document.documentElement).top -
                (offset || 0);
            // trigger any function listening for "onscroll" event
            window.dispatchEvent(new Event('scroll'));
            window.scrollTo(0, y);
            document.activeElement.blur();
            element.focus();
        }

        // determine position to place tooltip based on link position in
        // viewport and tooltip size
        function positionTooltip(link, left, top) {
            const tooltipElement = document.getElementById('tooltip');
            if (!tooltipElement)
                return;

            // get convenient vars for position/dimensions of
            // link/tooltip/page/view
            link = getRectInPage(link);
            const tooltip = getRectInPage(tooltipElement);
            const view = getRectInPage();

            // horizontal positioning
            if (left)
                // use explicit value
                left = left;
            else if (link.left + tooltip.width < view.right)
                // fit tooltip to right of link
                left = link.left;
            else if (link.right - tooltip.width > view.left)
                // fit tooltip to left of link
                left = link.right - tooltip.width;
            // center tooltip in view
            else
                left = (view.right - view.left) / 2 - tooltip.width / 2;

            // vertical positioning
            if (top)
                // use explicit value
                top = top;
            else if (link.top - tooltip.height > view.top)
                // fit tooltip above link
                top = link.top - tooltip.height;
            else if (link.bottom + tooltip.height < view.bottom)
                // fit tooltip below link
                top = link.bottom;
            else {
                // center tooltip in view
                top = view.top + view.height / 2 - tooltip.height / 2;
                // nudge off of link to left/right if possible
                if (link.right + tooltip.width < view.right)
                    left = link.right;
                else if (link.left - tooltip.width > view.left)
                    left = link.left - tooltip.width;
            }

            tooltipElement.style.left = left + 'px';
            tooltipElement.style.top = top + 'px';
        }

        // get position/dimensions of element or viewport
        function getRectInView(element) {
            let rect = {};
            rect.left = 0;
            rect.top = 0;
            rect.right = document.documentElement.clientWidth;
            rect.bottom = document.documentElement.clientHeight;
            let style = {};

            if (element instanceof HTMLElement) {
                rect = element.getBoundingClientRect();
                style = window.getComputedStyle(element);
            }

            const margin = {};
            margin.left = parseFloat(style.marginLeftWidth) || 0;
            margin.top = parseFloat(style.marginTopWidth) || 0;
            margin.right = parseFloat(style.marginRightWidth) || 0;
            margin.bottom = parseFloat(style.marginBottomWidth) || 0;

            const border = {};
            border.left = parseFloat(style.borderLeftWidth) || 0;
            border.top = parseFloat(style.borderTopWidth) || 0;
            border.right = parseFloat(style.borderRightWidth) || 0;
            border.bottom = parseFloat(style.borderBottomWidth) || 0;

            const newRect = {};
            newRect.left = rect.left + margin.left + border.left;
            newRect.top = rect.top + margin.top + border.top;
            newRect.right = rect.right + margin.right + border.right;
            newRect.bottom = rect.bottom + margin.bottom + border.bottom;
            newRect.width = newRect.right - newRect.left;
            newRect.height = newRect.bottom - newRect.top;

            return newRect;
        }

        // get position of element relative to page
        function getRectInPage(element) {
            const rect = getRectInView(element);
            const body = getRectInView(document.body);

            const newRect = {};
            newRect.left = rect.left - body.left;
            newRect.top = rect.top - body.top;
            newRect.right = rect.right - body.left;
            newRect.bottom = rect.bottom - body.top;
            newRect.width = rect.width;
            newRect.height = rect.height;

            return newRect;
        }

        // (for interaction with accordion plugin)
        // get closest element before specified element that matches query
        function firstBefore(element, query) {
            while (
                element &&
                element !== document.body &&
                !element.matches(query)
            )
                element = element.previousElementSibling || element.parentNode;

            return element;
        }

        // (for interaction with accordion plugin)
        // check if element is part of collapsed heading
        function isCollapsed(element) {
            while (element && element !== document.body) {
                if (element.dataset.collapsed === 'true')
                    return true;
                element = element.parentNode;
            }
            return false;
        }

        // (for interaction with accordion plugin)
        // expand heading containing element if necesary
        function expandElement(element) {
            if (isCollapsed(element)) {
                const heading = firstBefore(element, 'h2');
                if (heading)
                    heading.click();
            }
        }

        // load options from url parameters
        function loadOptions() {
            const url = window.location.search;
            const params = new URLSearchParams(url);
            for (const optionName of Object.keys(options)) {
                const paramName = pluginName + '-' + optionName;
                const param = params.get(paramName);
                if (param !== '' && param !== null)
                    options[optionName] = param;
            }
        }
        loadOptions();

        // start script when document is finished loading
        if (options.enabled === 'true')
            window.addEventListener('load', start);
    })();
</script>

<!-- caret left icon -->

<template class="icon_caret_left">
    <!-- modified from: https://fontawesome.com/icons/caret-left -->
    <svg width="16" height="16" viewBox="0 0 192 512">
        <path
            fill="currentColor"
            d="M192 127.338v257.324c0 17.818-21.543 26.741-34.142 14.142L29.196 270.142c-7.81-7.81-7.81-20.474 0-28.284l128.662-128.662c12.599-12.6 34.142-3.676 34.142 14.142z"
        ></path>
    </svg>
</template>

<!-- caret right icon -->

<template class="icon_caret_right">
    <!-- modified from: https://fontawesome.com/icons/caret-right -->
    <svg width="16" height="16" viewBox="0 0 192 512">
        <path
            fill="currentColor"
            d="M0 384.662V127.338c0-17.818 21.543-26.741 34.142-14.142l128.662 128.662c7.81 7.81 7.81 20.474 0 28.284L34.142 398.804C21.543 411.404 0 402.48 0 384.662z"
        ></path>
    </svg>
</template>
<!-- jump to first plugin -->

<script>
    (function() {
        // /////////////////////////
        // DESCRIPTION
        // /////////////////////////

        // This Manubot plugin adds a button next to each reference entry,
        // figure, and table that jumps the page to the first occurrence of a
        // link to that item in the manuscript.

        // /////////////////////////
        // OPTIONS
        // /////////////////////////

        // plugin name prefix for url parameters
        const pluginName = 'jumpToFirst';

        // default plugin options
        const options = {
            // whether to add buttons next to reference entries
            references: 'true',
            // whether to add buttons next to figures
            figures: 'true',
            // whether to add buttons next to tables
            tables: 'true',
            // whether plugin is on or not
            enabled: 'true'
        };

        // change options above, or override with url parameter, eg:
        // 'manuscript.html?pluginName-enabled=false'

        // /////////////////////////
        // SCRIPT
        // /////////////////////////

        // start script
        function start() {
            if (options.references !== 'false')
                makeReferenceButtons();
            if (options.figures !== 'false')
                makeFigureButtons();
            if (options.tables !== 'false')
                makeTableButtons();
        }

        // when jump button clicked
        function onButtonClick() {
            const first = getFirstOccurrence(this.dataset.id);
            if (!first)
                return;

            // update url hash so navigating "back" in history will return
            // user to jump button
            window.location.hash = this.dataset.id;
            // scroll to link
            window.setTimeout(function() {
                goToElement(first, window.innerHeight * 0.5);
            }, 0);
        }

        // get first occurence of link to item in document
        function getFirstOccurrence(id) {
            let query = 'a';
            query += '[href="#' + id + '"]';
            // exclude buttons, anchor links, toc links, etc
            query +=
                ':not(.button):not(.icon_button):not(.anchor):not(.toc_link)';
            return document.querySelector(query);
        }

        // add button next to each reference entry
        function makeReferenceButtons() {
            const references = document.querySelectorAll('div[id^="ref-"]');
            for (const reference of references) {
                // get reference id and element to add button to
                const id = reference.id;
                const container = reference.firstElementChild;
                const first = getFirstOccurrence(id);

                // if can't find link to reference, ignore
                if (!first)
                    continue;

                // make jump button
                let button = document.createElement('button');
                button.classList.add('icon_button', 'jump_arrow');
                button.title =
                    'Jump to the first occurence of this reference in the document';
                button.innerHTML = document.querySelector(
                    '.icon_angle_double_up'
                ).innerHTML;
                button.dataset.id = id;
                button.dataset.ignore = 'true';
                container.innerHTML = button.outerHTML + container.innerHTML;
                button = container.firstElementChild;
                button.addEventListener('click', onButtonClick);
            }
        }

        // add button next to each figure
        function makeFigureButtons() {
            const figures = document.querySelectorAll('img[id^="fig:"]');
            for (const figure of figures) {
                // get figure id and element to add button to
                const id = figure.id;
                const container = figure.nextElementSibling;
                const first = getFirstOccurrence(id);

                // if can't find link to figure, ignore
                if (!first)
                    continue;

                // make jump button
                const button = document.createElement('button');
                button.classList.add('icon_button', 'jump_arrow');
                button.title =
                    'Jump to the first occurence of this figure in the document';
                button.innerHTML = document.querySelector(
                    '.icon_angle_double_up'
                ).innerHTML;
                button.dataset.id = id;
                button.dataset.ignore = 'true';
                container.insertBefore(button, container.firstElementChild);
                button.addEventListener('click', onButtonClick);
            }
        }

        // add button next to each figure
        function makeTableButtons() {
            const tables = document.querySelectorAll('a[name^="tbl:"]');
            for (const table of tables) {
                // get ref id and element to add button to
                const id = table.name;
                const container = table.nextElementSibling.querySelector(
                    'caption'
                );
                const first = getFirstOccurrence(id);

                // if can't find link to table, ignore
                if (!first)
                    continue;

                // make jump button
                const button = document.createElement('button');
                button.classList.add('icon_button', 'jump_arrow');
                button.title =
                    'Jump to the first occurence of this table in the document';
                button.innerHTML = document.querySelector(
                    '.icon_angle_double_up'
                ).innerHTML;
                button.dataset.id = id;
                button.dataset.ignore = 'true';
                container.insertBefore(button, container.firstElementChild);
                button.addEventListener('click', onButtonClick);
            }
        }

        // scroll to and focus element
        function goToElement(element, offset) {
            // expand accordion section if collapsed
            expandElement(element);
            const y =
                getRectInView(element).top -
                getRectInView(document.documentElement).top -
                (offset || 0);
            // trigger any function listening for "onscroll" event
            window.dispatchEvent(new Event('scroll'));
            window.scrollTo(0, y);
            document.activeElement.blur();
            element.focus();
        }

        // get position/dimensions of element or viewport
        function getRectInView(element) {
            let rect = {};
            rect.left = 0;
            rect.top = 0;
            rect.right = document.documentElement.clientWidth;
            rect.bottom = document.documentElement.clientHeight;
            let style = {};

            if (element instanceof HTMLElement) {
                rect = element.getBoundingClientRect();
                style = window.getComputedStyle(element);
            }

            const margin = {};
            margin.left = parseFloat(style.marginLeftWidth) || 0;
            margin.top = parseFloat(style.marginTopWidth) || 0;
            margin.right = parseFloat(style.marginRightWidth) || 0;
            margin.bottom = parseFloat(style.marginBottomWidth) || 0;

            const border = {};
            border.left = parseFloat(style.borderLeftWidth) || 0;
            border.top = parseFloat(style.borderTopWidth) || 0;
            border.right = parseFloat(style.borderRightWidth) || 0;
            border.bottom = parseFloat(style.borderBottomWidth) || 0;

            const newRect = {};
            newRect.left = rect.left + margin.left + border.left;
            newRect.top = rect.top + margin.top + border.top;
            newRect.right = rect.right + margin.right + border.right;
            newRect.bottom = rect.bottom + margin.bottom + border.bottom;
            newRect.width = newRect.right - newRect.left;
            newRect.height = newRect.bottom - newRect.top;

            return newRect;
        }

        // get closest element before specified element that matches query
        function firstBefore(element, query) {
            while (
                element &&
                element !== document.body &&
                !element.matches(query)
            )
                element = element.previousElementSibling || element.parentNode;

            return element;
        }

        // check if element is part of collapsed heading
        function isCollapsed(element) {
            while (element && element !== document.body) {
                if (element.dataset.collapsed === 'true')
                    return true;
                element = element.parentNode;
            }
            return false;
        }

        // (for interaction with accordion plugin)
        // expand heading containing element if necesary
        function expandElement(element) {
            if (isCollapsed(element)) {
                const heading = firstBefore(element, 'h2');
                if (heading)
                    heading.click();
            }
        }

        // load options from url parameters
        function loadOptions() {
            const url = window.location.search;
            const params = new URLSearchParams(url);
            for (const optionName of Object.keys(options)) {
                const paramName = pluginName + '-' + optionName;
                const param = params.get(paramName);
                if (param !== '' && param !== null)
                    options[optionName] = param;
            }
        }
        loadOptions();

        // start script when document is finished loading
        if (options.enabled === 'true')
            window.addEventListener('load', start);
    })();
</script>

<!-- angle double up icon -->

<template class="icon_angle_double_up">
    <!-- modified from: https://fontawesome.com/icons/angle-double-up -->
    <svg width="16" height="16" viewBox="0 0 320 512">
        <path
            fill="currentColor"
            d="M177 255.7l136 136c9.4 9.4 9.4 24.6 0 33.9l-22.6 22.6c-9.4 9.4-24.6 9.4-33.9 0L160 351.9l-96.4 96.4c-9.4 9.4-24.6 9.4-33.9 0L7 425.7c-9.4-9.4-9.4-24.6 0-33.9l136-136c9.4-9.5 24.6-9.5 34-.1zm-34-192L7 199.7c-9.4 9.4-9.4 24.6 0 33.9l22.6 22.6c9.4 9.4 24.6 9.4 33.9 0l96.4-96.4 96.4 96.4c9.4 9.4 24.6 9.4 33.9 0l22.6-22.6c9.4-9.4 9.4-24.6 0-33.9l-136-136c-9.2-9.4-24.4-9.4-33.8 0z"
        ></path>
    </svg>
</template>
<!-- link highlight plugin -->

<script>
    (function() {
        // /////////////////////////
        // DESCRIPTION
        // /////////////////////////

        // This Manubot plugin makes it such that when a user hovers or
        // focuses a link, other links that have the same target will be
        // highlighted. It also makes it such that when clicking a link, the
        // target of the link (eg reference, figure, table) is briefly
        // highlighted.

        // /////////////////////////
        // OPTIONS
        // /////////////////////////

        // plugin name prefix for url parameters
        const pluginName = 'linkHighlight';

        // default plugin options
        const options = {
            // whether to also highlight links that go to external urls
            externalLinks: 'false',
            // whether user must click off to unhighlight instead of just
            // un-hovering
            clickUnhighlight: 'false',
            // whether to also highlight links that are unique
            highlightUnique: 'true',
            // whether plugin is on or not
            enabled: 'true'
        };

        // change options above, or override with url parameter, eg:
        // 'manuscript.html?pluginName-enabled=false'

        // /////////////////////////
        // SCRIPT
        // /////////////////////////

        // start script
        function start() {
            const links = getLinks();
            for (const link of links) {
                // attach mouse and focus listeners to link
                link.addEventListener('mouseenter', onLinkFocus);
                link.addEventListener('focus', onLinkFocus);
                link.addEventListener('mouseleave', onLinkUnhover);
            }

            // attach click and hash change listeners to window
            window.addEventListener('click', onClick);
            window.addEventListener('touchstart', onClick);
            window.addEventListener('hashchange', onHashChange);

            // run hash change on window load in case user has navigated
            // directly to hash
            onHashChange();
        }

        // when link is focused (tabbed to) or hovered
        function onLinkFocus() {
            highlight(this);
        }

        // when link is unhovered
        function onLinkUnhover() {
            if (options.clickUnhighlight !== 'true')
                unhighlightAll();
        }

        // when the mouse is clicked anywhere in window
        function onClick(event) {
            unhighlightAll();
        }

        // when hash (eg manuscript.html#introduction) changes
        function onHashChange() {
            const target = getHashTarget();
            if (target)
                glowElement(target);
        }

        // get element that is target of link or url hash
        function getHashTarget(link) {
            const hash = link ? link.hash : window.location.hash;
            const id = hash.slice(1);
            let target = document.querySelector(
                '[id="' + id + '"], [name="' + id + '"]'
            );
            if (!target)
                return;

            // if figure or table, modify target to get expected element
            if (hash.indexOf('#fig:') === 0)
                target = target.parentNode;
            else if (hash.indexOf('#tbl:') === 0)
                target = target.nextElementSibling.querySelector('caption');

            return target;
        }

        // start glow sequence on an element
        function glowElement(element) {
            const startGlow = function() {
                onGlowEnd();
                element.dataset.glow = 'true';
                element.addEventListener('animationend', onGlowEnd);
            };
            const onGlowEnd = function() {
                element.removeAttribute('data-glow');
                element.removeEventListener('animationend', onGlowEnd);
            };
            startGlow();
        }

        // highlight link and all others with same target
        function highlight(link) {
            // force unhighlight all to start fresh
            unhighlightAll();

            // get links with same target
            if (!link)
                return;
            const sameLinks = getSameLinks(link);

            // if link unique and option is off, exit and don't highlight
            if (sameLinks.length <= 1 && options.highlightUnique !== 'true')
                return;

            // highlight all same links, and "select" (special highlight) this
            // one
            for (const sameLink of sameLinks) {
                if (sameLink === link)
                    sameLink.setAttribute('data-selected', 'true');
                else
                    sameLink.setAttribute('data-highlighted', 'true');
            }
        }

        // unhighlight all links
        function unhighlightAll() {
            const links = getLinks();
            for (const link of links) {
                link.setAttribute('data-selected', 'false');
                link.setAttribute('data-highlighted', 'false');
            }
        }

        // get links with same target
        function getSameLinks(link) {
            const results = [];
            const links = getLinks();
            for (const otherLink of links) {
                if (
                    otherLink.getAttribute('href') === link.getAttribute('href')
                )
                    results.push(otherLink);
            }
            return results;
        }

        // get all links of types we wish to handle
        function getLinks() {
            let query = 'a';
            if (options.externalLinks !== 'true')
                query += '[href^="#"]';
            // exclude buttons, anchor links, toc links, etc
            query +=
                ':not(.button):not(.icon_button):not(.anchor):not(.toc_link)';
            return document.querySelectorAll(query);
        }

        // load options from url parameters
        function loadOptions() {
            const url = window.location.search;
            const params = new URLSearchParams(url);
            for (const optionName of Object.keys(options)) {
                const paramName = pluginName + '-' + optionName;
                const param = params.get(paramName);
                if (param !== '' && param !== null)
                    options[optionName] = param;
            }
        }
        loadOptions();

        // start script when document is finished loading
        if (options.enabled === 'true')
            window.addEventListener('load', start);
    })();
</script>
<!-- table of contents plugin -->

<script>
    (function() {
        // /////////////////////////
        // DESCRIPTION
        // /////////////////////////

        // This Manubot plugin provides a "table of contents" (toc) panel on
        // the side of the document that allows the user to conveniently
        // navigate between sections of the document.

        // /////////////////////////
        // OPTIONS
        // /////////////////////////

        // plugin name prefix for url parameters
        const pluginName = 'tableOfContents';

        // default plugin options
        const options = {
            // which types of elements to add links for, in
            // "document.querySelector" format
            typesQuery: 'h1, h2, h3',
            // whether default behavior is to be closed ('false'), open
            // ('true'), or only open when screen wide enough to fit panel
            // ('auto'). note: still always starts closed when page loads.
            open: 'auto',
            // if list item is more than this many characters, text will be
            // truncated
            charLimit: '50',
            // whether or not to show bullets next to each toc item
            bullets: 'false',
            // whether plugin is on or not
            enabled: 'true'
        };

        // change options above, or override with url parameter, eg:
        // 'manuscript.html?pluginName-enabled=false'

        // /////////////////////////
        // SCRIPT
        // /////////////////////////

        // start script
        function start() {
            // make toc panel and populate with entries (links to document
            // sections)
            const panel = makePanel();
            if (!panel)
                return;
            makeEntries(panel);
            document.body.insertBefore(panel, document.body.firstChild);

            closePanel();

            // attach click, scroll, and hash change listeners to window
            window.addEventListener('click', onClick);
            window.addEventListener('touchstart', onClick);
            window.addEventListener('scroll', onScroll);
            window.addEventListener('hashchange', onScroll);
            window.addEventListener('keyup', onKeyUp);
            onScroll();

            // add class to push document body down out of way of toc button
            document.body.classList.add('toc_body_nudge');
        }

        // determine if screen wide enough to fit toc panel
        function isSmallScreen() {
            // in default theme:
            // 816px = 8.5in = width of "page" (<body>) element
            // 260px = min width of toc panel (*2 for both sides of <body>)
            return window.innerWidth < 816 + 260 * 2;
        }

        // open/close panel based on option and screen size
        function openOrClosePanel() {
            if (
                options.open === 'true' ||
                (options.open === 'auto' && !isSmallScreen())
            )
                openPanel();
            else
                closePanel();
        }

        // when mouse is clicked anywhere in window
        function onClick() {
            const panel = document.getElementById('toc_panel');
            if (!panel)
                return;

            if (panel.dataset.open === 'true')
                openOrClosePanel();
        }

        // when window is scrolled or hash changed
        function onScroll() {
            highlightViewed();
        }

        // when key pressed
        function onKeyUp(event) {
            if (!event || !event.key)
                return;

            // close on esc
            if (event.key === 'Escape')
                closePanel();
        }

        // find entry of currently viewed document section in toc and highlight
        function highlightViewed() {
            const firstId = getFirstInView(options.typesQuery);

            // get toc entries (links), unhighlight all, then highlight viewed
            const list = document.getElementById('toc_list');
            if (!firstId || !list)
                return;
            const links = list.querySelectorAll('a');
            for (const link of links)
                link.dataset.viewing = 'false';
            const link = list.querySelector('a[href="#' + firstId + '"]');
            if (!link)
                return;
            link.dataset.viewing = 'true';
        }

        // get first or previous toc listed element in top half of view
        function getFirstInView(query) {
            // get all elements matching query and with id
            const elements = document.querySelectorAll(query);
            const elementsWithIds = [];
            for (const element of elements) {
                if (element.id)
                    elementsWithIds.push(element);
            }


            // get first or previous element in top half of view
            for (let i = 0; i < elementsWithIds.length; i++) {
                const element = elementsWithIds[i];
                const prevElement = elementsWithIds[Math.max(0, i - 1)];
                if (element.getBoundingClientRect().top >= 0) {
                    if (
                        element.getBoundingClientRect().top <
                        window.innerHeight / 2
                    )
                        return element.id;
                    else
                        return prevElement.id;
                }
            }
        }

        // make panel
        function makePanel() {
            // create panel
            const panel = document.createElement('div');
            panel.id = 'toc_panel';
            if (options.bullets === 'true')
                panel.dataset.bullets = 'true';

            // create header
            const header = document.createElement('div');
            header.id = 'toc_header';

            // create toc button
            const button = document.createElement('button');
            button.id = 'toc_button';
            button.innerHTML = document.querySelector(
                '.icon_th_list'
            ).innerHTML;
            button.classList.add('icon_button');

            // create header text
            const text = document.createElement('h3');
            text.innerHTML = 'Table of Contents';

            // create container for toc list
            const list = document.createElement('div');
            list.id = 'toc_list';

            // attach click listeners
            panel.addEventListener('click', onPanelClick);
            header.addEventListener('click', onHeaderClick);
            button.addEventListener('click', onButtonClick);

            // attach elements
            header.appendChild(button);
            header.appendChild(text);
            panel.appendChild(header);
            panel.appendChild(list);

            return panel;
        }

        // create toc entries (links) to each element of the specified types
        function makeEntries(panel) {
            const elements = document.querySelectorAll(options.typesQuery);
            for (const element of elements) {
                // do not add link if element doesn't have assigned id
                if (!element.id)
                    continue;

                // create link/list item
                const link = document.createElement('a');
                link.classList.add('toc_link');
                switch (element.tagName.toLowerCase()) {
                    case 'h1':
                        link.dataset.level = '1';
                        break;
                    case 'h2':
                        link.dataset.level = '2';
                        break;
                    case 'h3':
                        link.dataset.level = '3';
                        break;
                    case 'h4':
                        link.dataset.level = '4';
                        break;
                }
                link.title = element.innerText;
                let text = element.innerText;
                if (text.length > options.charLimit)
                    text = text.slice(0, options.charLimit) + '...';
                link.innerHTML = text;
                link.href = '#' + element.id;
                link.addEventListener('click', onLinkClick);

                // attach link
                panel.querySelector('#toc_list').appendChild(link);
            }
        }

        // when panel is clicked
        function onPanelClick(event) {
            // stop click from propagating to window/document and closing panel
            event.stopPropagation();
        }

        // when header itself is clicked
        function onHeaderClick(event) {
            togglePanel();
        }

        // when button is clicked
        function onButtonClick(event) {
            togglePanel();
            // stop header underneath button from also being clicked
            event.stopPropagation();
        }

        // when link is clicked
        function onLinkClick() {
            openOrClosePanel();
        }

        // open panel if closed, close if opened
        function togglePanel() {
            const panel = document.getElementById('toc_panel');
            if (!panel)
                return;

            if (panel.dataset.open === 'true')
                closePanel();
            else
                openPanel();
        }

        // open panel
        function openPanel() {
            const panel = document.getElementById('toc_panel');
            if (panel)
                panel.dataset.open = 'true';
        }

        // close panel
        function closePanel() {
            const panel = document.getElementById('toc_panel');
            if (panel)
                panel.dataset.open = 'false';
        }

        // load options from url parameters
        function loadOptions() {
            const url = window.location.search;
            const params = new URLSearchParams(url);
            for (const optionName of Object.keys(options)) {
                const paramName = pluginName + '-' + optionName;
                const param = params.get(paramName);
                if (param !== '' && param !== null)
                    options[optionName] = param;
            }
        }
        loadOptions();

        // start script when document is finished loading
        if (options.enabled === 'true')
            window.addEventListener('load', start);
    })();
</script>

<!-- th list icon -->

<template class="icon_th_list">
    <!-- modified from: https://fontawesome.com/icons/th-list -->
    <svg width="16" height="16" viewBox="0 0 512 512" tabindex="-1">
        <path
            fill="currentColor"
            d="M96 96c0 26.51-21.49 48-48 48S0 122.51 0 96s21.49-48 48-48 48 21.49 48 48zM48 208c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48zm0 160c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48zm96-236h352c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H144c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h352c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H144c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h352c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H144c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"
            tabindex="-1"
        ></path>
    </svg>
</template>
<!-- lightbox plugin -->

<script>
    (function() {
        // /////////////////////////
        // DESCRIPTION
        // /////////////////////////

        // This Manubot plugin makes it such that when a user clicks on an
        // image, the image fills the screen and the user can pan/drag/zoom
        // the image and navigate between other images in the document.

        // /////////////////////////
        // OPTIONS
        // /////////////////////////

        // plugin name prefix for url parameters
        const pluginName = 'lightbox';

        // default plugin options
        const options = {
            // list of possible zoom/scale factors
            zoomSteps:
                '0.1, 0.25, 0.333333, 0.5, 0.666666, 0.75, 1,' +
                '1.25, 1.5, 1.75, 2, 2.5, 3, 3.5, 4, 5, 6, 7, 8',
            // whether to fit image to view ('fit'), display at 100% and shrink
            // if necessary ('shrink'), or always display at 100% ('100')
            defaultZoom: 'fit',
            // whether to zoom in/out toward center of view ('true') or mouse
            // ('false')
            centerZoom: 'false',
            // whether plugin is on or not
            enabled: 'true'
        };

        // change options above, or override with url parameter, eg:
        // 'manuscript.html?pluginName-enabled=false'

        // /////////////////////////
        // SCRIPT
        // /////////////////////////

        // start script
        function start() {
            // run through each <img> element
            const imgs = document.querySelectorAll('figure > img');
            let count = 1;
            for (const img of imgs) {
                img.classList.add('lightbox_document_img');
                img.dataset.number = count;
                img.dataset.total = imgs.length;
                img.addEventListener('click', openLightbox);
                count++;
            }

            // attach mouse and key listeners to window
            window.addEventListener('mousemove', onWindowMouseMove);
            window.addEventListener('keyup', onKeyUp);
        }

        // when mouse is moved anywhere in window
        function onWindowMouseMove(event) {
            window.mouseX = event.clientX;
            window.mouseY = event.clientY;
        }

        // when key pressed
        function onKeyUp(event) {
            if (!event || !event.key)
                return;

            switch (event.key) {
                // trigger click of prev button
                case 'ArrowLeft':
                    const prevButton = document.getElementById(
                        'lightbox_prev_button'
                    );
                    if (prevButton)
                        prevButton.click();
                    break;
                // trigger click of next button
                case 'ArrowRight':
                    const nextButton = document.getElementById(
                        'lightbox_next_button'
                    );
                    if (nextButton)
                        nextButton.click();
                    break;
                // close on esc
                case 'Escape':
                    closeLightbox();
                    break;
            }
        }

        // open lightbox
        function openLightbox() {
            const lightbox = makeLightbox(this);
            if (!lightbox)
                return;

            blurBody(lightbox);
            document.body.appendChild(lightbox);
        }

        // make lightbox
        function makeLightbox(img) {
            // delete lightbox if it exists, start fresh
            closeLightbox();

            // create screen overlay containing lightbox
            const overlay = document.createElement('div');
            overlay.id = 'lightbox_overlay';

            // create image info boxes
            const numberInfo = document.createElement('div');
            const zoomInfo = document.createElement('div');
            numberInfo.id = 'lightbox_number_info';
            zoomInfo.id = 'lightbox_zoom_info';

            // create container for image
            const imageContainer = document.createElement('div');
            imageContainer.id = 'lightbox_image_container';
            const lightboxImg = makeLightboxImg(
                img,
                imageContainer,
                numberInfo,
                zoomInfo
            );
            imageContainer.appendChild(lightboxImg);

            // create bottom container for caption and navigation buttons
            const bottomContainer = document.createElement('div');
            bottomContainer.id = 'lightbox_bottom_container';
            const caption = makeCaption(img);
            const prevButton = makePrevButton(img);
            const nextButton = makeNextButton(img);
            bottomContainer.appendChild(prevButton);
            bottomContainer.appendChild(caption);
            bottomContainer.appendChild(nextButton);

            // attach top middle and bottom to overlay
            overlay.appendChild(numberInfo);
            overlay.appendChild(zoomInfo);
            overlay.appendChild(imageContainer);
            overlay.appendChild(bottomContainer);

            return overlay;
        }

        // make <img> object that is intuitively draggable and zoomable
        function makeLightboxImg(
            sourceImg,
            container,
            numberInfoBox,
            zoomInfoBox
        ) {
            // create copy of source <img>
            const img = sourceImg.cloneNode(true);
            img.classList.remove('lightbox_document_img');
            img.removeAttribute('id');
            img.removeAttribute('width');
            img.removeAttribute('height');
            img.style.position = 'unset';
            img.style.margin = '0';
            img.style.padding = '0';
            img.style.width = '';
            img.style.height = '';
            img.style.minWidth = '';
            img.style.minHeight = '';
            img.style.maxWidth = '';
            img.style.maxHeight = '';
            img.id = 'lightbox_img';

            // build sorted list of unique zoomSteps, always including a 100%
            let zoomSteps = [];
            const optionsZooms = options.zoomSteps.split(/[^0-9.]/);
            for (const optionZoom of optionsZooms) {
                const newZoom = parseFloat(optionZoom);
                if (newZoom && !zoomSteps.includes(newZoom))
                    zoomSteps.push(newZoom);
            }
            if (!zoomSteps.includes(1))
                zoomSteps.push(1);
            zoomSteps = zoomSteps.sort(function sortNumber(a, b) {
                return a - b;
            });

            // <img> object property variables
            let zoom = 1;
            let translateX = 0;
            let translateY = 0;
            let clickMouseX = undefined;
            let clickMouseY = undefined;
            let clickTranslateX = undefined;
            let clickTranslateY = undefined;

            updateNumberInfo();

            // update image numbers displayed in info box
            function updateNumberInfo() {
                numberInfoBox.innerHTML =
                    sourceImg.dataset.number + ' of ' + sourceImg.dataset.total;
            }

            // update zoom displayed in info box
            function updateZoomInfo() {
                let zoomInfo = zoom * 100;
                if (!Number.isInteger(zoomInfo))
                    zoomInfo = zoomInfo.toFixed(2);
                zoomInfoBox.innerHTML = zoomInfo + '%';
            }

            // move to closest zoom step above current zoom
            const zoomIn = function() {
                for (const zoomStep of zoomSteps) {
                    if (zoomStep > zoom) {
                        zoom = zoomStep;
                        break;
                    }
                }
                updateTransform();
            };

            // move to closest zoom step above current zoom
            const zoomOut = function() {
                zoomSteps.reverse();
                for (const zoomStep of zoomSteps) {
                    if (zoomStep < zoom) {
                        zoom = zoomStep;
                        break;
                    }
                }
                zoomSteps.reverse();

                updateTransform();
            };

            // update display of <img> based on scale/translate properties
            const updateTransform = function() {
                // set transform
                img.style.transform =
                    'translate(' +
                    (translateX || 0) +
                    'px,' +
                    (translateY || 0) +
                    'px) scale(' +
                    (zoom || 1) +
                    ')';

                // get new width/height after scale
                const rect = img.getBoundingClientRect();
                // limit translate
                translateX = Math.max(translateX, -rect.width / 2);
                translateX = Math.min(translateX, rect.width / 2);
                translateY = Math.max(translateY, -rect.height / 2);
                translateY = Math.min(translateY, rect.height / 2);

                // set transform
                img.style.transform =
                    'translate(' +
                    (translateX || 0) +
                    'px,' +
                    (translateY || 0) +
                    'px) scale(' +
                    (zoom || 1) +
                    ')';

                updateZoomInfo();
            };

            // fit <img> to container
            const fit = function() {
                // no x/y offset, 100% zoom by default
                translateX = 0;
                translateY = 0;
                zoom = 1;

                // widths of <img> and container
                const imgWidth = img.naturalWidth;
                const imgHeight = img.naturalHeight;
                const containerWidth = parseFloat(
                    window.getComputedStyle(container).width
                );
                const containerHeight = parseFloat(
                    window.getComputedStyle(container).height
                );

                // how much zooming is needed to fit <img> to container
                const xRatio = imgWidth / containerWidth;
                const yRatio = imgHeight / containerHeight;
                const maxRatio = Math.max(xRatio, yRatio);
                const newZoom = 1 / maxRatio;

                // fit <img> to container according to option
                if (options.defaultZoom === 'shrink') {
                    if (maxRatio > 1)
                        zoom = newZoom;
                } else if (options.defaultZoom === 'fit')
                    zoom = newZoom;

                updateTransform();
            };

            // when mouse wheel is rolled anywhere in container
            const onContainerWheel = function(event) {
                if (!event)
                    return;

                // let ctrl + mouse wheel to zoom behave as normal
                if (event.ctrlKey)
                    return;

                // prevent normal scroll behavior
                event.preventDefault();
                event.stopPropagation();

                // point around which to scale img
                const viewRect = container.getBoundingClientRect();
                const viewX = (viewRect.left + viewRect.right) / 2;
                const viewY = (viewRect.top + viewRect.bottom) / 2;
                const originX = options.centerZoom === 'true' ? viewX : mouseX;
                const originY = options.centerZoom === 'true' ? viewY : mouseY;

                // get point on image under origin
                const oldRect = img.getBoundingClientRect();
                const oldPercentX = (originX - oldRect.left) / oldRect.width;
                const oldPercentY = (originY - oldRect.top) / oldRect.height;

                // increment/decrement zoom
                if (event.deltaY < 0)
                    zoomIn();
                if (event.deltaY > 0)
                    zoomOut();

                // get offset between previous image point and origin
                const newRect = img.getBoundingClientRect();
                const offsetX =
                    originX - (newRect.left + newRect.width * oldPercentX);
                const offsetY =
                    originY - (newRect.top + newRect.height * oldPercentY);

                // translate image to keep image point under origin
                translateX += offsetX;
                translateY += offsetY;

                // perform translate
                updateTransform();
            };

            // when container is clicked
            function onContainerClick(event) {
                // if container itself is target of click, and not other
                // element above it
                if (event.target === this)
                    closeLightbox();
            }

            // when mouse button is pressed on image
            const onImageMouseDown = function(event) {
                // store original mouse position relative to image
                clickMouseX = window.mouseX;
                clickMouseY = window.mouseY;
                clickTranslateX = translateX;
                clickTranslateY = translateY;
                event.stopPropagation();
                event.preventDefault();
            };

            // when mouse button is released anywhere in window
            const onWindowMouseUp = function(event) {
                // reset original mouse position
                clickMouseX = undefined;
                clickMouseY = undefined;
                clickTranslateX = undefined;
                clickTranslateY = undefined;

                // remove global listener if lightbox removed from document
                if (!document.body.contains(container))
                    window.removeEventListener('mouseup', onWindowMouseUp);
            };

            // when mouse is moved anywhere in window
            const onWindowMouseMove = function(event) {
                if (
                    clickMouseX === undefined ||
                    clickMouseY === undefined ||
                    clickTranslateX === undefined ||
                    clickTranslateY === undefined
                )
                    return;

                // offset image based on original and current mouse position
                translateX = clickTranslateX + window.mouseX - clickMouseX;
                translateY = clickTranslateY + window.mouseY - clickMouseY;
                updateTransform();
                event.preventDefault();

                // remove global listener if lightbox removed from document
                if (!document.body.contains(container))
                    window.removeEventListener('mousemove', onWindowMouseMove);
            };

            // when window is resized
            const onWindowResize = function(event) {
                fit();

                // remove global listener if lightbox removed from document
                if (!document.body.contains(container))
                    window.removeEventListener('resize', onWindowResize);
            };

            // attach the necessary event listeners
            img.addEventListener('dblclick', fit);
            img.addEventListener('mousedown', onImageMouseDown);
            container.addEventListener('wheel', onContainerWheel);
            container.addEventListener('mousedown', onContainerClick);
            container.addEventListener('touchstart', onContainerClick);
            window.addEventListener('mouseup', onWindowMouseUp);
            window.addEventListener('mousemove', onWindowMouseMove);
            window.addEventListener('resize', onWindowResize);

            // run fit() after lightbox atttached to document and <img> Loaded
            // so needed container and img dimensions available
            img.addEventListener('load', fit);

            return img;
        }

        // make caption
        function makeCaption(img) {
            const caption = document.createElement('div');
            caption.id = 'lightbox_caption';
            const captionSource = img.nextElementSibling;
            if (captionSource.tagName.toLowerCase() === 'figcaption') {
                const captionCopy = makeCopy(captionSource);
                caption.innerHTML = captionCopy.innerHTML;
            }

            caption.addEventListener('touchstart', function(event) {
                event.stopPropagation();
            });

            return caption;
        }

        // make carbon copy of html dom element
        function makeCopy(source) {
            const sourceCopy = source.cloneNode(true);

            // delete elements marked with ignore (eg anchor and jump buttons)
            const deleteFromCopy = sourceCopy.querySelectorAll(
                '[data-ignore="true"]'
            );
            for (const element of deleteFromCopy)
                element.remove();

            // delete certain element attributes
            const attributes = [
                'id',
                'data-collapsed',
                'data-selected',
                'data-highlighted',
                'data-glow'
            ];
            for (const attribute of attributes) {
                sourceCopy.removeAttribute(attribute);
                const elements = sourceCopy.querySelectorAll(
                    '[' + attribute + ']'
                );
                for (const element of elements)
                    element.removeAttribute(attribute);
            }

            return sourceCopy;
        }

        // make button to jump to previous image in document
        function makePrevButton(img) {
            const prevButton = document.createElement('button');
            prevButton.id = 'lightbox_prev_button';
            prevButton.title = 'Jump to the previous image in the document [←]';
            prevButton.classList.add('icon_button', 'lightbox_button');
            prevButton.innerHTML = document.querySelector(
                '.icon_caret_left'
            ).innerHTML;

            // attach click listeners to button
            prevButton.addEventListener('click', function() {
                getPrevImg(img).click();
            });

            return prevButton;
        }

        // make button to jump to next image in document
        function makeNextButton(img) {
            const nextButton = document.createElement('button');
            nextButton.id = 'lightbox_next_button';
            nextButton.title = 'Jump to the next image in the document [→]';
            nextButton.classList.add('icon_button', 'lightbox_button');
            nextButton.innerHTML = document.querySelector(
                '.icon_caret_right'
            ).innerHTML;

            // attach click listeners to button
            nextButton.addEventListener('click', function() {
                getNextImg(img).click();
            });

            return nextButton;
        }

        // get previous image in document
        function getPrevImg(img) {
            const imgs = document.querySelectorAll('.lightbox_document_img');

            // find index of provided img
            let index;
            for (index = 0; index < imgs.length; index++) {
                if (imgs[index] === img)
                    break;
            }


            // wrap index to other side if < 1
            if (index - 1 >= 0)
                index--;
            else
                index = imgs.length - 1;
            return imgs[index];
        }

        // get next image in document
        function getNextImg(img) {
            const imgs = document.querySelectorAll('.lightbox_document_img');

            // find index of provided img
            let index;
            for (index = 0; index < imgs.length; index++) {
                if (imgs[index] === img)
                    break;
            }


            // wrap index to other side if > total
            if (index + 1 <= imgs.length - 1)
                index++;
            else
                index = 0;
            return imgs[index];
        }

        // close lightbox
        function closeLightbox() {
            focusBody();

            const lightbox = document.getElementById('lightbox_overlay');
            if (lightbox)
                lightbox.remove();
        }

        // make all elements behind lightbox non-focusable
        function blurBody(overlay) {
            const all = document.querySelectorAll('*');
            for (const element of all)
                element.tabIndex = -1;
            document.body.classList.add('body_no_scroll');
        }

        // make all elements focusable again
        function focusBody() {
            const all = document.querySelectorAll('*');
            for (const element of all)
                element.removeAttribute('tabIndex');
            document.body.classList.remove('body_no_scroll');
        }

        // load options from url parameters
        function loadOptions() {
            const url = window.location.search;
            const params = new URLSearchParams(url);
            for (const optionName of Object.keys(options)) {
                const paramName = pluginName + '-' + optionName;
                const param = params.get(paramName);
                if (param !== '' && param !== null)
                    options[optionName] = param;
            }
        }
        loadOptions();

        // start script when document is finished loading
        if (options.enabled === 'true')
            window.addEventListener('load', start);
    })();
</script>

<!-- caret left icon -->

<template class="icon_caret_left">
    <!-- modified from: https://fontawesome.com/icons/caret-left -->
    <svg width="16" height="16" viewBox="0 0 192 512">
        <path
            fill="currentColor"
            d="M192 127.338v257.324c0 17.818-21.543 26.741-34.142 14.142L29.196 270.142c-7.81-7.81-7.81-20.474 0-28.284l128.662-128.662c12.599-12.6 34.142-3.676 34.142 14.142z"
        ></path>
    </svg>
</template>

<!-- caret right icon -->

<template class="icon_caret_right">
    <!-- modified from: https://fontawesome.com/icons/caret-right -->
    <svg width="16" height="16" viewBox="0 0 192 512">
        <path
            fill="currentColor"
            d="M0 384.662V127.338c0-17.818 21.543-26.741 34.142-14.142l128.662 128.662c7.81 7.81 7.81 20.474 0 28.284L34.142 398.804C21.543 411.404 0 402.48 0 384.662z"
        ></path>
    </svg>
</template>
<!-- math plugin configuration -->

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        "CommonHTML": { linebreaks: { automatic: true } },
        "HTML-CSS": { linebreaks: { automatic: true } },
        "SVG": { linebreaks: { automatic: true } }
    });
</script>

<!-- math plugin -->

<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML'>
    // /////////////////////////
    // DESCRIPTION
    // /////////////////////////

    // This third-party plugin 'MathJax' allows the proper rendering of
    // math/equations written in LaTeX.

    // https://www.mathjax.org/
</script>
<!-- annotations plugin configuration -->

<script>
    window.hypothesisConfig = function() {
        return {
            branding: {
                accentColor: '#2196f3',
                appBackgroundColor: '#f8f8f8',
                ctaBackgroundColor: '#f8f8f8',
                ctaTextColor: '#000000',
                selectionFontFamily: 'Open Sans, Helvetica, sans serif',
                annotationFontFamily: 'Open Sans, Helvetica, sans serif'
            }
        };
    };
</script>

<!-- annotations plugin -->

<script src='https://hypothes.is/embed.js'>
    // /////////////////////////
    // DESCRIPTION
    // /////////////////////////

    // This third-party plugin 'Hypothesis' allows public annotation of the
    // manuscript.

    // https://web.hypothes.is/
</script>
<!-- analytics plugin -->
<!-- Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-20583020-10', 'auto');
  ga('send', 'pageview');
</script>
<!-- End Google Analytics -->

<!-- copy and paste code from Google Analytics or similar service here -->
</body>
</html>
